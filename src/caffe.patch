diff --git a/.gitignore b/.gitignore
index c88381f..0208d66 100644
--- a/.gitignore
+++ b/.gitignore
@@ -61,9 +61,6 @@ Makefile.config
 # Data and models are either
 # 1. reference, and not casually committed
 # 2. custom, and live on their own unless they're deliberated contributed
-#data/*
-#models/*
-#*.caffemodel
 *.caffemodel.h5
 *.solverstate
 *.solverstate.h5
@@ -97,3 +94,17 @@ LOCK
 LOG*
 CURRENT
 MANIFEST-*
+/Release
+/out
+/.vs
+/Debug
+/CMakeFiles
+/data/aie
+*.stamp
+*.depend
+*.vcxproj
+*.filters
+/src/caffe/CMakeFiles/cuda_compile_1.dir
+/src/caffe/test/CMakeFiles/cuda_compile_1.dir
+/install
+/x64-Debug
diff --git a/CMakeLists.txt b/CMakeLists.txt
index aa4ad2a..42fe435 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -1,12 +1,18 @@
 cmake_minimum_required(VERSION 2.8.7)
+if(MSVC)
+  # CMake 3.4 introduced a WINDOWS_EXPORT_ALL_SYMBOLS target property that makes it possible to
+  # build shared libraries without using the usual declspec() decoration.
+  # See: https://blog.kitware.com/create-dlls-on-windows-without-declspec-using-new-cmake-export-all-feature/
+  # and https://cmake.org/cmake/help/v3.5/prop_tgt/WINDOWS_EXPORT_ALL_SYMBOLS.html
+  # for details.
+  cmake_minimum_required(VERSION 3.4)
+endif()
 if(POLICY CMP0046)
   cmake_policy(SET CMP0046 NEW)
 endif()
 if(POLICY CMP0054)
   cmake_policy(SET CMP0054 NEW)
 endif()
-SET(CMAKE_C_COMPILER gcc-5)
-SET(CMAKE_CXX_COMPILER g++-5)
 # ---[ Caffe project
 project(Caffe C CXX)
 
@@ -26,26 +32,52 @@ include(cmake/Targets.cmake)
 include(cmake/Misc.cmake)
 include(cmake/Summary.cmake)
 include(cmake/ConfigGen.cmake)
+include(cmake/WindowsCreateLinkHeader.cmake)
+include(cmake/TargetResolvePrerequesites.cmake)
 
 # ---[ Options
-caffe_option(CPU_ONLY  "Build Caffe without CUDA support" OFF) # TODO: rename to USE_CUDA
+caffe_option(CPU_ONLY  "Build Caffe without CUDA support" OFF)
 caffe_option(USE_CUDNN "Build Caffe with cuDNN library support" OFF IF NOT CPU_ONLY)
 caffe_option(USE_NCCL "Build Caffe with NCCL library support" OFF)
-caffe_option(BUILD_SHARED_LIBS "Build shared libraries" ON)
-caffe_option(BUILD_python "Build Python wrapper" ON)
-set(python_version "2" CACHE STRING "Specify which Python version to use")
-caffe_option(BUILD_matlab "Build Matlab wrapper" OFF IF UNIX OR APPLE)
+if(MSVC)
+  # default to static libs
+  caffe_option(BUILD_SHARED_LIBS "Build shared libraries" OFF)
+else()
+  caffe_option(BUILD_SHARED_LIBS "Build shared libraries" ON)
+endif()
+caffe_option(BUILD_python "Build Python wrapper" OFF)
+set(python_version "3" CACHE STRING "Specify which Python version to use")
+caffe_option(BUILD_matlab "Build Matlab wrapper" OFF)
 caffe_option(BUILD_docs   "Build documentation" ON IF UNIX OR APPLE)
-caffe_option(BUILD_python_layer "Build the Caffe Python layer" ON)
+caffe_option(BUILD_python_layer "Build the Caffe Python layer" OFF)
 caffe_option(USE_OPENCV "Build with OpenCV support" ON)
-caffe_option(USE_LEVELDB "Build with levelDB" OFF)
+caffe_option(USE_LEVELDB "Build with levelDB" ON)
 caffe_option(USE_LMDB "Build with lmdb" ON)
 caffe_option(ALLOW_LMDB_NOLOCK "Allow MDB_NOLOCK when reading LMDB files (only if necessary)" OFF)
-caffe_option(USE_OPENMP "Link with OpenMP (when your BLAS wants OpenMP and you get linker errors)" OFF)
+caffe_option(USE_OPENMP "Link with OpenMP (when your BLAS wants OpenMP and you get linker errors)" ON)
+caffe_option(protobuf_MODULE_COMPATIBLE "Make the protobuf-config.cmake compatible with the module mode" ON IF MSVC)
+caffe_option(COPY_PREREQUISITES "Copy the prerequisites next to each executable or shared library directory" ON IF MSVC)
+caffe_option(INSTALL_PREREQUISITES "Install the prerequisites next to each executable or shared library directory" ON IF MSVC)
+
+if(MSVC AND BUILD_SHARED_LIBS)
+  if(CMAKE_GENERATOR MATCHES "Visual Studio")
+    # see issue https://gitlab.kitware.com/cmake/cmake/issues/16552#note_215236
+    message(FATAL_ERROR "The Visual Studio generator cannot build a shared library. Use the Ninja generator instead.")
+  endif()
+  # Some tests (solver tests) fail when caffe is built as a shared library. The problem comes
+  # from protobuf that has a global static empty_string_ variable. Since caffe and test.testbin
+  # link to a static protobuf library both end up with their own instance of the empty_string_
+  # variable. This causes some SEH exception to occur. In practice if the caffe executable does not link
+  # to protobuf this problem should not happen. Use at your own risk.
+  message(WARNING "Some tests (solvers) will fail when building as a shared library with MSVC")
+endif()
+
+# ---[ Prebuild dependencies on windows
+include(cmake/WindowsDownloadPrebuiltDependencies.cmake)
 
 # ---[ Dependencies
 include(cmake/Dependencies.cmake)
-SET( CMAKE_CXX_FLAGS "-std=c++11 -O3")
+
 # ---[ Flags
 if(UNIX OR APPLE)
   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fPIC -Wall")
@@ -67,8 +99,8 @@ configure_file(cmake/Templates/caffe_config.h.in "${PROJECT_BINARY_DIR}/caffe_co
 # ---[ Includes
 set(Caffe_INCLUDE_DIR ${PROJECT_SOURCE_DIR}/include)
 set(Caffe_SRC_DIR ${PROJECT_SOURCE_DIR}/src)
-include_directories(${PROJECT_BINARY_DIR})
-
+include_directories(${Caffe_INCLUDE_DIR} ${PROJECT_BINARY_DIR})
+include_directories(BEFORE src) # This is needed for gtest.
 # ---[ Includes & defines for CUDA
 
 # cuda_compile() does not have per-call dependencies or include pathes
@@ -97,11 +129,16 @@ add_subdirectory(matlab)
 add_subdirectory(docs)
 
 # ---[ Linter target
-add_custom_target(lint COMMAND ${CMAKE_COMMAND} -P ${PROJECT_SOURCE_DIR}/cmake/lint.cmake)
+add_custom_target(lint COMMAND ${CMAKE_COMMAND} -DPYTHON_EXECUTABLE=${PYTHON_EXECUTABLE} -P ${PROJECT_SOURCE_DIR}/cmake/lint.cmake)
 
 # ---[ pytest target
 if(BUILD_python)
-  add_custom_target(pytest COMMAND python${python_version} -m unittest discover -s caffe/test WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/python )
+  if(UNIX)
+    set(python_executable python${python_version})
+  else()
+    set(python_executable ${PYTHON_EXECUTABLE})
+  endif()
+  add_custom_target(pytest COMMAND ${python_executable} -m unittest discover -s caffe/test WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/python )
   add_dependencies(pytest pycaffe)
 endif()
 
@@ -121,3 +158,12 @@ caffe_print_configuration_summary()
 # ---[ Export configs generation
 caffe_generate_export_configs()
 
+SET(CMAKE_DEFAULT_STARTUP_PROJECT ssd_detect)
+
+install(DIRECTORY ${CMAKE_PREFIX_PATH}/include/ DESTINATION ${CMAKE_INSTALL_INCLUDEDIR})
+install(DIRECTORY ${CMAKE_PREFIX_PATH}/bin/ DESTINATION bin)
+install(DIRECTORY ${CMAKE_PREFIX_PATH}/lib/ DESTINATION lib)
+install(DIRECTORY ${CMAKE_PREFIX_PATH}/cmake/ DESTINATION cmake)
+install(DIRECTORY ${CMAKE_PREFIX_PATH}/share/ DESTINATION share)
+install(FILES ${CMAKE_PREFIX_PATH}/caffe-builder-config.cmake DESTINATION ${CMAKE_INSTALL_PREFIX})
+install(FILES ${CMAKE_PREFIX_PATH}/prependpath.bat DESTINATION bin)
diff --git a/VideoTest.mp4 b/VideoTest.mp4
new file mode 100644
index 0000000..e69de29
diff --git a/cmake/CaffeGetPrerequisites.cmake b/cmake/CaffeGetPrerequisites.cmake
new file mode 100644
index 0000000..bf5bc27
--- /dev/null
+++ b/cmake/CaffeGetPrerequisites.cmake
@@ -0,0 +1,1036 @@
+# Distributed under the OSI-approved BSD 3-Clause License.  See accompanying
+# file Copyright.txt or https://cmake.org/licensing for details.
+
+#.rst:
+# GetPrerequisites
+# ----------------
+#
+# Functions to analyze and list executable file prerequisites.
+#
+# This module provides functions to list the .dll, .dylib or .so files
+# that an executable or shared library file depends on.  (Its
+# prerequisites.)
+#
+# It uses various tools to obtain the list of required shared library
+# files:
+#
+# ::
+#
+#    dumpbin (Windows)
+#    objdump (MinGW on Windows)
+#    ldd (Linux/Unix)
+#    otool (Mac OSX)
+#
+# The following functions are provided by this module:
+#
+# ::
+#
+#    get_prerequisites
+#    list_prerequisites
+#    list_prerequisites_by_glob
+#    gp_append_unique
+#    is_file_executable
+#    gp_item_default_embedded_path
+#      (projects can override with gp_item_default_embedded_path_override)
+#    gp_resolve_item
+#      (projects can override with gp_resolve_item_override)
+#    gp_resolved_file_type
+#      (projects can override with gp_resolved_file_type_override)
+#    gp_file_type
+#
+# Requires CMake 2.6 or greater because it uses function, break, return
+# and PARENT_SCOPE.
+#
+# ::
+#
+#   GET_PREREQUISITES(<target> <prerequisites_var> <exclude_system> <recurse>
+#                     <exepath> <dirs> [<rpaths>])
+#
+# Get the list of shared library files required by <target>.  The list
+# in the variable named <prerequisites_var> should be empty on first
+# entry to this function.  On exit, <prerequisites_var> will contain the
+# list of required shared library files.
+#
+# <target> is the full path to an executable file.  <prerequisites_var>
+# is the name of a CMake variable to contain the results.
+# <exclude_system> must be 0 or 1 indicating whether to include or
+# exclude "system" prerequisites.  If <recurse> is set to 1 all
+# prerequisites will be found recursively, if set to 0 only direct
+# prerequisites are listed.  <exepath> is the path to the top level
+# executable used for @executable_path replacment on the Mac.  <dirs> is
+# a list of paths where libraries might be found: these paths are
+# searched first when a target without any path info is given.  Then
+# standard system locations are also searched: PATH, Framework
+# locations, /usr/lib...
+#
+# ::
+#
+#   LIST_PREREQUISITES(<target> [<recurse> [<exclude_system> [<verbose>]]])
+#
+# Print a message listing the prerequisites of <target>.
+#
+# <target> is the name of a shared library or executable target or the
+# full path to a shared library or executable file.  If <recurse> is set
+# to 1 all prerequisites will be found recursively, if set to 0 only
+# direct prerequisites are listed.  <exclude_system> must be 0 or 1
+# indicating whether to include or exclude "system" prerequisites.  With
+# <verbose> set to 0 only the full path names of the prerequisites are
+# printed, set to 1 extra informatin will be displayed.
+#
+# ::
+#
+#   LIST_PREREQUISITES_BY_GLOB(<glob_arg> <glob_exp>)
+#
+# Print the prerequisites of shared library and executable files
+# matching a globbing pattern.  <glob_arg> is GLOB or GLOB_RECURSE and
+# <glob_exp> is a globbing expression used with "file(GLOB" or
+# "file(GLOB_RECURSE" to retrieve a list of matching files.  If a
+# matching file is executable, its prerequisites are listed.
+#
+# Any additional (optional) arguments provided are passed along as the
+# optional arguments to the list_prerequisites calls.
+#
+# ::
+#
+#   GP_APPEND_UNIQUE(<list_var> <value>)
+#
+# Append <value> to the list variable <list_var> only if the value is
+# not already in the list.
+#
+# ::
+#
+#   IS_FILE_EXECUTABLE(<file> <result_var>)
+#
+# Return 1 in <result_var> if <file> is a binary executable, 0
+# otherwise.
+#
+# ::
+#
+#   GP_ITEM_DEFAULT_EMBEDDED_PATH(<item> <default_embedded_path_var>)
+#
+# Return the path that others should refer to the item by when the item
+# is embedded inside a bundle.
+#
+# Override on a per-project basis by providing a project-specific
+# gp_item_default_embedded_path_override function.
+#
+# ::
+#
+#   GP_RESOLVE_ITEM(<context> <item> <exepath> <dirs> <resolved_item_var>
+#                   [<rpaths>])
+#
+# Resolve an item into an existing full path file.
+#
+# Override on a per-project basis by providing a project-specific
+# gp_resolve_item_override function.
+#
+# ::
+#
+#   GP_RESOLVED_FILE_TYPE(<original_file> <file> <exepath> <dirs> <type_var>
+#                         [<rpaths>])
+#
+# Return the type of <file> with respect to <original_file>.  String
+# describing type of prerequisite is returned in variable named
+# <type_var>.
+#
+# Use <exepath> and <dirs> if necessary to resolve non-absolute <file>
+# values -- but only for non-embedded items.
+#
+# Possible types are:
+#
+# ::
+#
+#    system
+#    local
+#    embedded
+#    other
+#
+# Override on a per-project basis by providing a project-specific
+# gp_resolved_file_type_override function.
+#
+# ::
+#
+#   GP_FILE_TYPE(<original_file> <file> <type_var>)
+#
+# Return the type of <file> with respect to <original_file>.  String
+# describing type of prerequisite is returned in variable named
+# <type_var>.
+#
+# Possible types are:
+#
+# ::
+#
+#    system
+#    local
+#    embedded
+#    other
+
+function(gp_append_unique list_var value)
+  set(contains 0)
+
+  foreach(item ${${list_var}})
+    if(item STREQUAL "${value}")
+      set(contains 1)
+      break()
+    endif()
+  endforeach()
+
+  if(NOT contains)
+    set(${list_var} ${${list_var}} "${value}" PARENT_SCOPE)
+  endif()
+endfunction()
+
+
+function(is_file_executable file result_var)
+  #
+  # A file is not executable until proven otherwise:
+  #
+  set(${result_var} 0 PARENT_SCOPE)
+
+  get_filename_component(file_full "${file}" ABSOLUTE)
+  string(TOLOWER "${file_full}" file_full_lower)
+
+  # If file name ends in .exe on Windows, *assume* executable:
+  #
+  if(WIN32 AND NOT UNIX)
+    if("${file_full_lower}" MATCHES "\\.exe$")
+      set(${result_var} 1 PARENT_SCOPE)
+      return()
+    endif()
+
+    # A clause could be added here that uses output or return value of dumpbin
+    # to determine ${result_var}. In 99%+? practical cases, the exe name
+    # match will be sufficient...
+    #
+  endif()
+
+  # Use the information returned from the Unix shell command "file" to
+  # determine if ${file_full} should be considered an executable file...
+  #
+  # If the file command's output contains "executable" and does *not* contain
+  # "text" then it is likely an executable suitable for prerequisite analysis
+  # via the get_prerequisites macro.
+  #
+  if(UNIX)
+    if(NOT file_cmd)
+      find_program(file_cmd "file")
+      mark_as_advanced(file_cmd)
+    endif()
+
+    if(file_cmd)
+      execute_process(COMMAND "${file_cmd}" "${file_full}"
+        RESULT_VARIABLE file_rv
+        OUTPUT_VARIABLE file_ov
+        ERROR_VARIABLE file_ev
+        OUTPUT_STRIP_TRAILING_WHITESPACE
+        )
+      if(NOT file_rv STREQUAL "0")
+        message(FATAL_ERROR "${file_cmd} failed: ${file_rv}\n${file_ev}")
+      endif()
+
+      # Replace the name of the file in the output with a placeholder token
+      # (the string " _file_full_ ") so that just in case the path name of
+      # the file contains the word "text" or "executable" we are not fooled
+      # into thinking "the wrong thing" because the file name matches the
+      # other 'file' command output we are looking for...
+      #
+      string(REPLACE "${file_full}" " _file_full_ " file_ov "${file_ov}")
+      string(TOLOWER "${file_ov}" file_ov)
+
+      #message(STATUS "file_ov='${file_ov}'")
+      if("${file_ov}" MATCHES "executable")
+        #message(STATUS "executable!")
+        if("${file_ov}" MATCHES "text")
+          #message(STATUS "but text, so *not* a binary executable!")
+        else()
+          set(${result_var} 1 PARENT_SCOPE)
+          return()
+        endif()
+      endif()
+
+      # Also detect position independent executables on Linux,
+      # where "file" gives "shared object ... (uses shared libraries)"
+      if("${file_ov}" MATCHES "shared object.*\(uses shared libs\)")
+        set(${result_var} 1 PARENT_SCOPE)
+        return()
+      endif()
+
+      # "file" version 5.22 does not print "(used shared libraries)"
+      # but uses "interpreter"
+      if("${file_ov}" MATCHES "shared object.*interpreter")
+        set(${result_var} 1 PARENT_SCOPE)
+        return()
+      endif()
+
+    else()
+      message(STATUS "warning: No 'file' command, skipping execute_process...")
+    endif()
+  endif()
+endfunction()
+
+
+function(gp_item_default_embedded_path item default_embedded_path_var)
+
+  # On Windows and Linux, "embed" prerequisites in the same directory
+  # as the executable by default:
+  #
+  set(path "@executable_path")
+  set(overridden 0)
+
+  # On the Mac, relative to the executable depending on the type
+  # of the thing we are embedding:
+  #
+  if(APPLE)
+    #
+    # The assumption here is that all executables in the bundle will be
+    # in same-level-directories inside the bundle. The parent directory
+    # of an executable inside the bundle should be MacOS or a sibling of
+    # MacOS and all embedded paths returned from here will begin with
+    # "@executable_path/../" and will work from all executables in all
+    # such same-level-directories inside the bundle.
+    #
+
+    # By default, embed things right next to the main bundle executable:
+    #
+    set(path "@executable_path/../../Contents/MacOS")
+
+    # Embed .dylibs right next to the main bundle executable:
+    #
+    if(item MATCHES "\\.dylib$")
+      set(path "@executable_path/../MacOS")
+      set(overridden 1)
+    endif()
+
+    # Embed frameworks in the embedded "Frameworks" directory (sibling of MacOS):
+    #
+    if(NOT overridden)
+      if(item MATCHES "[^/]+\\.framework/")
+        set(path "@executable_path/../Frameworks")
+        set(overridden 1)
+      endif()
+    endif()
+  endif()
+
+  # Provide a hook so that projects can override the default embedded location
+  # of any given library by whatever logic they choose:
+  #
+  if(COMMAND gp_item_default_embedded_path_override)
+    gp_item_default_embedded_path_override("${item}" path)
+  endif()
+
+  set(${default_embedded_path_var} "${path}" PARENT_SCOPE)
+endfunction()
+
+
+function(gp_resolve_item context item exepath dirs resolved_item_var)
+  set(resolved 0)
+  set(resolved_item "${item}")
+  if(ARGC GREATER 5)
+    set(rpaths "${ARGV5}")
+  else()
+    set(rpaths "")
+  endif()
+
+  # Is it already resolved?
+  #
+  if(IS_ABSOLUTE "${resolved_item}" AND EXISTS "${resolved_item}")
+    set(resolved 1)
+  endif()
+
+  if(NOT resolved)
+    if(item MATCHES "^@executable_path")
+      #
+      # @executable_path references are assumed relative to exepath
+      #
+      string(REPLACE "@executable_path" "${exepath}" ri "${item}")
+      get_filename_component(ri "${ri}" ABSOLUTE)
+
+      if(EXISTS "${ri}")
+        #message(STATUS "info: embedded item exists (${ri})")
+        set(resolved 1)
+        set(resolved_item "${ri}")
+      else()
+        message(STATUS "warning: embedded item does not exist '${ri}'")
+      endif()
+    endif()
+  endif()
+
+  if(NOT resolved)
+    if(item MATCHES "^@loader_path")
+      #
+      # @loader_path references are assumed relative to the
+      # PATH of the given "context" (presumably another library)
+      #
+      get_filename_component(contextpath "${context}" PATH)
+      string(REPLACE "@loader_path" "${contextpath}" ri "${item}")
+      get_filename_component(ri "${ri}" ABSOLUTE)
+
+      if(EXISTS "${ri}")
+        #message(STATUS "info: embedded item exists (${ri})")
+        set(resolved 1)
+        set(resolved_item "${ri}")
+      else()
+        message(STATUS "warning: embedded item does not exist '${ri}'")
+      endif()
+    endif()
+  endif()
+
+  if(NOT resolved)
+    if(item MATCHES "^@rpath")
+      #
+      # @rpath references are relative to the paths built into the binaries with -rpath
+      # We handle this case like we do for other Unixes
+      #
+      string(REPLACE "@rpath/" "" norpath_item "${item}")
+
+      set(ri "ri-NOTFOUND")
+      find_file(ri "${norpath_item}" ${exepath} ${dirs} ${rpaths} NO_DEFAULT_PATH)
+      if(ri)
+        #message(STATUS "info: 'find_file' in exepath/dirs/rpaths (${ri})")
+        set(resolved 1)
+        set(resolved_item "${ri}")
+        set(ri "ri-NOTFOUND")
+      endif()
+
+    endif()
+  endif()
+
+  if(NOT resolved)
+    set(ri "ri-NOTFOUND")
+    find_file(ri "${item}" ${exepath} ${dirs} NO_DEFAULT_PATH)
+    find_file(ri "${item}" ${exepath} ${dirs} /usr/lib)
+    if(ri)
+      #message(STATUS "info: 'find_file' in exepath/dirs (${ri})")
+      set(resolved 1)
+      set(resolved_item "${ri}")
+      set(ri "ri-NOTFOUND")
+    endif()
+  endif()
+
+  if(NOT resolved)
+    if(item MATCHES "[^/]+\\.framework/")
+      set(fw "fw-NOTFOUND")
+      find_file(fw "${item}"
+        "~/Library/Frameworks"
+        "/Library/Frameworks"
+        "/System/Library/Frameworks"
+      )
+      if(fw)
+        #message(STATUS "info: 'find_file' found framework (${fw})")
+        set(resolved 1)
+        set(resolved_item "${fw}")
+        set(fw "fw-NOTFOUND")
+      endif()
+    endif()
+  endif()
+
+  # Using find_program on Windows will find dll files that are in the PATH.
+  # (Converting simple file names into full path names if found.)
+  #
+  if(WIN32 AND NOT UNIX)
+  if(NOT resolved)
+    set(ri "ri-NOTFOUND")
+    find_program(ri "${item}" PATHS ${exepath} ${dirs} NO_DEFAULT_PATH)
+    find_program(ri "${item}" PATHS ${exepath} ${dirs})
+    if(ri)
+      #message(STATUS "info: 'find_program' in exepath/dirs (${ri})")
+      set(resolved 1)
+      set(resolved_item "${ri}")
+      set(ri "ri-NOTFOUND")
+    endif()
+  endif()
+  endif()
+
+  # Provide a hook so that projects can override item resolution
+  # by whatever logic they choose:
+  #
+  if(COMMAND gp_resolve_item_override)
+    gp_resolve_item_override("${context}" "${item}" "${exepath}" "${dirs}" resolved_item resolved)
+  endif()
+
+  if(NOT resolved)
+    message(STATUS "
+warning: cannot resolve item '${item}'
+
+  possible problems:
+    need more directories?
+    need to use InstallRequiredSystemLibraries?
+    run in install tree instead of build tree?
+")
+#    message(STATUS "
+#******************************************************************************
+#warning: cannot resolve item '${item}'
+#
+#  possible problems:
+#    need more directories?
+#    need to use InstallRequiredSystemLibraries?
+#    run in install tree instead of build tree?
+#
+#    context='${context}'
+#    item='${item}'
+#    exepath='${exepath}'
+#    dirs='${dirs}'
+#    resolved_item_var='${resolved_item_var}'
+#******************************************************************************
+#")
+  endif()
+
+  set(${resolved_item_var} "${resolved_item}" PARENT_SCOPE)
+endfunction()
+
+
+function(gp_resolved_file_type original_file file exepath dirs type_var)
+  if(ARGC GREATER 5)
+    set(rpaths "${ARGV5}")
+  else()
+    set(rpaths "")
+  endif()
+  #message(STATUS "**")
+
+  if(NOT IS_ABSOLUTE "${original_file}")
+    message(STATUS "warning: gp_resolved_file_type expects absolute full path for first arg original_file")
+  endif()
+  if(IS_ABSOLUTE "${original_file}")
+    get_filename_component(original_file "${original_file}" ABSOLUTE) # canonicalize path
+  endif()
+
+  set(is_embedded 0)
+  set(is_local 0)
+  set(is_system 0)
+
+  set(resolved_file "${file}")
+
+  if("${file}" MATCHES "^@(executable|loader)_path")
+    set(is_embedded 1)
+  endif()
+
+  if(NOT is_embedded)
+    if(NOT IS_ABSOLUTE "${file}")
+      gp_resolve_item("${original_file}" "${file}" "${exepath}" "${dirs}" resolved_file "${rpaths}")
+    endif()
+    if(IS_ABSOLUTE "${resolved_file}")
+      get_filename_component(resolved_file "${resolved_file}" ABSOLUTE) # canonicalize path
+    endif()
+
+    string(TOLOWER "${original_file}" original_lower)
+    string(TOLOWER "${resolved_file}" lower)
+
+    if(UNIX)
+      if(resolved_file MATCHES "^(/lib/|/lib32/|/lib64/|/usr/lib/|/usr/lib32/|/usr/lib64/|/usr/X11R6/|/usr/bin/)")
+        set(is_system 1)
+      endif()
+    endif()
+
+    if(APPLE)
+      if(resolved_file MATCHES "^(/System/Library/|/usr/lib/)")
+        set(is_system 1)
+      endif()
+    endif()
+
+    if(WIN32)
+      string(TOLOWER "$ENV{SystemRoot}" sysroot)
+      file(TO_CMAKE_PATH "${sysroot}" sysroot)
+
+      string(TOLOWER "$ENV{windir}" windir)
+      file(TO_CMAKE_PATH "${windir}" windir)
+
+      if(lower MATCHES "^(${sysroot}/sys(tem|wow)|${windir}/sys(tem|wow)|(.*/)*(msvc|api-ms-win-)[^/]+dll)")
+        set(is_system 1)
+      endif()
+
+      if(UNIX)
+        # if cygwin, we can get the properly formed windows paths from cygpath
+        find_program(CYGPATH_EXECUTABLE cygpath)
+
+        if(CYGPATH_EXECUTABLE)
+          execute_process(COMMAND ${CYGPATH_EXECUTABLE} -W
+                          RESULT_VARIABLE env_rv
+                          OUTPUT_VARIABLE env_windir
+                          ERROR_VARIABLE env_ev
+                          OUTPUT_STRIP_TRAILING_WHITESPACE)
+          if(NOT env_rv STREQUAL "0")
+            message(FATAL_ERROR "${CYGPATH_EXECUTABLE} -W failed: ${env_rv}\n${env_ev}")
+          endif()
+          execute_process(COMMAND ${CYGPATH_EXECUTABLE} -S
+                          RESULT_VARIABLE env_rv
+                          OUTPUT_VARIABLE env_sysdir
+                          ERROR_VARIABLE env_ev
+                          OUTPUT_STRIP_TRAILING_WHITESPACE)
+          if(NOT env_rv STREQUAL "0")
+            message(FATAL_ERROR "${CYGPATH_EXECUTABLE} -S failed: ${env_rv}\n${env_ev}")
+          endif()
+          string(TOLOWER "${env_windir}" windir)
+          string(TOLOWER "${env_sysdir}" sysroot)
+
+          if(lower MATCHES "^(${sysroot}/sys(tem|wow)|${windir}/sys(tem|wow)|(.*/)*(msvc|api-ms-win-)[^/]+dll)")
+            set(is_system 1)
+          endif()
+        endif()
+      endif()
+    endif()
+
+    if(NOT is_system)
+      get_filename_component(original_path "${original_lower}" PATH)
+      get_filename_component(path "${lower}" PATH)
+      if(original_path STREQUAL path)
+        set(is_local 1)
+      else()
+        string(LENGTH "${original_path}/" original_length)
+        string(LENGTH "${lower}" path_length)
+        if(${path_length} GREATER ${original_length})
+          string(SUBSTRING "${lower}" 0 ${original_length} path)
+          if("${original_path}/" STREQUAL path)
+            set(is_embedded 1)
+          endif()
+        endif()
+      endif()
+    endif()
+  endif()
+
+  # Return type string based on computed booleans:
+  #
+  set(type "other")
+
+  if(is_system)
+    set(type "system")
+  elseif(is_embedded)
+    set(type "embedded")
+  elseif(is_local)
+    set(type "local")
+  endif()
+
+  #message(STATUS "gp_resolved_file_type: '${file}' '${resolved_file}'")
+  #message(STATUS "                type: '${type}'")
+
+  if(NOT is_embedded)
+    if(NOT IS_ABSOLUTE "${resolved_file}")
+      if(lower MATCHES "^msvc[^/]+dll" AND is_system)
+        message(STATUS "info: non-absolute msvc file '${file}' returning type '${type}'")
+      else()
+        message(STATUS "warning: gp_resolved_file_type non-absolute file '${file}' returning type '${type}' -- possibly incorrect")
+      endif()
+    endif()
+  endif()
+
+  # Provide a hook so that projects can override the decision on whether a
+  # library belongs to the system or not by whatever logic they choose:
+  #
+  if(COMMAND gp_resolved_file_type_override)
+    gp_resolved_file_type_override("${resolved_file}" type)
+  endif()
+
+  set(${type_var} "${type}" PARENT_SCOPE)
+
+  #message(STATUS "**")
+endfunction()
+
+
+function(gp_file_type original_file file type_var)
+  if(NOT IS_ABSOLUTE "${original_file}")
+    message(STATUS "warning: gp_file_type expects absolute full path for first arg original_file")
+  endif()
+
+  get_filename_component(exepath "${original_file}" PATH)
+
+  set(type "")
+  gp_resolved_file_type("${original_file}" "${file}" "${exepath}" "" type)
+
+  set(${type_var} "${type}" PARENT_SCOPE)
+endfunction()
+
+
+function(get_prerequisites target prerequisites_var exclude_system recurse exepath dirs)
+  set(verbose 0)
+  set(eol_char "E")
+  if(ARGC GREATER 6)
+    set(rpaths "${ARGV6}")
+  else()
+    set(rpaths "")
+  endif()
+
+  if(NOT IS_ABSOLUTE "${target}")
+    message("warning: target '${target}' is not absolute...")
+  endif()
+
+  if(NOT EXISTS "${target}")
+    message("warning: target '${target}' does not exist...")
+    set(${prerequisites_var} "" PARENT_SCOPE)
+    return()
+  endif()
+
+  set(gp_cmd_paths ${gp_cmd_paths}
+    "[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\VisualStudio\\14.0;InstallDir]/../../VC/bin"
+    "$ENV{VS140COMNTOOLS}/../../VC/bin"
+    "C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin"
+    "[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\VisualStudio\\12.0;InstallDir]/../../VC/bin"
+    "$ENV{VS120COMNTOOLS}/../../VC/bin"
+    "C:/Program Files (x86)/Microsoft Visual Studio 12.0/VC/bin"
+    "[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\VisualStudio\\11.0;InstallDir]/../../VC/bin"
+    "$ENV{VS110COMNTOOLS}/../../VC/bin"
+    "C:/Program Files (x86)/Microsoft Visual Studio 11.0/VC/bin"
+    "[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\VisualStudio\\10.0;InstallDir]/../../VC/bin"
+    "$ENV{VS100COMNTOOLS}/../../VC/bin"
+    "C:/Program Files (x86)/Microsoft Visual Studio 10.0/VC/bin"
+    "[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\VisualStudio\\9.0;InstallDir]/../../VC/bin"
+    "$ENV{VS90COMNTOOLS}/../../VC/bin"
+    "C:/Program Files/Microsoft Visual Studio 9.0/VC/bin"
+    "C:/Program Files (x86)/Microsoft Visual Studio 9.0/VC/bin"
+    "[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\VisualStudio\\8.0;InstallDir]/../../VC/bin"
+    "$ENV{VS80COMNTOOLS}/../../VC/bin"
+    "C:/Program Files/Microsoft Visual Studio 8/VC/BIN"
+    "C:/Program Files (x86)/Microsoft Visual Studio 8/VC/BIN"
+    "[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\VisualStudio\\7.1;InstallDir]/../../VC7/bin"
+    "$ENV{VS71COMNTOOLS}/../../VC7/bin"
+    "C:/Program Files/Microsoft Visual Studio .NET 2003/VC7/BIN"
+    "C:/Program Files (x86)/Microsoft Visual Studio .NET 2003/VC7/BIN"
+    "/usr/local/bin"
+    "/usr/bin"
+    )
+
+  # <setup-gp_tool-vars>
+  #
+  # Try to choose the right tool by default. Caller can set gp_tool prior to
+  # calling this function to force using a different tool.
+  #
+  if(NOT gp_tool)
+    set(gp_tool "ldd")
+
+    if(APPLE)
+      set(gp_tool "otool")
+    endif()
+
+    if(WIN32 AND NOT UNIX) # This is how to check for cygwin, har!
+      find_program(gp_dumpbin "dumpbin" PATHS ${gp_cmd_paths})
+      if(gp_dumpbin)
+        set(gp_tool "dumpbin")
+      else() # Try harder. Maybe we're on MinGW
+        set(gp_tool "objdump")
+      endif()
+    endif()
+  endif()
+
+  find_program(gp_cmd ${gp_tool} PATHS ${gp_cmd_paths})
+
+  if(NOT gp_cmd)
+    message(STATUS "warning: could not find '${gp_tool}' - cannot analyze prerequisites...")
+    return()
+  endif()
+
+  set(gp_cmd_maybe_filter)      # optional command to pre-filter gp_tool results
+
+  if(gp_tool STREQUAL "ldd")
+    set(gp_cmd_args "")
+    set(gp_regex "^[\t ]*[^\t ]+ => ([^\t\(]+) .*${eol_char}$")
+    set(gp_regex_error "not found${eol_char}$")
+    set(gp_regex_fallback "^[\t ]*([^\t ]+) => ([^\t ]+).*${eol_char}$")
+    set(gp_regex_cmp_count 1)
+  elseif(gp_tool STREQUAL "otool")
+    set(gp_cmd_args "-L")
+    set(gp_regex "^\t([^\t]+) \\(compatibility version ([0-9]+.[0-9]+.[0-9]+), current version ([0-9]+.[0-9]+.[0-9]+)\\)${eol_char}$")
+    set(gp_regex_error "")
+    set(gp_regex_fallback "")
+    set(gp_regex_cmp_count 3)
+  elseif(gp_tool STREQUAL "dumpbin")
+    set(gp_cmd_args "/dependents")
+    set(gp_regex "^    ([^ ].*[Dd][Ll][Ll])${eol_char}$")
+    set(gp_regex_error "")
+    set(gp_regex_fallback "")
+    set(gp_regex_cmp_count 1)
+  elseif(gp_tool STREQUAL "objdump")
+    set(gp_cmd_args "-p")
+    set(gp_regex "^\t*DLL Name: (.*\\.[Dd][Ll][Ll])${eol_char}$")
+    set(gp_regex_error "")
+    set(gp_regex_fallback "")
+    set(gp_regex_cmp_count 1)
+    # objdump generates copious output so we create a grep filter to pre-filter results
+    if(WIN32)
+      find_program(gp_grep_cmd findstr)
+    else()
+      find_program(gp_grep_cmd grep)
+    endif()
+    if(gp_grep_cmd)
+      set(gp_cmd_maybe_filter COMMAND ${gp_grep_cmd} "-a" "^[[:blank:]]*DLL Name: ")
+    endif()
+  else()
+    message(STATUS "warning: gp_tool='${gp_tool}' is an unknown tool...")
+    message(STATUS "CMake function get_prerequisites needs more code to handle '${gp_tool}'")
+    message(STATUS "Valid gp_tool values are dumpbin, ldd, objdump and otool.")
+    return()
+  endif()
+
+
+  if(gp_tool STREQUAL "dumpbin")
+    # When running dumpbin, it also needs the "Common7/IDE" directory in the
+    # PATH. It will already be in the PATH if being run from a Visual Studio
+    # command prompt. Add it to the PATH here in case we are running from a
+    # different command prompt.
+    #
+    get_filename_component(gp_cmd_dir "${gp_cmd}" PATH)
+    get_filename_component(gp_cmd_dlls_dir "${gp_cmd_dir}/../../Common7/IDE" ABSOLUTE)
+    # Use cmake paths as a user may have a PATH element ending with a backslash.
+    # This will escape the list delimiter and create havoc!
+    if(EXISTS "${gp_cmd_dlls_dir}")
+      # only add to the path if it is not already in the path
+      set(gp_found_cmd_dlls_dir 0)
+      file(TO_CMAKE_PATH "$ENV{PATH}" env_path)
+      foreach(gp_env_path_element ${env_path})
+        if(gp_env_path_element STREQUAL gp_cmd_dlls_dir)
+          set(gp_found_cmd_dlls_dir 1)
+        endif()
+      endforeach()
+
+      if(NOT gp_found_cmd_dlls_dir)
+        file(TO_NATIVE_PATH "${gp_cmd_dlls_dir}" gp_cmd_dlls_dir)
+        set(ENV{PATH} "$ENV{PATH};${gp_cmd_dlls_dir}")
+      endif()
+    endif()
+  endif()
+  #
+  # </setup-gp_tool-vars>
+
+  if(gp_tool STREQUAL "ldd")
+    set(old_ld_env "$ENV{LD_LIBRARY_PATH}")
+    set(new_ld_env "${exepath}")
+    foreach(dir ${dirs})
+      string(APPEND new_ld_env ":${dir}")
+    endforeach()
+    set(ENV{LD_LIBRARY_PATH} "${new_ld_env}:$ENV{LD_LIBRARY_PATH}")
+  endif()
+
+
+  # Track new prerequisites at each new level of recursion. Start with an
+  # empty list at each level:
+  #
+  set(unseen_prereqs)
+
+  # Run gp_cmd on the target:
+  #
+  execute_process(
+    COMMAND ${gp_cmd} ${gp_cmd_args} ${target}
+    ${gp_cmd_maybe_filter}
+    RESULT_VARIABLE gp_rv
+    OUTPUT_VARIABLE gp_cmd_ov
+    ERROR_VARIABLE gp_ev
+    )
+
+  if(gp_tool STREQUAL "dumpbin")
+    # Exclude delay load dependencies under windows (they are listed in dumpbin output after the message below)
+    string(FIND "${gp_cmd_ov}" "Image has the following delay load dependencies" gp_delayload_pos)
+    if (${gp_delayload_pos} GREATER -1)
+      string(SUBSTRING "${gp_cmd_ov}" 0 ${gp_delayload_pos} gp_cmd_ov_no_delayload_deps)
+      string(SUBSTRING "${gp_cmd_ov}" ${gp_delayload_pos} -1 gp_cmd_ov_delayload_deps)
+      if (verbose)
+        message(STATUS "GetPrequisites(${target}) : ignoring the following delay load dependencies :\n ${gp_cmd_ov_delayload_deps}")
+      endif()
+      set(gp_cmd_ov ${gp_cmd_ov_no_delayload_deps})
+    endif()
+  endif()
+
+  if(NOT gp_rv STREQUAL "0")
+    if(gp_tool STREQUAL "dumpbin")
+      # dumpbin error messages seem to go to stdout
+      message(FATAL_ERROR "${gp_cmd} failed: ${gp_rv}\n${gp_ev}\n${gp_cmd_ov}")
+    else()
+      message(FATAL_ERROR "${gp_cmd} failed: ${gp_rv}\n${gp_ev}")
+    endif()
+  endif()
+
+  if(gp_tool STREQUAL "ldd")
+    set(ENV{LD_LIBRARY_PATH} "${old_ld_env}")
+  endif()
+
+  if(verbose)
+    message(STATUS "<RawOutput cmd='${gp_cmd} ${gp_cmd_args} ${target}'>")
+    message(STATUS "gp_cmd_ov='${gp_cmd_ov}'")
+    message(STATUS "</RawOutput>")
+  endif()
+
+  get_filename_component(target_dir "${target}" PATH)
+
+  # Convert to a list of lines:
+  #
+  string(REPLACE ";" "\\;" candidates "${gp_cmd_ov}")
+  string(REPLACE "\n" "${eol_char};" candidates "${candidates}")
+
+  # check for install id and remove it from list, since otool -L can include a
+  # reference to itself
+  set(gp_install_id)
+  if(gp_tool STREQUAL "otool")
+    execute_process(
+      COMMAND otool -D ${target}
+      RESULT_VARIABLE otool_rv
+      OUTPUT_VARIABLE gp_install_id_ov
+      ERROR_VARIABLE otool_ev
+      )
+    if(NOT otool_rv STREQUAL "0")
+      message(FATAL_ERROR "otool -D failed: ${otool_rv}\n${otool_ev}")
+    endif()
+    # second line is install name
+    string(REGEX REPLACE ".*:\n" "" gp_install_id "${gp_install_id_ov}")
+    if(gp_install_id)
+      # trim
+      string(REGEX MATCH "[^\n ].*[^\n ]" gp_install_id "${gp_install_id}")
+      #message("INSTALL ID is \"${gp_install_id}\"")
+    endif()
+  endif()
+
+  # Analyze each line for file names that match the regular expression:
+  #
+  foreach(candidate ${candidates})
+  if("${candidate}" MATCHES "${gp_regex}")
+
+    # Extract information from each candidate:
+    if(gp_regex_error AND "${candidate}" MATCHES "${gp_regex_error}")
+      string(REGEX REPLACE "${gp_regex_fallback}" "\\1" raw_item "${candidate}")
+    else()
+      string(REGEX REPLACE "${gp_regex}" "\\1" raw_item "${candidate}")
+    endif()
+
+    if(gp_regex_cmp_count GREATER 1)
+      string(REGEX REPLACE "${gp_regex}" "\\2" raw_compat_version "${candidate}")
+      string(REGEX REPLACE "^([0-9]+)\\.([0-9]+)\\.([0-9]+)$" "\\1" compat_major_version "${raw_compat_version}")
+      string(REGEX REPLACE "^([0-9]+)\\.([0-9]+)\\.([0-9]+)$" "\\2" compat_minor_version "${raw_compat_version}")
+      string(REGEX REPLACE "^([0-9]+)\\.([0-9]+)\\.([0-9]+)$" "\\3" compat_patch_version "${raw_compat_version}")
+    endif()
+
+    if(gp_regex_cmp_count GREATER 2)
+      string(REGEX REPLACE "${gp_regex}" "\\3" raw_current_version "${candidate}")
+      string(REGEX REPLACE "^([0-9]+)\\.([0-9]+)\\.([0-9]+)$" "\\1" current_major_version "${raw_current_version}")
+      string(REGEX REPLACE "^([0-9]+)\\.([0-9]+)\\.([0-9]+)$" "\\2" current_minor_version "${raw_current_version}")
+      string(REGEX REPLACE "^([0-9]+)\\.([0-9]+)\\.([0-9]+)$" "\\3" current_patch_version "${raw_current_version}")
+    endif()
+
+    # Use the raw_item as the list entries returned by this function. Use the
+    # gp_resolve_item function to resolve it to an actual full path file if
+    # necessary.
+    #
+    set(item "${raw_item}")
+
+    # Add each item unless it is excluded:
+    #
+    set(add_item 1)
+
+    if(item STREQUAL gp_install_id)
+      set(add_item 0)
+    endif()
+
+    if(add_item AND ${exclude_system})
+      set(type "")
+      gp_resolved_file_type("${target}" "${item}" "${exepath}" "${dirs}" type "${rpaths}")
+
+      if(type STREQUAL "system")
+        set(add_item 0)
+      endif()
+    endif()
+
+    if(add_item)
+      list(LENGTH ${prerequisites_var} list_length_before_append)
+      gp_append_unique(${prerequisites_var} "${item}")
+      list(LENGTH ${prerequisites_var} list_length_after_append)
+
+      if(${recurse})
+        # If item was really added, this is the first time we have seen it.
+        # Add it to unseen_prereqs so that we can recursively add *its*
+        # prerequisites...
+        #
+        # But first: resolve its name to an absolute full path name such
+        # that the analysis tools can simply accept it as input.
+        #
+        if(NOT list_length_before_append EQUAL list_length_after_append)
+          gp_resolve_item("${target}" "${item}" "${exepath}" "${dirs}" resolved_item "${rpaths}")
+          if(EXISTS ${resolved_item})
+            # Recurse only if we could resolve the item.
+	    # Otherwise the prerequisites_var list will be cleared
+            set(unseen_prereqs ${unseen_prereqs} "${resolved_item}")
+          endif()
+        endif()
+      endif()
+    endif()
+  else()
+    if(verbose)
+      message(STATUS "ignoring non-matching line: '${candidate}'")
+    endif()
+  endif()
+  endforeach()
+
+  list(LENGTH ${prerequisites_var} prerequisites_var_length)
+  if(prerequisites_var_length GREATER 0)
+    list(SORT ${prerequisites_var})
+  endif()
+  if(${recurse})
+    set(more_inputs ${unseen_prereqs})
+    foreach(input ${more_inputs})
+      get_prerequisites("${input}" ${prerequisites_var} ${exclude_system} ${recurse} "${exepath}" "${dirs}" "${rpaths}")
+    endforeach()
+  endif()
+
+  set(${prerequisites_var} ${${prerequisites_var}} PARENT_SCOPE)
+endfunction()
+
+
+function(list_prerequisites target)
+  if(ARGC GREATER 1 AND NOT "${ARGV1}" STREQUAL "")
+    set(all "${ARGV1}")
+  else()
+    set(all 1)
+  endif()
+
+  if(ARGC GREATER 2 AND NOT "${ARGV2}" STREQUAL "")
+    set(exclude_system "${ARGV2}")
+  else()
+    set(exclude_system 0)
+  endif()
+
+  if(ARGC GREATER 3 AND NOT "${ARGV3}" STREQUAL "")
+    set(verbose "${ARGV3}")
+  else()
+    set(verbose 0)
+  endif()
+
+  set(count 0)
+  set(count_str "")
+  set(print_count "${verbose}")
+  set(print_prerequisite_type "${verbose}")
+  set(print_target "${verbose}")
+  set(type_str "")
+
+  get_filename_component(exepath "${target}" PATH)
+
+  set(prereqs "")
+  get_prerequisites("${target}" prereqs ${exclude_system} ${all} "${exepath}" "")
+
+  if(print_target)
+    message(STATUS "File '${target}' depends on:")
+  endif()
+
+  foreach(d ${prereqs})
+    math(EXPR count "${count} + 1")
+
+    if(print_count)
+      set(count_str "${count}. ")
+    endif()
+
+    if(print_prerequisite_type)
+      gp_file_type("${target}" "${d}" type)
+      set(type_str " (${type})")
+    endif()
+
+    message(STATUS "${count_str}${d}${type_str}")
+  endforeach()
+endfunction()
+
+
+function(list_prerequisites_by_glob glob_arg glob_exp)
+  message(STATUS "=============================================================================")
+  message(STATUS "List prerequisites of executables matching ${glob_arg} '${glob_exp}'")
+  message(STATUS "")
+  file(${glob_arg} file_list ${glob_exp})
+  foreach(f ${file_list})
+    is_file_executable("${f}" is_f_executable)
+    if(is_f_executable)
+      message(STATUS "=============================================================================")
+      list_prerequisites("${f}" ${ARGN})
+      message(STATUS "")
+    endif()
+  endforeach()
+endfunction()
diff --git a/cmake/ConfigGen.cmake b/cmake/ConfigGen.cmake
index 09bb09b..1664823 100644
--- a/cmake/ConfigGen.cmake
+++ b/cmake/ConfigGen.cmake
@@ -24,12 +24,55 @@ function(caffe_generate_export_configs)
     set(HAVE_CUDA FALSE)
   endif()
 
+  set(GFLAGS_IMPORTED OFF)
+  foreach(_lib ${GFLAGS_LIBRARIES})
+    if(TARGET ${_lib})
+      set(GFLAGS_IMPORTED ON)
+    endif()
+  endforeach()
+
+  set(GLOG_IMPORTED OFF)
+  foreach(_lib ${GLOG_LIBRARIES})
+    if(TARGET ${_lib})
+      set(GLOG_IMPORTED ON)
+    endif()
+  endforeach()
+
+  set(HDF5_IMPORTED OFF)
+  foreach(_lib ${HDF5_LIBRARIES} ${HDF5_HL_LIBRARIES})
+    if(TARGET ${_lib})
+      set(HDF5_IMPORTED ON)
+    endif()
+  endforeach()
+
+  set(LMDB_IMPORTED OFF)
+  if(USE_LMDB)
+    foreach(_lib ${LMDB_LIBRARIES})
+      if(TARGET ${_lib})
+        set(LMDB_IMPORTED ON)
+      endif()
+    endforeach()
+  endif()
+  set(LEVELDB_IMPORTED OFF)
+  set(SNAPPY_IMPORTED OFF)
+  if(USE_LEVELDB)
+    foreach(_lib ${LevelDB_LIBRARIES})
+      if(TARGET ${_lib})
+        set(LEVELDB_IMPORTED ON)
+      endif()
+    endforeach()
+    foreach(_lib ${Snappy_LIBRARIES})
+      if(TARGET ${_lib})
+        set(SNAPPY_IMPORTED ON)
+      endif()
+    endforeach()
+  endif()
+
   if(NOT HAVE_CUDNN)
     set(HAVE_CUDNN FALSE)
   endif()
-
+  
   # ---[ Configure build-tree CaffeConfig.cmake file ]---
-
   configure_file("cmake/Templates/CaffeConfig.cmake.in" "${PROJECT_BINARY_DIR}/CaffeConfig.cmake" @ONLY)
 
   # Add targets to the build-tree export set
@@ -37,7 +80,6 @@ function(caffe_generate_export_configs)
   export(PACKAGE Caffe)
 
   # ---[ Configure install-tree CaffeConfig.cmake file ]---
-
   configure_file("cmake/Templates/CaffeConfig.cmake.in" "${PROJECT_BINARY_DIR}/cmake/CaffeConfig.cmake" @ONLY)
 
   # Install the CaffeConfig.cmake and export set to use with install-tree
diff --git a/cmake/Cuda.cmake b/cmake/Cuda.cmake
index e03feab..7c335ac 100644
--- a/cmake/Cuda.cmake
+++ b/cmake/Cuda.cmake
@@ -2,16 +2,14 @@ if(CPU_ONLY)
   return()
 endif()
 
-# Known NVIDIA GPU achitectures Caffe can be compiled for.
-# This list will be used for CUDA_ARCH_NAME = All option
-set(Caffe_known_gpu_archs "20 21(20) 30 35 50 60 61")
-
 ################################################################################################
 # A function for automatic detection of GPUs installed  (if autodetection is enabled)
 # Usage:
 #   caffe_detect_installed_gpus(out_variable)
 function(caffe_detect_installed_gpus out_variable)
+message(STATUS "caffe_detect_installed_gpus()")
   if(NOT CUDA_gpu_detect_output)
+message(STATUS "  NOT CUDA_gpu_detect_output")
     set(__cufile ${PROJECT_BINARY_DIR}/detect_cuda_archs.cu)
 
     file(WRITE ${__cufile} ""
@@ -35,9 +33,14 @@ function(caffe_detect_installed_gpus out_variable)
                     RESULT_VARIABLE __nvcc_res OUTPUT_VARIABLE __nvcc_out
                     ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE)
 
+message(STATUS "  __nvcc_res : " ${__nvcc_res})
+message(STATUS "  __nvcc_out : " ${__nvcc_out})
     if(__nvcc_res EQUAL 0)
-      string(REPLACE "2.1" "2.1(2.0)" __nvcc_out "${__nvcc_out}")
-      set(CUDA_gpu_detect_output ${__nvcc_out} CACHE INTERNAL "Returned GPU architetures from caffe_detect_gpus tool" FORCE)
+      # nvcc outputs text containing line breaks when building with MSVC.
+      # The line below prevents CMake from inserting a variable with line
+      # breaks in the cache
+      string(REGEX MATCH "([1-9].[0-9])" __nvcc_out "${__nvcc_out}")
+      set(CUDA_gpu_detect_output ${__nvcc_out} CACHE INTERNAL "Returned GPU architetures from caffe_detect_gpus tool" FORCE)      
     endif()
   endif()
 
@@ -56,7 +59,7 @@ endfunction()
 #   caffe_select_nvcc_arch_flags(out_variable)
 function(caffe_select_nvcc_arch_flags out_variable)
   # List of arch names
-  set(__archs_names "Fermi" "Kepler" "Maxwell" "Pascal" "All" "Manual")
+  set(__archs_names "Kepler" "Maxwell" "Pascal" "All" "Manual")
   set(__archs_name_default "All")
   if(NOT CMAKE_CROSSCOMPILING)
     list(APPEND __archs_names "Auto")
@@ -76,16 +79,14 @@ function(caffe_select_nvcc_arch_flags out_variable)
 
   if(${CUDA_ARCH_NAME} STREQUAL "Manual")
     set(CUDA_ARCH_BIN ${Caffe_known_gpu_archs} CACHE STRING "Specify 'real' GPU architectures to build binaries for, BIN(PTX) format is supported")
-    set(CUDA_ARCH_PTX "50"                     CACHE STRING "Specify 'virtual' PTX architectures to build PTX intermediate code for")
+    set(CUDA_ARCH_PTX "60"                     CACHE STRING "Specify 'virtual' PTX architectures to build PTX intermediate code for")
     mark_as_advanced(CUDA_ARCH_BIN CUDA_ARCH_PTX)
   else()
     unset(CUDA_ARCH_BIN CACHE)
     unset(CUDA_ARCH_PTX CACHE)
   endif()
 
-  if(${CUDA_ARCH_NAME} STREQUAL "Fermi")
-    set(__cuda_arch_bin "20 21(20)")
-  elseif(${CUDA_ARCH_NAME} STREQUAL "Kepler")
+  if(${CUDA_ARCH_NAME} STREQUAL "Kepler")
     set(__cuda_arch_bin "30 35")
   elseif(${CUDA_ARCH_NAME} STREQUAL "Maxwell")
     set(__cuda_arch_bin "50")
@@ -109,12 +110,6 @@ function(caffe_select_nvcc_arch_flags out_variable)
   set(__nvcc_flags "")
   set(__nvcc_archs_readable "")
 
-  string(COMPARE LESS "${CUDA_VERSION}" "9.0" iscudaolderthan90)
-  if(NOT iscudaolderthan90)
-    string(REPLACE "21(20)" "" __cuda_arch_bin "${__cuda_arch_bin}")
-    string(REPLACE "20" "" __cuda_arch_bin "${__cuda_arch_bin}")
-  endif()
-
   # Tell NVCC to add binaries for the specified GPUs
   foreach(__arch ${__cuda_arch_bin})
     if(__arch MATCHES "([0-9]+)\\(([0-9]+)\\)")
@@ -149,9 +144,14 @@ macro(caffe_cuda_compile objlist_variable)
 
     # we remove /EHa as it generates warnings under windows
     string(REPLACE "/EHa" "" ${var} "${${var}}")
+    string(REPLACE "/W3" "" ${var} "${${var}}")
 
   endforeach()
 
+#  if(MSVC)
+#    set(CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS})
+#  endif()
+
   if(UNIX OR APPLE)
     list(APPEND CUDA_NVCC_FLAGS -Xcompiler -fPIC)
   endif()
@@ -180,11 +180,21 @@ function(detect_cuDNN)
 
   find_path(CUDNN_INCLUDE cudnn.h
             PATHS ${CUDNN_ROOT} $ENV{CUDNN_ROOT} ${CUDA_TOOLKIT_INCLUDE}
+            PATH_SUFFIXES include
             DOC "Path to cuDNN include directory." )
+           
+  unset(_path_suffixes)
+  if(MSVC AND ${CMAKE_SIZEOF_VOID_P} EQUAL 8)
+    set(_path_suffixes PATH_SUFFIXES lib/x64)
+  else()
+    set(_path_suffixes PATH_SUFFIXES lib/Win32)    
+  endif()
 
   # dynamic libs have different suffix in mac and linux
   if(APPLE)
     set(CUDNN_LIB_NAME "libcudnn.dylib")
+  elseif(MSVC)
+    set(CUDNN_LIB_NAME "cudnn")
   else()
     set(CUDNN_LIB_NAME "libcudnn.so")
   endif()
@@ -192,6 +202,7 @@ function(detect_cuDNN)
   get_filename_component(__libpath_hist ${CUDA_CUDART_LIBRARY} PATH)
   find_library(CUDNN_LIBRARY NAMES ${CUDNN_LIB_NAME}
    PATHS ${CUDNN_ROOT} $ENV{CUDNN_ROOT} ${CUDNN_INCLUDE} ${__libpath_hist} ${__libpath_hist}/../lib
+   ${_path_suffixes}
    DOC "Path to cuDNN library.")
   
   if(CUDNN_INCLUDE AND CUDNN_LIBRARY)
@@ -238,7 +249,7 @@ endfunction()
 ################################################################################################
 
 find_package(CUDA 5.5 QUIET)
-find_cuda_helper_libs(curand)  # cmake 2.8.7 compatibility which doesn't search for curand
+find_cuda_helper_libs(curand)  # cmake 2.8.7 compartibility which doesn't search for curand
 
 if(NOT CUDA_FOUND)
   return()
@@ -247,9 +258,16 @@ endif()
 set(HAVE_CUDA TRUE)
 message(STATUS "CUDA detected: " ${CUDA_VERSION})
 list(APPEND Caffe_INCLUDE_DIRS PUBLIC ${CUDA_INCLUDE_DIRS})
+if(CUDA_VERSION GREATER  10.0)
+	string(REPLACE "CUDA_cublas_device_LIBRARY-NOTFOUND" "" CUDA_CUBLAS_LIBRARIES ${CUDA_CUBLAS_LIBRARIES})
+	set(CUDA_cublas_device_LIBRARY "")
+	message(STATUS "CUDA_CUBLAS_LIBRARIES: ${CUDA_CUBLAS_LIBRARIES}")
+endif()
+
 list(APPEND Caffe_LINKER_LIBS PUBLIC ${CUDA_CUDART_LIBRARY}
                                      ${CUDA_curand_LIBRARY} ${CUDA_CUBLAS_LIBRARIES})
 
+
 # cudnn detection
 if(USE_CUDNN)
   detect_cuDNN()
diff --git a/cmake/Dependencies.cmake b/cmake/Dependencies.cmake
index 5012e47..ddd2b7b 100644
--- a/cmake/Dependencies.cmake
+++ b/cmake/Dependencies.cmake
@@ -7,8 +7,13 @@ set(Caffe_COMPILE_OPTIONS "")
 # ---[ Boost
 find_package(Boost 1.54 REQUIRED COMPONENTS system thread filesystem)
 list(APPEND Caffe_INCLUDE_DIRS PUBLIC ${Boost_INCLUDE_DIRS})
+list(APPEND Caffe_DEFINITIONS PUBLIC -DBOOST_ALL_NO_LIB)
 list(APPEND Caffe_LINKER_LIBS PUBLIC ${Boost_LIBRARIES})
 
+if(DEFINED MSVC AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 18.0.40629.0)
+  # Required for VS 2013 Update 4 or earlier.
+  list(APPEND Caffe_DEFINITIONS PUBLIC -DBOOST_NO_CXX11_TEMPLATE_ALIASES)
+endif()
 # ---[ Threads
 find_package(Threads REQUIRED)
 list(APPEND Caffe_LINKER_LIBS PRIVATE ${CMAKE_THREAD_LIBS_INIT})
@@ -25,8 +30,13 @@ if(USE_OPENMP)
   # into their buildsystem again, so we put these options into per-target PUBLIC
   # compile options and link flags, so that they will be exported properly.
   find_package(OpenMP REQUIRED)
-  list(APPEND Caffe_LINKER_LIBS PRIVATE ${OpenMP_CXX_FLAGS})
-  list(APPEND Caffe_COMPILE_OPTIONS PRIVATE ${OpenMP_CXX_FLAGS})
+  if(MSVC)
+	set(OpenMP_CXX_FLAGS "/openmp")
+    list(APPEND Caffe_COMPILE_OPTIONS PRIVATE ${OpenMP_CXX_FLAGS})
+  else()
+	list(APPEND Caffe_LINKER_LIBS PRIVATE ${OpenMP_CXX_FLAGS})
+	list(APPEND Caffe_COMPILE_OPTIONS PRIVATE ${OpenMP_CXX_FLAGS})
+  endif()
 endif()
 
 # ---[ Google-glog
@@ -43,7 +53,17 @@ list(APPEND Caffe_LINKER_LIBS PUBLIC ${GFLAGS_LIBRARIES})
 include(cmake/ProtoBuf.cmake)
 
 # ---[ HDF5
-find_package(HDF5 COMPONENTS HL REQUIRED)
+if(MSVC)
+  # Find HDF5 using it's hdf5-config.cmake file with MSVC
+  if(DEFINED HDF5_DIR)
+    list(APPEND CMAKE_MODULE_PATH ${HDF5_DIR})
+  endif()
+  find_package(HDF5 COMPONENTS C HL REQUIRED)
+  set(HDF5_LIBRARIES hdf5-shared)
+  set(HDF5_HL_LIBRARIES hdf5_hl-shared)
+else()
+  find_package(HDF5 COMPONENTS HL REQUIRED)
+endif()
 list(APPEND Caffe_INCLUDE_DIRS PUBLIC ${HDF5_INCLUDE_DIRS})
 list(APPEND Caffe_LINKER_LIBS PUBLIC ${HDF5_LIBRARIES} ${HDF5_HL_LIBRARIES})
 
@@ -86,7 +106,7 @@ if(NOT HAVE_CUDA)
 endif()
 
 if(USE_NCCL)
-  find_package(NCCL REQUIRED)
+  include("cmake/External/nccl.cmake")
   include_directories(SYSTEM ${NCCL_INCLUDE_DIR})
   list(APPEND Caffe_LINKER_LIBS ${NCCL_LIBRARIES})
   add_definitions(-DUSE_NCCL)
@@ -94,7 +114,9 @@ endif()
 
 # ---[ OpenCV
 if(USE_OPENCV)
-  find_package(OpenCV QUIET COMPONENTS core highgui imgproc imgcodecs videoio)
+  find_package(OpenCV QUIET COMPONENTS core highgui imgproc imgcodecs videoio video
+  tracking freetype  cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters 
+  cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev)
   if(NOT OpenCV_FOUND) # if not OpenCV 3.x, then imgcodecs are not found
     find_package(OpenCV REQUIRED COMPONENTS core highgui imgproc videoio)
   endif()
@@ -102,11 +124,18 @@ if(USE_OPENCV)
   list(APPEND Caffe_LINKER_LIBS PUBLIC ${OpenCV_LIBS})
   message(STATUS "OpenCV found (${OpenCV_CONFIG_PATH})")
   list(APPEND Caffe_DEFINITIONS PUBLIC -DUSE_OPENCV)
+#  if(OpenCV_VERSION  "4.1.2")
+    message(STATUS "Add OpenCV Contrib")
+    find_package(OpenGL REQUIRED)
+    list(APPEND Caffe_INCLUDE_DIRS PUBLIC ${OPENGL_INCLUDE_DIRS}   ${GLUT_INCLUDE_DIRS} )
+    list(APPEND Caffe_LINKER_LIBS PUBLIC ${OPENGL_LIBRARIES})
+
+#  endif()
 endif()
 
 # ---[ BLAS
 if(NOT APPLE)
-  set(BLAS "Atlas" CACHE STRING "Selected BLAS library")
+  set(BLAS "Open" CACHE STRING "Selected BLAS library")
   set_property(CACHE BLAS PROPERTY STRINGS "Atlas;Open;MKL")
 
   if(BLAS STREQUAL "Atlas" OR BLAS STREQUAL "atlas")
@@ -173,6 +202,9 @@ if(BUILD_python)
   endif()
   if(PYTHONLIBS_FOUND AND NUMPY_FOUND AND Boost_PYTHON_FOUND)
     set(HAVE_PYTHON TRUE)
+    if(Boost_USE_STATIC_LIBS AND MSVC)
+      list(APPEND Caffe_DEFINITIONS PUBLIC -DBOOST_PYTHON_STATIC_LIB)
+    endif()
     if(BUILD_python_layer)
       list(APPEND Caffe_DEFINITIONS PRIVATE -DWITH_PYTHON_LAYER)
       list(APPEND Caffe_INCLUDE_DIRS PRIVATE ${PYTHON_INCLUDE_DIRS} ${NUMPY_INCLUDE_DIR} PUBLIC ${Boost_INCLUDE_DIRS})
@@ -183,11 +215,17 @@ endif()
 
 # ---[ Matlab
 if(BUILD_matlab)
-  find_package(MatlabMex)
-  if(MATLABMEX_FOUND)
-    set(HAVE_MATLAB TRUE)
+  if(MSVC)
+    find_package(Matlab COMPONENTS MAIN_PROGRAM MX_LIBRARY)
+    if(MATLAB_FOUND)
+      set(HAVE_MATLAB TRUE)
+    endif()
+  else()
+    find_package(MatlabMex)
+    if(MATLABMEX_FOUND)
+      set(HAVE_MATLAB TRUE)
+    endif()
   endif()
-
   # sudo apt-get install liboctave-dev
   find_program(Octave_compiler NAMES mkoctfile DOC "Octave C++ compiler")
 
diff --git a/cmake/External/nccl.cmake b/cmake/External/nccl.cmake
new file mode 100644
index 0000000..5f97f29
--- /dev/null
+++ b/cmake/External/nccl.cmake
@@ -0,0 +1,35 @@
+# if (NOT __NCCL_INCLUDED) # guard against multiple includes
+  set(__NCCL_INCLUDED TRUE)
+  if(MSVC)
+    # use the system-wide nccl if present
+    find_package(NCCL)
+    if (NCCL_FOUND)
+        set(NCCL_EXTERNAL FALSE)
+    else()
+        # build directory
+        set(nccl_PREFIX ${CMAKE_BINARY_DIR}/external/nccl-prefix)
+        # install directory
+        set(nccl_INSTALL ${CMAKE_BINARY_DIR}/external/nccl-install)
+        ExternalProject_Add(nccl
+        PREFIX ${nccl_PREFIX}
+        URL https://github.com/willyd/nccl/archive/470b3130457f125f4608c7baee71123aa16a3b12.zip
+        UPDATE_COMMAND ""
+        INSTALL_DIR ${nccl_INSTALL}
+        CMAKE_ARGS -DCMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}
+                   -DCMAKE_INSTALL_PREFIX=${nccl_INSTALL}
+                   -DBUILD_SHARED_LIBS=OFF
+                   -DNCCL_BUILD_TESTS:BOOL=OFF
+
+        LOG_DOWNLOAD 1
+        LOG_INSTALL 1
+        BUILD_BYPRODUCTS ${nccl_INSTALL}/include ${nccl_INSTALL}/lib/nccl.lib
+        )
+
+        set(NCCL_INCLUDE_DIR ${nccl_INSTALL}/include)
+        set(NCCL_LIBRARIES ${nccl_INSTALL}/lib/nccl.lib)
+    endif()
+  else()
+    # default to find package on UNIX systems
+    find_package(NCCL REQUIRED)
+  endif()
+# endif()
\ No newline at end of file
diff --git a/cmake/Misc.cmake b/cmake/Misc.cmake
index fcb2464..e506c38 100644
--- a/cmake/Misc.cmake
+++ b/cmake/Misc.cmake
@@ -1,5 +1,7 @@
-# ---[ Configuration types
-set(CMAKE_CONFIGURATION_TYPES "Debug;Release" CACHE STRING "Possible configurations" FORCE)
+# ---[ Configuration types
+set(CMAKE_CONFIGURATION_TYPES "Debug;Release;RelWithDebInfo" CACHE STRING "Possible configurations" FORCE)
+# in case RelWithDebInfo, file name libprotobuf.lib will be libprotobufd.lib
+
 mark_as_advanced(CMAKE_CONFIGURATION_TYPES)
 
 if(DEFINED CMAKE_BUILD_TYPE)
@@ -8,7 +10,7 @@ endif()
 
 # --[ If user doesn't specify build type then assume release
 if("${CMAKE_BUILD_TYPE}" STREQUAL "")
-  set(CMAKE_BUILD_TYPE Release)
+  set(CMAKE_BUILD_TYPE RelWithDebInfo)
 endif()
 
 if("${CMAKE_CXX_COMPILER_ID}" STREQUAL "Clang")
@@ -29,13 +31,12 @@ if(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)
 endif()
 
 # ---[ RPATH settings
-set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE CACHE BOOLEAN "Use link paths for shared library rpath")
+set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE CACHE BOOL "Use link paths for shared library rpath")
 set(CMAKE_MACOSX_RPATH TRUE)
 
-list(FIND CMAKE_PLATFORM_IMPLICIT_LINK_DIRECTORIES
-     ${CMAKE_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR} __is_systtem_dir)
+list(FIND CMAKE_PLATFORM_IMPLICIT_LINK_DIRECTORIES ${CMAKE_INSTALL_PREFIX}/lib __is_systtem_dir)
 if(${__is_systtem_dir} STREQUAL -1)
-  set(CMAKE_INSTALL_RPATH ${CMAKE_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR})
+  set(CMAKE_INSTALL_RPATH ${CMAKE_INSTALL_PREFIX}/lib)
 endif()
 
 # ---[ Funny target
diff --git a/cmake/Modules/FindGFlags.cmake b/cmake/Modules/FindGFlags.cmake
index 29b60f0..f44ecc0 100644
--- a/cmake/Modules/FindGFlags.cmake
+++ b/cmake/Modules/FindGFlags.cmake
@@ -14,26 +14,15 @@ include(FindPackageHandleStandardArgs)
 set(GFLAGS_ROOT_DIR "" CACHE PATH "Folder contains Gflags")
 
 # We are testing only a couple of files in the include directories
-if(WIN32)
-    find_path(GFLAGS_INCLUDE_DIR gflags/gflags.h
-        PATHS ${GFLAGS_ROOT_DIR}/src/windows)
-else()
-    find_path(GFLAGS_INCLUDE_DIR gflags/gflags.h
-        PATHS ${GFLAGS_ROOT_DIR})
-endif()
+find_path(GFLAGS_INCLUDE_DIR gflags/gflags.h
+    PATHS ${GFLAGS_ROOT_DIR})
 
 if(MSVC)
-    find_library(GFLAGS_LIBRARY_RELEASE
-        NAMES libgflags
-        PATHS ${GFLAGS_ROOT_DIR}
-        PATH_SUFFIXES Release)
-
-    find_library(GFLAGS_LIBRARY_DEBUG
-        NAMES libgflags-debug
-        PATHS ${GFLAGS_ROOT_DIR}
-        PATH_SUFFIXES Debug)
+    # rely on gflags-config.cmake
+    find_package(gflags NO_MODULE)
 
-    set(GFLAGS_LIBRARY optimized ${GFLAGS_LIBRARY_RELEASE} debug ${GFLAGS_LIBRARY_DEBUG})
+    set(GFLAGS_LIBRARY ${gflags_LIBRARIES})
+    set(GFLAGS_INCLUDE_DIR ${gflags_INCLUDE_DIRS})
 else()
     find_library(GFLAGS_LIBRARY gflags)
 endif()
diff --git a/cmake/Modules/FindGlog.cmake b/cmake/Modules/FindGlog.cmake
index 99abbe4..eec263a 100644
--- a/cmake/Modules/FindGlog.cmake
+++ b/cmake/Modules/FindGlog.cmake
@@ -13,24 +13,15 @@ include(FindPackageHandleStandardArgs)
 
 set(GLOG_ROOT_DIR "" CACHE PATH "Folder contains Google glog")
 
-if(WIN32)
-    find_path(GLOG_INCLUDE_DIR glog/logging.h
-        PATHS ${GLOG_ROOT_DIR}/src/windows)
-else()
-    find_path(GLOG_INCLUDE_DIR glog/logging.h
-        PATHS ${GLOG_ROOT_DIR})
-endif()
+find_path(GLOG_INCLUDE_DIR glog/logging.h
+    PATHS ${GLOG_ROOT_DIR})
 
 if(MSVC)
-    find_library(GLOG_LIBRARY_RELEASE libglog_static
-        PATHS ${GLOG_ROOT_DIR}
-        PATH_SUFFIXES Release)
-
-    find_library(GLOG_LIBRARY_DEBUG libglog_static
-        PATHS ${GLOG_ROOT_DIR}
-        PATH_SUFFIXES Debug)
+    # rely on glog-config.cmake
+    find_package(glog NO_MODULE)
 
-    set(GLOG_LIBRARY optimized ${GLOG_LIBRARY_RELEASE} debug ${GLOG_LIBRARY_DEBUG})
+    set(GLOG_LIBRARY ${glog_LIBRARIES})
+    set(GLOG_INCLUDE_DIR ${glog_INCLUDE_DIRS})
 else()
     find_library(GLOG_LIBRARY glog
         PATHS ${GLOG_ROOT_DIR}
diff --git a/cmake/Modules/FindLMDB.cmake b/cmake/Modules/FindLMDB.cmake
index 8a817fd..2f0adb1 100644
--- a/cmake/Modules/FindLMDB.cmake
+++ b/cmake/Modules/FindLMDB.cmake
@@ -12,8 +12,12 @@
 # Copyright 2013 Conrad Steenberg <conrad.steenberg@gmail.com>
 # Aug 31, 2013
 
-find_path(LMDB_INCLUDE_DIR NAMES  lmdb.h PATHS "$ENV{LMDB_DIR}/include")
-find_library(LMDB_LIBRARIES NAMES lmdb   PATHS "$ENV{LMDB_DIR}/lib" )
+if(MSVC)
+  find_package(LMDB NO_MODULE)
+else()
+  find_path(LMDB_INCLUDE_DIR NAMES  lmdb.h PATHS "$ENV{LMDB_DIR}/include")
+  find_library(LMDB_LIBRARIES NAMES lmdb   PATHS "$ENV{LMDB_DIR}/lib" )
+endif()
 
 include(FindPackageHandleStandardArgs)
 find_package_handle_standard_args(LMDB DEFAULT_MSG LMDB_INCLUDE_DIR LMDB_LIBRARIES)
diff --git a/cmake/Modules/FindLevelDB.cmake b/cmake/Modules/FindLevelDB.cmake
index 97f08ac..6e6a92d 100644
--- a/cmake/Modules/FindLevelDB.cmake
+++ b/cmake/Modules/FindLevelDB.cmake
@@ -5,14 +5,20 @@
 #  LevelDB_FOUND     - True if LevelDB found.
 
 # Look for the header file.
-find_path(LevelDB_INCLUDE NAMES leveldb/db.h
-                          PATHS $ENV{LEVELDB_ROOT}/include /opt/local/include /usr/local/include /usr/include
-                          DOC "Path in which the file leveldb/db.h is located." )
-
-# Look for the library.
-find_library(LevelDB_LIBRARY NAMES leveldb
-                             PATHS /usr/lib $ENV{LEVELDB_ROOT}/lib
-                             DOC "Path to leveldb library." )
+if(MSVC)
+  find_package(LevelDB NO_MODULE)
+  set(LevelDB_INCLUDE ${LevelDB_INCLUDE_DIRS})
+  set(LevelDB_LIBRARY ${LevelDB_LIBRARIES})
+else()
+  find_path(LevelDB_INCLUDE NAMES leveldb/db.h
+                            PATHS $ENV{LEVELDB_ROOT}/include /opt/local/include /usr/local/include /usr/include
+                            DOC "Path in which the file leveldb/db.h is located." )
+
+  # Look for the library.
+  find_library(LevelDB_LIBRARY NAMES leveldb
+                              PATHS /usr/lib $ENV{LEVELDB_ROOT}/lib
+                              DOC "Path to leveldb library." )
+endif()
 
 include(FindPackageHandleStandardArgs)
 find_package_handle_standard_args(LevelDB DEFAULT_MSG LevelDB_INCLUDE LevelDB_LIBRARY)
diff --git a/cmake/Modules/FindOpenBLAS.cmake b/cmake/Modules/FindOpenBLAS.cmake
index a6512ae..58e9aee 100644
--- a/cmake/Modules/FindOpenBLAS.cmake
+++ b/cmake/Modules/FindOpenBLAS.cmake
@@ -28,8 +28,13 @@ SET(Open_BLAS_LIB_SEARCH_PATHS
         $ENV{OpenBLAS_HOME}/lib
  )
 
+if(MSVC)
+  set(OpenBLAS_LIB_NAMES libopenblas.dll.a)
+else()
+  set(OpenBLAS_LIB_NAMES openblas)
+endif()
 FIND_PATH(OpenBLAS_INCLUDE_DIR NAMES cblas.h PATHS ${Open_BLAS_INCLUDE_SEARCH_PATHS})
-FIND_LIBRARY(OpenBLAS_LIB NAMES openblas PATHS ${Open_BLAS_LIB_SEARCH_PATHS})
+FIND_LIBRARY(OpenBLAS_LIB NAMES ${OpenBLAS_LIB_NAMES} PATHS ${Open_BLAS_LIB_SEARCH_PATHS})
 
 SET(OpenBLAS_FOUND ON)
 
diff --git a/cmake/Modules/FindSnappy.cmake b/cmake/Modules/FindSnappy.cmake
index eff2a86..3e4f5e6 100644
--- a/cmake/Modules/FindSnappy.cmake
+++ b/cmake/Modules/FindSnappy.cmake
@@ -7,12 +7,16 @@
 #  SNAPPY_FOUND
 #  Snappy_INCLUDE_DIR
 #  Snappy_LIBRARIES
+if(MSVC)
+  # rely on snappy-config.cmake
+  find_package(Snappy NO_MODULE)
+else()
+  find_path(Snappy_INCLUDE_DIR NAMES snappy.h
+                              PATHS ${SNAPPY_ROOT_DIR} ${SNAPPY_ROOT_DIR}/include)
 
-find_path(Snappy_INCLUDE_DIR NAMES snappy.h
-                             PATHS ${SNAPPY_ROOT_DIR} ${SNAPPY_ROOT_DIR}/include)
-
-find_library(Snappy_LIBRARIES NAMES snappy
-                              PATHS ${SNAPPY_ROOT_DIR} ${SNAPPY_ROOT_DIR}/lib)
+  find_library(Snappy_LIBRARIES NAMES snappy
+                                PATHS ${SNAPPY_ROOT_DIR} ${SNAPPY_ROOT_DIR}/lib)
+endif()
 
 include(FindPackageHandleStandardArgs)
 find_package_handle_standard_args(Snappy DEFAULT_MSG Snappy_INCLUDE_DIR Snappy_LIBRARIES)
diff --git a/cmake/Modules/FindvecLib.cmake b/cmake/Modules/FindvecLib.cmake
index 4d44e61..8eaab59 100644
--- a/cmake/Modules/FindvecLib.cmake
+++ b/cmake/Modules/FindvecLib.cmake
@@ -12,12 +12,11 @@ endif()
 
 set(__veclib_include_suffix "Frameworks/vecLib.framework/Versions/Current/Headers")
 
-exec_program(xcode-select ARGS -print-path OUTPUT_VARIABLE CMAKE_XCODE_DEVELOPER_DIR)
 find_path(vecLib_INCLUDE_DIR vecLib.h
           DOC "vecLib include directory"
           PATHS /System/Library/Frameworks/Accelerate.framework/Versions/Current/${__veclib_include_suffix}
                 /System/Library/${__veclib_include_suffix}
-                ${CMAKE_XCODE_DEVELOPER_DIR}/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks/Accelerate.framework/Versions/Current/Frameworks/vecLib.framework/Headers/
+                /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks/Accelerate.framework/Versions/Current/Frameworks/vecLib.framework/Headers/
           NO_DEFAULT_PATH)
 
 include(FindPackageHandleStandardArgs)
diff --git a/cmake/Modules/leveldb-targets-RelWithDebInfo.cmake b/cmake/Modules/leveldb-targets-RelWithDebInfo.cmake
new file mode 100644
index 0000000..f7e51c3
--- /dev/null
+++ b/cmake/Modules/leveldb-targets-RelWithDebInfo.cmake
@@ -0,0 +1,20 @@
+#----------------------------------------------------------------
+# Generated CMake target import file for configuration "WithDebInfo".
+#----------------------------------------------------------------
+
+# Commands may need to know the format version.
+set(CMAKE_IMPORT_FILE_VERSION 1)
+
+# Import target "leveldb" for configuration "WithDebInfo"
+set_property(TARGET leveldb APPEND PROPERTY IMPORTED_CONFIGURATIONS RELWITHDEBINFO)
+set_target_properties(leveldb PROPERTIES
+  IMPORTED_LINK_INTERFACE_LANGUAGES_RELWITHDEBINFO "CXX"
+  IMPORTED_LINK_INTERFACE_LIBRARIES_RELWITHDEBINFO "${CMAKE_CURRENT_LIST_DIR}/../lib/boost_date_time-vc140-mt-1_61.lib;${CMAKE_CURRENT_LIST_DIR}/../lib/boost_filesystem-vc140-mt-1_61.lib;${CMAKE_CURRENT_LIST_DIR}/../lib/boost_system-vc140-mt-1_61.lib"
+  IMPORTED_LOCATION_RELWITHDEBINFO "${_IMPORT_PREFIX}/lib/leveldb.lib"
+  )
+
+list(APPEND _IMPORT_CHECK_TARGETS leveldb )
+list(APPEND _IMPORT_CHECK_FILES_FOR_leveldb "${_IMPORT_PREFIX}/lib/leveldb.lib" )
+
+# Commands beyond this point should not need to know the version.
+set(CMAKE_IMPORT_FILE_VERSION)
diff --git a/cmake/ProtoBuf.cmake b/cmake/ProtoBuf.cmake
index 72ea323..a0d4295 100644
--- a/cmake/ProtoBuf.cmake
+++ b/cmake/ProtoBuf.cmake
@@ -1,7 +1,13 @@
 # Finds Google Protocol Buffers library and compilers and extends
 # the standard cmake script with version and python generation support
 
-find_package( Protobuf REQUIRED )
+if(MSVC)
+  # search using protobuf-config.cmake
+  find_package( Protobuf REQUIRED NO_MODULE)
+  set(PROTOBUF_INCLUDE_DIR ${PROTOBUF_INCLUDE_DIRS})
+else()
+  find_package( Protobuf REQUIRED )
+endif()
 list(APPEND Caffe_INCLUDE_DIRS PUBLIC ${PROTOBUF_INCLUDE_DIR})
 list(APPEND Caffe_LINKER_LIBS PUBLIC ${PROTOBUF_LIBRARIES})
 
@@ -78,7 +84,7 @@ function(caffe_protobuf_generate_cpp_py output_dir srcs_var hdrs_var python_var)
              "${output_dir}/${fil_we}_pb2.py"
       COMMAND ${CMAKE_COMMAND} -E make_directory "${output_dir}"
       COMMAND ${PROTOBUF_PROTOC_EXECUTABLE} --cpp_out    ${output_dir} ${_protoc_include} ${abs_fil}
-      COMMAND ${PROTOBUF_PROTOC_EXECUTABLE} --python_out ${PROJECT_BINARY_DIR}/include --proto_path ${PROJECT_SOURCE_DIR}/src ${_protoc_include} ${abs_fil}
+      COMMAND ${PROTOBUF_PROTOC_EXECUTABLE} --python_out ${output_dir} ${_protoc_include} ${abs_fil}
       DEPENDS ${abs_fil}
       COMMENT "Running C++/Python protocol buffer compiler on ${fil}" VERBATIM )
   endforeach()
diff --git a/cmake/TargetResolvePrerequesites.cmake b/cmake/TargetResolvePrerequesites.cmake
new file mode 100644
index 0000000..429c113
--- /dev/null
+++ b/cmake/TargetResolvePrerequesites.cmake
@@ -0,0 +1,189 @@
+set(THIS_FILE ${CMAKE_CURRENT_LIST_FILE})
+set(THIS_DIR ${CMAKE_CURRENT_LIST_DIR})
+
+include(CMakeParseArguments)
+
+function(caffe_prerequisites_directories VAR)
+  if(BUILD_SHARED_LIBS)
+    # Append the caffe library output directory
+    list(APPEND _directories $<TARGET_FILE_DIR:caffe>)
+  endif()
+  # Add boost to search directories
+  list(APPEND _directories ${Boost_LIBRARY_DIRS})
+  # Add gflags to search directories
+  # gflags_DIR should point to root/CMake
+  get_filename_component(_dir ${gflags_DIR} DIRECTORY)
+  list(APPEND _directories ${_dir}/lib)
+  # Add glog to search directories
+  # glog_DIR should point to root/lib/cmake/glog
+  get_filename_component(_dir ${glog_DIR} DIRECTORY)
+  get_filename_component(_dir ${_dir} DIRECTORY)
+  get_filename_component(_dir ${_dir} DIRECTORY)
+  list(APPEND _directories ${_dir}/bin)
+  # Add HDF5 to search directories
+  # HDF5_DIR should point to root/CMake
+  get_filename_component(_dir ${HDF5_DIR} DIRECTORY)
+  list(APPEND _directories ${_dir}/bin)
+  # Add OpenCV to search directories
+  get_filename_component(_dir ${OpenCV_LIB_PATH} DIRECTORY)
+  list(APPEND _directories ${_dir}/bin)
+  if(CUDNN_FOUND AND HAVE_CUDNN)
+    # Add OpenCV to search directories
+    get_filename_component(_dir ${CUDNN_LIBRARY} DIRECTORY)
+    get_filename_component(_dir ${_dir} DIRECTORY)
+    get_filename_component(_dir ${_dir} DIRECTORY)
+    list(APPEND _directories ${_dir}/bin)
+  endif()
+  if(USE_NCCL)
+    # add the nvml.dll path if we are using nccl
+    file(TO_CMAKE_PATH "$ENV{NVTOOLSEXT_PATH}" _nvtools_ext)
+    if(NOT "${_nvtools_ext}" STREQUAL "")
+      get_filename_component(_nvsmi_path ${_nvtools_ext}/../nvsmi ABSOLUTE)
+      list(APPEND _directories ${_nvsmi_path})
+    endif()
+  endif()
+  list(REMOVE_DUPLICATES _directories)
+  set(${VAR} ${_directories} PARENT_SCOPE)
+endfunction()
+
+function(caffe_copy_prerequisites target)
+  caffe_prerequisites_directories(_directories)
+  target_copy_prerequisites(${target} ${ARGN} DIRECTORIES ${_directories})
+endfunction()
+
+function(caffe_install_prerequisites target)
+  caffe_prerequisites_directories(_directories)
+  target_install_prerequisites(${target} ${ARGN} DIRECTORIES ${_directories})
+endfunction()
+
+function(target_copy_prerequisites target)
+  set(options USE_HARD_LINKS)
+  set(oneValueArgs DESTINATION)
+  set(multiValueArgs DIRECTORIES)
+  cmake_parse_arguments(tcp "${options}" "${oneValueArgs}"
+                            "${multiValueArgs}" ${ARGN})
+  if(NOT tcp_DESTINATION)
+    set(tcp_DESTINATION $<TARGET_FILE_DIR:${target}>)
+  endif()
+  string(REPLACE ";" "@@" tcp_DIRECTORIES "${tcp_DIRECTORIES}")
+  if(USE_NCCL)
+    # nccl loads the nvml.dll dynamically so we need
+    # to list it explicitely
+    list(APPEND _plugins nvml.dll)
+  endif()
+  string(REPLACE ";" "@@" _plugins "${_plugins}")
+  add_custom_command(TARGET ${target} POST_BUILD
+                     COMMAND ${CMAKE_COMMAND}
+                             -DTARGET=$<TARGET_FILE:${target}>
+                             -DDESTINATION=${tcp_DESTINATION}
+                             -DUSE_HARD_LINKS=${tcp_USE_HARD_LINKS}
+                             -DDIRECTORIES=${tcp_DIRECTORIES}
+                             -DPLUGINS=${_plugins}
+                             -P ${THIS_FILE}
+                     )
+endfunction()
+
+function(target_install_prerequisites target)
+  set(options )
+  set(oneValueArgs DESTINATION)
+  set(multiValueArgs DIRECTORIES)
+  cmake_parse_arguments(tcp "${options}" "${oneValueArgs}"
+                            "${multiValueArgs}" ${ARGN})
+  if(NOT tcp_DESTINATION)
+    set(tcp_DESTINATION bin)
+  endif()
+  if(NOT IS_ABSOLUTE ${tcp_DESTINATION})
+    set(tcp_DESTINATION ${CMAKE_INSTALL_PREFIX}/${tcp_DESTINATION})
+  endif()
+  string(REPLACE ";" "@@" tcp_DIRECTORIES "${tcp_DIRECTORIES}")
+  if(USE_NCCL)
+    # nccl loads the nvml.dll dynamically so we need
+    # to list it explicitely
+    list(APPEND _plugins nvml.dll)
+  endif()
+  string(REPLACE ";" "@@" _plugins "${_plugins}")
+  set(_command_output ${CMAKE_CURRENT_BINARY_DIR}/${target}-install-prerequisites.stamp)
+  add_custom_command(OUTPUT ${_command_output}
+                     COMMAND ${CMAKE_COMMAND}
+                             -DTARGET=$<TARGET_FILE:${target}>
+                             -DDESTINATION=${tcp_DESTINATION}
+                             -DUSE_HARD_LINKS=0
+                             -DDIRECTORIES=${tcp_DIRECTORIES}
+                             -DPLUGINS=${_plugins}
+                             -P ${THIS_FILE}
+                     COMMAND ${CMAKE_COMMAND} -E touch ${_command_output}
+                     )
+  add_custom_target(${target}_install_prerequisites ALL
+                    DEPENDS ${_command_output})
+  install(FILES ${_command_output} DESTINATION ${CMAKE_CURRENT_BINARY_DIR}/tmp)
+endfunction()
+
+function(create_hardlink link target result_variable)
+    file(TO_NATIVE_PATH ${link} _link)
+    file(TO_NATIVE_PATH ${target} _target)
+    execute_process(COMMAND cmd /c mklink /H "${_link}" "${_target}"
+                    RESULT_VARIABLE _result
+                    OUTPUT_VARIABLE _stdout
+                    ERROR_VARIABLE _stderr
+                    )
+    set(${result_variable} ${_result} PARENT_SCOPE)
+endfunction()
+
+function(copy_changed_file filename destination use_hard_links)
+    set(_copy 1)
+    set(_src_name ${filename})
+    get_filename_component(_name ${_src_name} NAME)
+    set(_dst_name ${destination}/${_name})
+
+    # lock a file to ensure that no two cmake processes
+    # try to copy the same file at the same time in parallel
+    # builds
+    string(SHA1 _hash ${_dst_name})
+    set(_lock_file ${CMAKE_BINARY_DIR}/${_hash}.lock)
+    file(LOCK ${_lock_file} GUARD FUNCTION)
+
+    if(EXISTS ${_dst_name})
+        file(TIMESTAMP ${_dst_name} _dst_time)
+        file(TIMESTAMP ${_src_name} _src_time)
+        if(${_dst_time} STREQUAL ${_src_time})
+            # skip this library if the destination and source
+            # have the same time stamp
+            return()
+        else()
+            # file has changed remove
+            file(REMOVE ${_dst_name})
+        endif()
+    endif()
+
+    if(use_hard_links)
+        message(STATUS "Creating hardlink for ${_name} in ${destination}")
+        create_hardlink(${_dst_name} ${_src_name} _result)
+        if(_result EQUAL 0)
+            set(_copy 0)
+        else()
+            message(STATUS "Failed to create hardlink ${_dst_name}. Copying instead.")
+        endif()
+    endif()
+    if(_copy)
+        message(STATUS "Copying ${_name} to ${destination}")
+        file(COPY ${_src_name} DESTINATION ${DESTINATION})
+    endif()
+endfunction()
+
+
+if(CMAKE_SCRIPT_MODE_FILE)
+  include(${THIS_DIR}/CaffeGetPrerequisites.cmake)
+  # Recreate a list by replacing the @@ with ;
+  string(REPLACE "@@" ";" DIRECTORIES "${DIRECTORIES}")
+  string(REPLACE "@@" ";" PLUGINS "${PLUGINS}")
+  # Get a recursive list of dependencies required by target using dumpbin
+  get_prerequisites(${TARGET} _prerequisites 1 1 "" "${DIRECTORIES}")
+  foreach(_prereq ${_prerequisites} ${PLUGINS})
+    # Resolve the dependency using the list of directories
+    gp_resolve_item("${TARGET}" "${_prereq}" "" "${DIRECTORIES}" resolved_file)
+    # Copy or create hardlink (if possible)
+    if(EXISTS ${resolved_file})
+      copy_changed_file(${resolved_file} ${DESTINATION} ${USE_HARD_LINKS})
+    endif()
+  endforeach()
+endif()
\ No newline at end of file
diff --git a/cmake/Targets.cmake b/cmake/Targets.cmake
index 090f86c..3755461 100644
--- a/cmake/Targets.cmake
+++ b/cmake/Targets.cmake
@@ -1,7 +1,16 @@
 ################################################################################################
 # Defines global Caffe_LINK flag, This flag is required to prevent linker from excluding
 # some objects which are not addressed directly but are registered via static constructors
-macro(caffe_set_caffe_link)
+macro(caffe_set_caffe_link)  
+  if(MSVC AND CMAKE_GENERATOR MATCHES Ninja)        
+    foreach(_suffix "" ${CMAKE_CONFIGURATION_TYPES})
+      if(NOT _suffix STREQUAL "")
+        string(TOUPPER _${_suffix} _suffix)
+      endif()
+      set(CMAKE_CXX_FLAGS${_suffix} "${CMAKE_CXX_FLAGS${_suffix}} /FS")
+      set(CMAKE_C_FLAGS${_suffix} "${CMAKE_C_FLAGS${_suffix}} /FS")              
+    endforeach()
+  endif()
   if(BUILD_SHARED_LIBS)
     set(Caffe_LINK caffe)
   else()
@@ -9,6 +18,8 @@ macro(caffe_set_caffe_link)
       set(Caffe_LINK -Wl,-force_load caffe)
     elseif("${CMAKE_CXX_COMPILER_ID}" STREQUAL "GNU")
       set(Caffe_LINK -Wl,--whole-archive caffe -Wl,--no-whole-archive)
+    elseif(MSVC)
+      set(Caffe_LINK caffe)
     endif()
   endif()
 endmacro()
@@ -52,9 +63,18 @@ endfunction()
 #   caffe_pickup_caffe_sources(<root>)
 function(caffe_pickup_caffe_sources root)
   # put all files in source groups (visible as subfolder in many IDEs)
+  set(caffe_export_hdr_in ${PROJECT_SOURCE_DIR}/cmake/Templates/export.hpp.in)
+  set(caffe_export_hdr ${PROJECT_BINARY_DIR}/caffe/export.hpp)  
+  set(caffe_symbols_hdr ${PROJECT_BINARY_DIR}/caffe/include_symbols.hpp)  
+  set_source_files_properties(${caffe_export_hdr} ${caffe_symbols_hdr} PROPERTIES GENERATED TRUE)
+  
   caffe_source_group("Include"        GLOB "${root}/include/caffe/*.h*")
   caffe_source_group("Include\\Util"  GLOB "${root}/include/caffe/util/*.h*")
   caffe_source_group("Include"        GLOB "${PROJECT_BINARY_DIR}/caffe_config.h*")
+  caffe_source_group("Include"        GLOB "${caffe_export_hdr}")
+  if(MSVC AND NOT BUILD_SHARED_LIBS)
+    caffe_source_group("Include"        GLOB "${caffe_symbols_hdr}")
+  endif()
   caffe_source_group("Source"         GLOB "${root}/src/caffe/*.cpp")
   caffe_source_group("Source\\Util"   GLOB "${root}/src/caffe/util/*.cpp")
   caffe_source_group("Source\\Layers" GLOB "${root}/src/caffe/layers/*.cpp")
@@ -76,7 +96,13 @@ function(caffe_pickup_caffe_sources root)
   list(REMOVE_ITEM  srcs ${test_srcs})
 
   # adding headers to make the visible in some IDEs (Qt, VS, Xcode)
-  list(APPEND srcs ${hdrs} ${PROJECT_BINARY_DIR}/caffe_config.h)
+  list(APPEND srcs ${hdrs}
+                   ${PROJECT_BINARY_DIR}/caffe_config.h
+                   ${caffe_export_hdr}
+  )
+  if(MSVC AND NOT BUILD_SHARED_LIBS)
+    list(APPEND srcs ${caffe_symbols_hdr})
+  endif()
   list(APPEND test_srcs ${test_hdrs})
 
   # collect cuda files
@@ -99,6 +125,10 @@ function(caffe_pickup_caffe_sources root)
   set(cuda ${cuda} PARENT_SCOPE)
   set(test_srcs ${test_srcs} PARENT_SCOPE)
   set(test_cuda ${test_cuda} PARENT_SCOPE)
+  set(caffe_export_hdr_in ${caffe_export_hdr_in} PARENT_SCOPE)
+  set(caffe_export_hdr ${caffe_export_hdr} PARENT_SCOPE)
+  set(caffe_symbols_hdr ${caffe_symbols_hdr} PARENT_SCOPE)
+  
 endfunction()
 
 ################################################################################################
diff --git a/cmake/Templates/CaffeConfig.cmake.in b/cmake/Templates/CaffeConfig.cmake.in
index 77c4059..f17719d 100644
--- a/cmake/Templates/CaffeConfig.cmake.in
+++ b/cmake/Templates/CaffeConfig.cmake.in
@@ -37,6 +37,198 @@ if(@USE_OPENCV@)
   endif()
 endif()
 
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
 # Compute paths
 get_filename_component(Caffe_CMAKE_DIR "${CMAKE_CURRENT_LIST_FILE}" PATH)
 
diff --git a/cmake/Templates/CaffeConfig.cmake.in.orig b/cmake/Templates/CaffeConfig.cmake.in.orig
new file mode 100644
index 0000000..73cf814
--- /dev/null
+++ b/cmake/Templates/CaffeConfig.cmake.in.orig
@@ -0,0 +1,151 @@
+# Config file for the Caffe package.
+#
+# Note:
+#   Caffe and this config file depends on opencv,
+#   so put `find_package(OpenCV)` before searching Caffe
+#   via `find_package(Caffe)`. All other lib/includes
+#   dependencies are hard coded in the file
+#
+# After successful configuration the following variables
+# will be defined:
+#
+#   Caffe_LIBRARIES    - IMPORTED targets to link against
+#                        (There is no Caffe_INCLUDE_DIRS and Caffe_DEFINITIONS
+#                         because they are specified in the IMPORTED target interface.)
+#
+#   Caffe_HAVE_CUDA    - signals about CUDA support
+#   Caffe_HAVE_CUDNN   - signals about cuDNN support
+
+
+# OpenCV dependency (optional)
+
+if(@USE_OPENCV@)
+  if(NOT OpenCV_FOUND)
+    set(Caffe_OpenCV_CONFIG_PATH "@OpenCV_CONFIG_PATH@")
+    if(Caffe_OpenCV_CONFIG_PATH)
+      get_filename_component(Caffe_OpenCV_CONFIG_PATH ${Caffe_OpenCV_CONFIG_PATH} ABSOLUTE)
+
+      if(EXISTS ${Caffe_OpenCV_CONFIG_PATH} AND NOT TARGET opencv_core)
+        message(STATUS "Caffe: using OpenCV config from ${Caffe_OpenCV_CONFIG_PATH}")
+	include(${Caffe_OpenCV_CONFIG_PATH}/OpenCVConfig.cmake)
+      endif()
+
+    else()
+      find_package(OpenCV REQUIRED)
+    endif()
+    unset(Caffe_OpenCV_CONFIG_PATH)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Handle other imported targets libraries
+if(@GFLAGS_IMPORTED@)
+  find_package(gflags REQUIRED NO_MODULE)
+endif()
+
+if(@GLOG_IMPORTED@)
+  find_package(glog REQUIRED NO_MODULE)
+endif()
+
+if(@HDF5_IMPORTED@)
+  find_package(HDF5 COMPONENTS C HL REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LMDB@ AND @LMDB_IMPORTED@)
+  find_package(LMDB REQUIRED NO_MODULE)
+endif()
+
+if(@USE_LEVELDB@ AND @LEVELDB_IMPORTED@)
+  find_package(LevelDB REQUIRED NO_MODULE)
+  if(@SNAPPY_IMPORTED@)
+    find_package(Snappy REQUIRED NO_MODULE)
+  endif()
+endif()
+
+# Compute paths
+get_filename_component(Caffe_CMAKE_DIR "${CMAKE_CURRENT_LIST_FILE}" PATH)
+
+# Our library dependencies
+if(NOT TARGET caffe AND NOT caffe_BINARY_DIR)
+  include("${Caffe_CMAKE_DIR}/CaffeTargets.cmake")
+endif()
+
+# List of IMPORTED libs created by CaffeTargets.cmake
+# These targets already specify all needed definitions and include pathes
+set(Caffe_LIBRARIES caffe)
+
+# Cuda support variables
+set(Caffe_CPU_ONLY @CPU_ONLY@)
+set(Caffe_HAVE_CUDA @HAVE_CUDA@)
+set(Caffe_HAVE_CUDNN @HAVE_CUDNN@)
diff --git a/cmake/Templates/export.hpp.in b/cmake/Templates/export.hpp.in
new file mode 100644
index 0000000..33ff222
--- /dev/null
+++ b/cmake/Templates/export.hpp.in
@@ -0,0 +1,10 @@
+#ifndef CAFFE_EXPORT_HPP_
+#define CAFFE_EXPORT_HPP_
+
+// CAFFE_BUILDING_STATIC_LIB should be defined 
+// only by the caffe target
+#if defined(_MSC_VER) && !defined(CAFFE_BUILDING_STATIC_LIB) 
+    ${CAFFE_INCLUDE_SYMBOLS}
+#endif
+
+#endif  // CAFFE_EXPORT_HPP_
\ No newline at end of file
diff --git a/cmake/WindowsCreateLinkHeader.cmake b/cmake/WindowsCreateLinkHeader.cmake
new file mode 100644
index 0000000..29e77b0
--- /dev/null
+++ b/cmake/WindowsCreateLinkHeader.cmake
@@ -0,0 +1,72 @@
+set(_windows_create_link_header "${CMAKE_CURRENT_LIST_FILE}")
+
+# function to add a post build command to create a link header
+function(windows_create_link_header target outputfile)
+    add_custom_command(TARGET ${target} POST_BUILD
+                       COMMAND ${CMAKE_COMMAND}
+                                #-DCMAKE_GENERATOR=${CMAKE_GENERATOR}
+                                -DMSVC_VERSION=${MSVC_VERSION}
+                                -DTARGET_FILE=$<TARGET_FILE:${target}>
+                                #-DPROJECT_BINARY_DIR=${PROJECT_BINARY_DIR}
+                                #-DCMAKE_CURRENT_BINARY_DIR=${CMAKE_CURRENT_BINARY_DIR}
+                                #-DCONFIGURATION=$<CONFIGURATION>
+                                -DOUTPUT_FILE=${outputfile}
+                                -P ${_windows_create_link_header}
+                        BYPRODUCTS ${outputfile}
+                      )
+endfunction()
+
+
+function(find_dumpbin var)
+    # MSVC_VERSION =
+    # 1200 = VS  6.0
+    # 1300 = VS  7.0
+    # 1310 = VS  7.1
+    # 1400 = VS  8.0
+    # 1500 = VS  9.0
+    # 1600 = VS 10.0
+    # 1700 = VS 11.0
+    # 1800 = VS 12.0
+    # 1900 = VS 14.0
+    set(MSVC_PRODUCT_VERSION_1200 6.0)
+    set(MSVC_PRODUCT_VERSION_1300 7.0)
+    set(MSVC_PRODUCT_VERSION_1310 7.1)
+    set(MSVC_PRODUCT_VERSION_1400 8.0)
+    set(MSVC_PRODUCT_VERSION_1500 9.0)
+    set(MSVC_PRODUCT_VERSION_1600 10.0)
+    set(MSVC_PRODUCT_VERSION_1700 11.0)
+    set(MSVC_PRODUCT_VERSION_1800 12.0)
+    set(MSVC_PRODUCT_VERSION_1900 14.0)
+    get_filename_component(MSVC_VC_DIR [HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\VisualStudio\\${MSVC_PRODUCT_VERSION_${MSVC_VERSION}}\\Setup\\VC;ProductDir] REALPATH CACHE)
+
+    find_program(DUMPBIN_EXECUTABLE dumpbin ${MSVC_VC_DIR}/bin)
+    if(NOT DUMPBIN_EXECUTABLE)
+        message(FATAL_ERROR "Could not find DUMPBIN_EXECUTABLE please define this variable")
+    endif()
+    set(${var} ${DUMPBIN_EXECUTABLE} PARENT_SCOPE)
+endfunction()
+
+macro(print_date)
+    execute_process(COMMAND powershell -NoProfile -Command "get-date")
+endmacro()
+
+
+if(CMAKE_SCRIPT_MODE_FILE)
+    cmake_policy(SET CMP0007 NEW)
+    # find the dumpbin exe
+    find_dumpbin(dumpbin)
+    # execute dumpbin to generate a list of symbols
+    execute_process(COMMAND ${dumpbin} /SYMBOLS ${TARGET_FILE}
+                    RESULT_VARIABLE _result
+                    OUTPUT_VARIABLE _output
+                    ERROR_VARIABLE _error
+    )
+    # match all layers and solvers instantiation guard
+    string(REGEX MATCHALL "\\?gInstantiationGuard[^\\(\\) ]*" __symbols ${_output})
+    # define a string to generate a list of pragmas
+    foreach(__symbol ${__symbols})
+        set(__pragma "${__pragma}#pragma comment(linker, \"/include:${__symbol}\")\n")        
+    endforeach()
+    file(WRITE ${OUTPUT_FILE} ${__pragma})
+endif()
+
diff --git a/cmake/WindowsDownloadPrebuiltDependencies.cmake b/cmake/WindowsDownloadPrebuiltDependencies.cmake
new file mode 100644
index 0000000..8c2115e
--- /dev/null
+++ b/cmake/WindowsDownloadPrebuiltDependencies.cmake
@@ -0,0 +1,124 @@
+set(DEPENDENCIES_VERSION 1.1.0)
+set(DEPENDENCIES_NAME_1800_27 libraries_v120_x64_py27_${DEPENDENCIES_VERSION})
+set(DEPENDENCIES_NAME_1900_27 libraries_v140_x64_py27_${DEPENDENCIES_VERSION})
+set(DEPENDENCIES_NAME_1900_35 libraries_v140_x64_py35_${DEPENDENCIES_VERSION})
+
+set(DEPENDENCIES_URL_BASE https://github.com/willyd/caffe-builder/releases/download)
+set(DEPENDENCIES_FILE_EXT .tar.bz2)
+set(DEPENDENCIES_URL_1800_27 "${DEPENDENCIES_URL_BASE}/v${DEPENDENCIES_VERSION}/${DEPENDENCIES_NAME_1800_27}${DEPENDENCIES_FILE_EXT}")
+set(DEPENDENCIES_SHA_1800_27 "ba833d86d19b162a04d68b09b06df5e0dad947d4")
+set(DEPENDENCIES_URL_1900_27 "${DEPENDENCIES_URL_BASE}/v${DEPENDENCIES_VERSION}/${DEPENDENCIES_NAME_1900_27}${DEPENDENCIES_FILE_EXT}")
+set(DEPENDENCIES_SHA_1900_27 "17eecb095bd3b0774a87a38624a77ce35e497cd2")
+set(DEPENDENCIES_URL_1900_35 "${DEPENDENCIES_URL_BASE}/v${DEPENDENCIES_VERSION}/${DEPENDENCIES_NAME_1900_35}${DEPENDENCIES_FILE_EXT}")
+set(DEPENDENCIES_SHA_1900_35 "f060403fd1a7448d866d27c0e5b7dced39c0a607")
+
+caffe_option(USE_PREBUILT_DEPENDENCIES "Download and use the prebuilt dependencies" ON IF MSVC)
+if(MSVC)
+  if(EXISTS $ENV{DOWNLOAD})
+    message(STATUS "EXISTS DOWNLOAD : " $ENV{DOWNLOAD})
+	set(CAFFE_DEPENDENCIES_DOWNLOAD_DIR $ENV{DOWNLOAD} CACHE PATH "Download directory for prebuilt dependencies")
+	set(CAFFE_DEPENDENCIES_ROOT_DIR $ENV{DOWNLOAD} CACHE PATH "Prebuild depdendencies root directory")
+  else()
+    file(TO_CMAKE_PATH $ENV{USERPROFILE} USERPROFILE_DIR)
+	if(NOT EXISTS ${USERPROFILE_DIR})
+	  message(FATAL_ERROR "Could not find %USERPROFILE% directory. Please specify an alternate CAFFE_DEPENDENCIES_ROOT_DIR")
+	endif()
+	set(CAFFE_DEPENDENCIES_ROOT_DIR ${USERPROFILE_DIR}/.caffe/dependencies CACHE PATH "Prebuild depdendencies root directory")
+	set(CAFFE_DEPENDENCIES_DOWNLOAD_DIR ${CAFFE_DEPENDENCIES_ROOT_DIR}/download CACHE PATH "Download directory for prebuilt dependencies")
+  endif()
+  message(STATUS "CAFFE_DEPENDENCIES_ROOT_DIR : " ${CAFFE_DEPENDENCIES_ROOT_DIR})
+endif()
+if(USE_PREBUILT_DEPENDENCIES)
+    # Determine the python version
+    if(BUILD_python)
+        if(NOT PYTHONINTERP_FOUND)
+            if(NOT "${python_version}" VERSION_LESS "3.0.0")
+                find_package(PythonInterp 3.5)
+            else()
+                find_package(PythonInterp 2.7)
+            endif()
+        endif()
+        set(_pyver ${PYTHON_VERSION_MAJOR}${PYTHON_VERSION_MINOR})
+    else()
+        message(STATUS "Building without python. Prebuilt dependencies will default to Python 3.5")
+        set(_pyver 35)
+    endif()
+	if(MSVC_VERSION GREATER "1900")
+	  set(VC_VER "1900")
+	else()
+	  set(VC_VER ${MSVC_VERSION})
+	endif()
+    if(NOT DEFINED DEPENDENCIES_URL_${VC_VER}_${_pyver})
+        message(FATAL_ERROR "Could not find url for MSVC version = ${VC_VER} and Python version = ${PYTHON_VERSION_MAJOR}.${PYTHON_VERSION_MINOR}.")
+    endif()
+    # set the dependencies URL and SHA1
+    set(DEPENDENCIES_URL ${DEPENDENCIES_URL_${VC_VER}_${_pyver}})
+    set(DEPENDENCIES_SHA ${DEPENDENCIES_SHA_${VC_VER}_${_pyver}})
+    set(CAFFE_DEPENDENCIES_DIR ${CAFFE_DEPENDENCIES_ROOT_DIR}/${DEPENDENCIES_NAME_${VC_VER}_${_pyver}})
+
+    foreach(_dir ${CAFFE_DEPENDENCIES_ROOT_DIR}
+                 ${CAFFE_DEPENDENCIES_DOWNLOAD_DIR}
+                 ${CAFFE_DEPENDENCIES_DIR})
+      # create the directory if it does not exist
+      if(NOT EXISTS ${_dir})
+        file(MAKE_DIRECTORY ${_dir})
+      endif()
+    endforeach()
+    # download and extract the file if it does not exist or if does not match the sha1
+    get_filename_component(_download_filename ${DEPENDENCIES_URL} NAME)
+    set(_download_path ${CAFFE_DEPENDENCIES_DOWNLOAD_DIR}/${_download_filename})
+    set(_download_file 1)
+    if(EXISTS ${_download_path})
+        file(SHA1 ${_download_path} _file_sha)
+        if("${_file_sha}" STREQUAL "${DEPENDENCIES_SHA}")
+            set(_download_file 0)
+        else()
+            set(_download_file 1)
+            message(STATUS "Removing file because sha1 does not match.")
+            file(REMOVE ${_download_path})
+        endif()
+    endif()
+    if(_download_file)
+        message(STATUS "Downloading prebuilt dependencies to ${_download_path}")
+        file(DOWNLOAD "${DEPENDENCIES_URL}"
+                      "${_download_path}"
+                      EXPECTED_HASH SHA1=${DEPENDENCIES_SHA}
+                      SHOW_PROGRESS
+                      )
+        if(EXISTS ${CAFFE_DEPENDENCIES_DIR}/libraries)
+            file(REMOVE_RECURSE ${CAFFE_DEPENDENCIES_DIR}/libraries)
+        endif()
+    endif()
+    if(EXISTS ${_download_path} AND NOT EXISTS ${CAFFE_DEPENDENCIES_DIR}/libraries)
+        message(STATUS "Extracting dependencies")
+        execute_process(COMMAND ${CMAKE_COMMAND} -E tar xjf ${_download_path}
+                        WORKING_DIRECTORY ${CAFFE_DEPENDENCIES_DIR}
+		)
+        execute_process(COMMAND patch -p1 -i ${CMAKE_CURRENT_LIST_DIR}/caffeDepend.patch
+                        WORKING_DIRECTORY ${CAFFE_DEPENDENCIES_DIR}
+        )
+		file(REMOVE_RECURSE ${CAFFE_DEPENDENCIES_DIR}/libraries/include/opencv)
+		file(REMOVE_RECURSE ${CAFFE_DEPENDENCIES_DIR}/libraries/include/opencv2)
+		file(REMOVE_RECURSE ${CAFFE_DEPENDENCIES_DIR}/libraries/x64)
+		file(REMOVE ${CAFFE_DEPENDENCIES_DIR}/libraries/OpenCVConfig.cmake)
+		file(REMOVE ${CAFFE_DEPENDENCIES_DIR}/libraries/OpenCVConfig-version.cmake)
+
+    endif()
+    if(EXISTS ${CAFFE_DEPENDENCIES_DIR}/libraries/caffe-builder-config.cmake)
+        include(${CAFFE_DEPENDENCIES_DIR}/libraries/caffe-builder-config.cmake)
+    else()
+        message(FATAL_ERROR "Something went wrong while dowloading dependencies could not open caffe-builder-config.cmake")
+    endif()
+	set(NVCCVH ${CAFFE_DEPENDENCIES_DIR}/libraries/include/boost-1_61/boost/config/compiler/nvcc.hpp)
+    if(CUDA_VERSION GREATER 10.0)
+		if(EXISTS ${NVCCVH})
+		  file(READ "${NVCCVH}" NVCCV_HPP)
+		  string(REGEX REPLACE
+			"#if !defined" "#if 0 // !defined"
+			NVCCV_HPP "${NVCCV_HPP}")
+		  file(WRITE "${NVCCVH}" "${NVCCV_HPP}")
+		endif()
+    endif()
+
+endif()
+
diff --git a/cmake/caffeDepend.patch b/cmake/caffeDepend.patch
new file mode 100644
index 0000000..3614e89
--- /dev/null
+++ b/cmake/caffeDepend.patch
@@ -0,0 +1,65 @@
+diff -uprN libraries_v140_x64_py35_1.1.0.org/libraries/caffe-builder-config.cmake libraries_v140_x64_py35_1.1.0.old/libraries/caffe-builder-config.cmake
+--- libraries_v140_x64_py35_1.1.0.org/libraries/caffe-builder-config.cmake	2017-03-04 11:25:30.000000000 +0900
++++ libraries_v140_x64_py35_1.1.0.old/libraries/caffe-builder-config.cmake	2020-02-16 15:13:48.414547100 +0900
+@@ -49,10 +49,10 @@ set(OpenBLAS_LIB ${CMAKE_CURRENT_LIST_DI
+ 
+ 
+ # OPENCV config
+-set(OPENCV_DIR ${CMAKE_CURRENT_LIST_DIR}  CACHE PATH "")
+-set(opencv_DIR ${CMAKE_CURRENT_LIST_DIR}  CACHE PATH "")
+-set(OpenCV_DIR ${CMAKE_CURRENT_LIST_DIR}  CACHE PATH "")
+-set(OpenCV_STATIC OFF CACHE BOOL "")
++# set(OPENCV_DIR ${CMAKE_CURRENT_LIST_DIR}  CACHE PATH "")
++# set(opencv_DIR ${CMAKE_CURRENT_LIST_DIR}  CACHE PATH "")
++# set(OpenCV_DIR ${CMAKE_CURRENT_LIST_DIR}  CACHE PATH "")
++# set(OpenCV_STATIC OFF CACHE BOOL "")
+ 
+ 
+ # PROTOBUF config
+diff -uprN libraries_v140_x64_py35_1.1.0.org/libraries/cmake/leveldb-targets-RelWithDebInfo.cmake libraries_v140_x64_py35_1.1.0.old/libraries/cmake/leveldb-targets-RelWithDebInfo.cmake
+--- libraries_v140_x64_py35_1.1.0.org/libraries/cmake/leveldb-targets-RelWithDebInfo.cmake	1970-01-01 09:00:00.000000000 +0900
++++ libraries_v140_x64_py35_1.1.0.old/libraries/cmake/leveldb-targets-RelWithDebInfo.cmake	2020-02-11 21:20:17.096281700 +0900
+@@ -0,0 +1,20 @@
++#----------------------------------------------------------------
++# Generated CMake target import file for configuration "WithDebInfo".
++#----------------------------------------------------------------
++
++# Commands may need to know the format version.
++set(CMAKE_IMPORT_FILE_VERSION 1)
++
++# Import target "leveldb" for configuration "WithDebInfo"
++set_property(TARGET leveldb APPEND PROPERTY IMPORTED_CONFIGURATIONS RELWITHDEBINFO)
++set_target_properties(leveldb PROPERTIES
++  IMPORTED_LINK_INTERFACE_LANGUAGES_RELWITHDEBINFO "CXX"
++  IMPORTED_LINK_INTERFACE_LIBRARIES_RELWITHDEBINFO "${CMAKE_CURRENT_LIST_DIR}/../lib/boost_date_time-vc140-mt-1_61.lib;${CMAKE_CURRENT_LIST_DIR}/../lib/boost_filesystem-vc140-mt-1_61.lib;${CMAKE_CURRENT_LIST_DIR}/../lib/boost_system-vc140-mt-1_61.lib"
++  IMPORTED_LOCATION_RELWITHDEBINFO "${_IMPORT_PREFIX}/lib/leveldb.lib"
++  )
++
++list(APPEND _IMPORT_CHECK_TARGETS leveldb )
++list(APPEND _IMPORT_CHECK_FILES_FOR_leveldb "${_IMPORT_PREFIX}/lib/leveldb.lib" )
++
++# Commands beyond this point should not need to know the version.
++set(CMAKE_IMPORT_FILE_VERSION)
+diff -uprN libraries_v140_x64_py35_1.1.0.org/libraries/include/boost-1_61/boost/config/compiler/nvcc.hpp libraries_v140_x64_py35_1.1.0.old/libraries/include/boost-1_61/boost/config/compiler/nvcc.hpp
+--- libraries_v140_x64_py35_1.1.0.org/libraries/include/boost-1_61/boost/config/compiler/nvcc.hpp	2017-03-04 11:41:32.000000000 +0900
++++ libraries_v140_x64_py35_1.1.0.old/libraries/include/boost-1_61/boost/config/compiler/nvcc.hpp	2020-02-15 16:10:39.909375400 +0900
+@@ -19,6 +19,6 @@
+ // https://svn.boost.org/trac/boost/ticket/11897
+ // This is fixed in 7.5. As the following version macro was introduced in 7.5 an existance
+ // check is enough to detect versions < 7.5
+-#if !defined(__CUDACC_VER__) || (__CUDACC_VER__ < 70500)
++#if 0 // !defined(__CUDACC_VER__) || (__CUDACC_VER__ < 70500)
+ #   define BOOST_NO_CXX11_VARIADIC_TEMPLATES
+ #endif
+diff -uprN libraries_v140_x64_py35_1.1.0.org/libraries/include/google/protobuf/message_lite.h libraries_v140_x64_py35_1.1.0.old/libraries/include/google/protobuf/message_lite.h
+--- libraries_v140_x64_py35_1.1.0.org/libraries/include/google/protobuf/message_lite.h	2016-09-24 11:12:45.000000000 +0900
++++ libraries_v140_x64_py35_1.1.0.old/libraries/include/google/protobuf/message_lite.h	2020-02-15 15:27:11.498614500 +0900
+@@ -244,7 +244,7 @@ class LIBPROTOBUF_EXPORT MessageLite {
+   //
+   // ByteSize() is generally linear in the number of fields defined for the
+   // proto.
+-  virtual int ByteSize() const { return ByteSizeLong(); }
++  virtual int ByteSize() const { return (int)ByteSizeLong(); }
+   virtual size_t ByteSizeLong() const;
+ 
+   // Serializes the message without recomputing the size.  The message must
diff --git a/cmake/lint.cmake b/cmake/lint.cmake
index 70a0065..8cca27d 100644
--- a/cmake/lint.cmake
+++ b/cmake/lint.cmake
@@ -1,6 +1,10 @@
 
 set(CMAKE_SOURCE_DIR ..)
-set(LINT_COMMAND ${CMAKE_SOURCE_DIR}/scripts/cpp_lint.py)
+set(python_executable)
+if(WIN32)
+  set(python_executable ${PYTHON_EXECUTABLE})
+endif()
+set(LINT_COMMAND ${python_executable} ${CMAKE_SOURCE_DIR}/scripts/cpp_lint.py)
 set(SRC_FILE_EXTENSIONS h hpp hu c cpp cu cc)
 set(EXCLUDE_FILE_EXTENSTIONS pb.h pb.cc)
 set(LINT_DIRS include src/caffe examples tools python matlab)
@@ -22,7 +26,9 @@ foreach(ext ${EXCLUDE_FILE_EXTENSTIONS})
 endforeach()
 
 # exclude generated pb files
-list(REMOVE_ITEM LINT_SOURCES ${EXCLUDED_FILES})
+if(EXCLUDED_FILES)
+  list(REMOVE_ITEM LINT_SOURCES ${EXCLUDED_FILES})
+endif()
 
 execute_process(
     COMMAND ${LINT_COMMAND} ${LINT_SOURCES}
diff --git a/demo_yolo_detect.sh b/demo_yolo_detect.sh
new file mode 100644
index 0000000..0b8c9be
--- /dev/null
+++ b/demo_yolo_detect.sh
@@ -0,0 +1,5 @@
+yolo_detect E:/src/ws/Release/Depends/src/caffe/models/cityscapes/mobilenet_yolov3_deploy.prototxt \
+D:/dataset/citycapes/mobilenet_yolov3_deploy_iter_46000.caffemodel \
+-detect_mode 1 -cpu_mode gpu -file_type image -wait_time 1 -mean_value 1.0,1.0,1.0 -normalize_value 0.007843 -confidence_threshold 0.3 \
+-indir D:/dataset/aie/test_images -ext jpg
+
diff --git a/examples/CMakeLists.txt b/examples/CMakeLists.txt
index 43bbcb8..e3db723 100644
--- a/examples/CMakeLists.txt
+++ b/examples/CMakeLists.txt
@@ -7,7 +7,7 @@ foreach(source_file ${examples_srcs})
   # get folder name
   get_filename_component(path ${source_file} PATH)
   get_filename_component(folder ${path} NAME_WE)
-    
+  message(STATUS "config example :" ${name})
   add_executable(${name} ${source_file})
   target_link_libraries(${name} ${Caffe_LINK})
   caffe_default_properties(${name})
diff --git a/examples/ssd/ssd_detect.cpp b/examples/ssd/ssd_detect.cpp
index 7aefa3c..8b8f57f 100644
--- a/examples/ssd/ssd_detect.cpp
+++ b/examples/ssd/ssd_detect.cpp
@@ -1,3 +1,5 @@
+// models\mobilenetv2_voc\ssd_lite\deploy.prototxt data\carwindow\ssd_lite_deploy_iter_3.caffemodel data\carwindow\Images -file_type image -custom 1 -mean_value 1.0,1.0,1.0 -normalize_value 0.007843 -confidence_threshold 0.3
+
 // This is a demo code for using a SSD model to do detection.
 // The code is modified from examples/cpp_classification/classification.cpp.
 // Usage:
@@ -29,44 +31,8 @@
 #include <utility>
 #include <vector>
 #include "caffe/util/benchmark.hpp"
-//#define custom_class
-#ifdef custom_class
-//char* CLASSES[6] = { "__background__",
-//"bicyle", "car", "motorbike", "person","cones"
-//};
-//char* CLASSES[5] = { "__background__",
-//"big car","car", "motorbike","person"
-//};
-char* CLASSES[81] = { "__background__",
-"person", "bicycle", "car", "motorcycle",
-"airplane", "bus", "train", "truck", "boat",
-"traffic light", "fire hydrant", "stop sign", "parking meter",
-"bench", "bird", "cat",
-"dog", "horse", "sheep", "cow",
-"elephant", "bear", "zebra", "giraffe" ,
-"backpack", "umbrella", "handbag", "tie" ,
-"suitcase", "frisbee", "skis", "snowboard" ,
-"sports ball", "kite", "baseball bat", "baseball glove" ,
-"skateboard", "surfboard", "tennis racket", "bottle" ,
-"wine glass", "cup", "fork", "knife" ,
-"spoon", "bowl", "banana", "apple" ,
-"sandwich", "orange", "broccoli", "carrot" ,
-"hot dog", "pizza", "donut", "cake" ,
-"chair", "couch", "potted plant", "bed" ,
-"dining table", "toilet", "tv", "laptop" ,
-"mouse", "remote", "keyboard", "cell phone" ,
-"microwave", "oven", "toaster", "sink" ,
-"refrigerator", "book", "clock", "vase" ,
-"scissors", "teddy bear", "hair drier", "toothbrush" ,
-};
-#else
-char* CLASSES[21] = { "__background__",
-"aeroplane", "bicycle", "bird", "boat",
-"bottle", "bus", "car", "cat", "chair",
-"cow", "diningtable", "dog", "horse",
-"motorbike", "person", "pottedplant",
-"sheep", "sofa", "train", "tvmonitor" };
-#endif
+#include "ssd_detect.hpp"
+
 #ifdef USE_OPENCV
 using namespace caffe;  // NOLINT(build/namespaces)
 
@@ -127,10 +93,7 @@ Detector::Detector(const string& model_file,
   SetMean(mean_file, mean_value);
   nor_val = normalize_value;
 }
-float sec(clock_t clocks)
-{
-	return (float)clocks / CLOCKS_PER_SEC;
-}
+
 std::vector<vector<float> > Detector::Detect(const cv::Mat& img) {
   Blob<float>* input_layer = net_->input_blobs()[0];
   input_layer->Reshape(1, num_channels_,
@@ -322,6 +285,7 @@ void Detector::Preprocess(const cv::Mat& img,
     << "Input channels are not wrapping the input layer of the network.";
 }
 
+
 DEFINE_string(mean_file, "",
     "The mean file used to subtract from the input image.");
 DEFINE_string(mean_value, "104,117,123",
@@ -338,6 +302,9 @@ DEFINE_double(normalize_value, 1.0,
 	"Normalize image to 0~1");
 DEFINE_int32(wait_time, 1000,
 	"cv imshow window waiting time ");
+DEFINE_int32(custom, 0,
+	"custom type code ");
+
 int main(int argc, char** argv) {
   ::google::InitGoogleLogging(argv[0]);
   // Print output to stderr (while still logging)
@@ -366,6 +333,8 @@ int main(int argc, char** argv) {
   const float& confidence_threshold = FLAGS_confidence_threshold;
   const float& normalize_value = FLAGS_normalize_value;
   const int& wait_time = FLAGS_wait_time;
+  const int& custom = FLAGS_custom;
+  
   // Initialize the network.
   
   Detector detector(model_file, weights_file, mean_file, mean_value, confidence_threshold, normalize_value);
@@ -382,15 +351,13 @@ int main(int argc, char** argv) {
   std::ostream out(buf);
 
   // Process image one by one.
-  //std::ifstream infile(argv[3]);
-  const string &indir = "//data";
+  fstream infile();
+  string indir(argv[3]);
   std::string file;
   out << file_type <<"demo";
   if (file_type == "image")
   {
-	  char buf[1000];
-	  sprintf(buf, "%s/*.jpg", "data//");
-	  cv::String path(buf); //select only jpg
+	  cv::String path(indir + "/*.jpg"); //select only jpg
 
 	  vector<cv::String> fn;
 	  vector<cv::Mat> data;
@@ -426,24 +393,27 @@ int main(int argc, char** argv) {
 				  pt1.y = (img.rows*d[4]);
 				  pt2.x = (img.cols*d[5]);
 				  pt2.y = (img.rows*d[6]);
-				  cv::rectangle(img, pt1, pt2, cv::Scalar(0, 255, 0), 1, 8, 0);
+				  int index = static_cast<int>(d[1]);
+				  cv::Scalar c(textColor(index));
+				  pt1 = marker(img, pt1, pt2, cv::Scalar(0), index, custom);
 
 				  char label[100];
-				  sprintf(label, "%s,%f", CLASSES[static_cast<int>(d[1])], score);
+				  sprintf(label, "%s,%.2f", CLASSES[index], score);
 				  int baseline;
 				  cv::Size size = cv::getTextSize(label, cv::FONT_HERSHEY_SIMPLEX, 0.5, 0, &baseline);
 				  cv::Point pt3;
 				  pt3.x = pt1.x + size.width;
 				  pt3.y = pt1.y - size.height;
-				  cv::rectangle(img, pt1, pt3, cv::Scalar(0, 255, 0), -1);
-
-				  cv::putText(img, label, pt1, cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 0, 0));
+				  cv::putText(img, label, pt1, cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0),3);
+				  cv::putText(img, label, pt1, cv::FONT_HERSHEY_SIMPLEX, 0.5, c);
 			  }
 		  }
 		  cv::imshow("show", img);
-		  sprintf(buf, "out//%05d.jpg", k);
-		  cv::imwrite(buf, img);
-		  cv::waitKey(wait_time);
+		  path = cv::format("out//%05d.jpg", k);
+		  cv::imwrite(path, img);
+		  if (-1 != cv::waitKey(wait_time)) {
+			  break;
+		  }
 		  data.push_back(img);
 	  }
   }
@@ -489,10 +459,8 @@ int main(int argc, char** argv) {
 					  pt2.x = (img.cols*d[5]);
 					  pt2.y = (img.rows*d[6]);
 					  int index = static_cast<int>(d[1]);
-					  int green = 255 * ((index + 1) % 3);
-					  int blue = 255 * (index % 3);
-					  int red = 255 * ((index + 1) % 4);
-					  cv::rectangle(img, pt1, pt2, cv::Scalar(red, green, blue), 1, 8, 0);
+					  cv::Scalar c(textColor(index));
+					  pt1 = marker(img, pt1, pt2, c, index, custom);
 
 					  char label[100];
 					  sprintf(label, "%s,%f", CLASSES[static_cast<int>(d[1])], score);
@@ -502,7 +470,7 @@ int main(int argc, char** argv) {
 					  pt3.x = pt1.x + size.width;
 					  pt3.y = pt1.y - size.height;
 
-					  cv::rectangle(img, pt1, pt3, cv::Scalar(red, green, blue), -1);
+					  cv::rectangle(img, pt1, pt3, c, -1);
 
 					  cv::putText(img, label, pt1, cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 0, 0));
 				  }
@@ -570,10 +538,8 @@ int main(int argc, char** argv) {
 					  pt2.x = (img.cols*d[5]);
 					  pt2.y = (img.rows*d[6]);
 					  int index = static_cast<int>(d[1]);
-					  int green = 255 * ((index + 1) % 3);
-					  int blue = 255 * (index % 3);
-					  int red = 255 * ((index + 1) % 2);
-					  cv::rectangle(img, pt1, pt2, cv::Scalar(red, green, blue), 1, 8, 0);
+					  cv::Scalar c(textColor(index));
+					  pt1 = marker(img, pt1, pt2, c, index, custom);
 
 					  char label[100];
 					  sprintf(label, "%s,%f", CLASSES[static_cast<int>(d[1])], score);
@@ -583,7 +549,7 @@ int main(int argc, char** argv) {
 					  pt3.x = pt1.x + size.width;
 					  pt3.y = pt1.y - size.height;
 
-					  cv::rectangle(img, pt1, pt3, cv::Scalar(red, green, blue), -1);
+					  cv::rectangle(img, pt1, pt3, c, -1);
 
 					  cv::putText(img, label, pt1, cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 0, 0));
 
diff --git a/examples/ssd/ssd_detect.hpp b/examples/ssd/ssd_detect.hpp
new file mode 100644
index 0000000..31a2a00
--- /dev/null
+++ b/examples/ssd/ssd_detect.hpp
@@ -0,0 +1,69 @@
+
+
+#define CLASSES  classes[custom]
+static char* classes[2][21] = { { "__background__",
+"aeroplane", "bicycle", "bird", "boat",
+"bottle", "bus", "car", "cat", "chair",
+"cow", "diningtable", "dog", "horse",
+"motorbike", "person", "pottedplant",
+"sheep", "sofa", "train", "tvmonitor" },
+ { "__background__",
+"aeroplane", "bicycle", "bird", "boat",		// 1 - 4
+"bottle", "bus", "car", "cat", "chair",
+"cow", "diningtable", "dog", "horse",
+"motorbike", "person", "H_D",			// 15 - 17 
+"H_U", "V_D", "V_U", "tvmonitor" } };		// 18 - 20
+
+static inline float sec(clock_t clocks)
+{
+	return (float)clocks / CLOCKS_PER_SEC;
+}
+enum {
+	H_D = 16, H_U, V_D, V_U // =EAE
+};
+
+static inline cv::Point  marker(cv::InputOutputArray img, cv::Point pt1, cv::Point pt2,
+	const cv::Scalar& color, int index, int code = 0)
+{
+	cv::Scalar c(color);
+	int i=index;
+	if (color == cv::Scalar(0)) {
+		if (i == H_D || i == H_U) {
+			c = cv::Scalar(122, 122, 255);
+		}
+		else if (i == V_U || i == V_D) {
+			c = cv::Scalar(255, 122, 122);
+		}
+		else {
+			c = cv::Scalar(0, 255, 0);
+		}
+	}
+	if ((i == H_D || i == V_D) && code) {
+		cv::line(img, pt1, pt2, c, 1, 8, 0);
+	} else if ((i == H_U || i == V_U) && code) {
+		cv::Point p[2] = { {pt1.x,pt2.y}, {pt2.x,pt1.y} };
+		cv::line(img, p[0], p[1], c, 1, 8, 0);
+		return p[0];
+	} else {
+		cv::rectangle(img, pt1, pt2, c, 1, 8, 0);
+	}
+	return pt1;
+}
+static inline cv::Scalar textColor(int index){
+	int green = 255 * ((index + 1) % 3);
+	int blue = 255 * (index % 3);
+	int red = 255 * ((index + 1) % 2);
+
+	cv::Scalar c(red, green, blue);
+	int i = index;
+	if (i == H_D || i == H_U) {
+		c = cv::Scalar(122, 122, 255);
+	}
+	else if (i == V_U || i == V_D) {
+		c = cv::Scalar(255, 122, 122);
+	}
+	else {
+		c = cv::Scalar(0, 255, 0);
+	}
+	return c;
+}
\ No newline at end of file
diff --git a/include/caffe/blob.hpp b/include/caffe/blob.hpp
index 2f59471..bda3852 100644
--- a/include/caffe/blob.hpp
+++ b/include/caffe/blob.hpp
@@ -71,7 +71,7 @@ class Blob {
   inline int shape(int index) const {
     return shape_[CanonicalAxisIndex(index)];
   }
-  inline int num_axes() const { return shape_.size(); }
+  inline int num_axes() const { return (int)shape_.size(); }
   inline int count() const { return count_; }
 
   /**
diff --git a/include/caffe/layer_factory.hpp b/include/caffe/layer_factory.hpp
index 2369c13..5960620 100644
--- a/include/caffe/layer_factory.hpp
+++ b/include/caffe/layer_factory.hpp
@@ -58,10 +58,7 @@ class LayerRegistry {
   typedef shared_ptr<Layer<Dtype> > (*Creator)(const LayerParameter&);
   typedef std::map<string, Creator> CreatorRegistry;
 
-  static CreatorRegistry& Registry() {
-    static CreatorRegistry* g_registry_ = new CreatorRegistry();
-    return *g_registry_;
-  }
+  static CreatorRegistry& Registry();
 
   // Adds a creator.
   static void AddCreator(const string& type, Creator creator) {
@@ -115,12 +112,12 @@ class LayerRegistry {
 
 template <typename Dtype>
 class LayerRegisterer {
- public:
-  LayerRegisterer(const string& type,
-                  shared_ptr<Layer<Dtype> > (*creator)(const LayerParameter&)) {
-    // LOG(INFO) << "Registering layer type: " << type;
-    LayerRegistry<Dtype>::AddCreator(type, creator);
-  }
+public:
+	LayerRegisterer(const string& type,
+		shared_ptr<Layer<Dtype> > (*creator)(const LayerParameter&)) {
+		// LOG(INFO) << "Registering layer type: " << type;
+		LayerRegistry<Dtype>::AddCreator(type, creator);
+	}
 };
 
 
diff --git a/include/caffe/layers/yolo_detection_output_layer.hpp b/include/caffe/layers/yolo_detection_output_layer.hpp
index 21fc356..9ae357e 100644
--- a/include/caffe/layers/yolo_detection_output_layer.hpp
+++ b/include/caffe/layers/yolo_detection_output_layer.hpp
@@ -37,7 +37,7 @@ class YoloDetectionOutputLayer : public Layer<Dtype> {
   virtual void Reshape(const vector<Blob<Dtype>*>& bottom,
       const vector<Blob<Dtype>*>& top);
 
-  virtual inline const char* type() const { return "DetectionOutput"; }
+  virtual inline const char* type() const { return "YoloDetectionOutput"; }
   virtual inline int MinBottomBlobs() const { return 1; }
   //virtual inline int MaxBottomBlobs() const { return 4; }
   virtual inline int ExactNumTopBlobs() const { return 1; }
diff --git a/include/caffe/layers/yolov3_detection_output_layer.hpp b/include/caffe/layers/yolov3_detection_output_layer.hpp
index ad6eca8..45144e4 100644
--- a/include/caffe/layers/yolov3_detection_output_layer.hpp
+++ b/include/caffe/layers/yolov3_detection_output_layer.hpp
@@ -16,18 +16,18 @@
 
 
 namespace caffe {
-template <typename Dtype>
-class PredictionResult {
-public:
-	Dtype x;
-	Dtype y;
-	Dtype w;
-	Dtype h;
-	Dtype objScore;
-	Dtype classScore;
-	Dtype confidence;
-	int classType;
-};
+//template <typename Dtype>
+//class PredictionResult {
+//public:
+//	Dtype x;
+//	Dtype y;
+//	Dtype w;
+//	Dtype h;
+//	Dtype objScore;
+//	Dtype classScore;
+//	Dtype confidence;
+//	int classType;
+//};
 /**
  * @brief Generate the detection output based on location and confidence
  * predictions by doing non maximum suppression.
diff --git a/include/caffe/layers/yolov3_layer.hpp b/include/caffe/layers/yolov3_layer.hpp
index 3e90e89..02cbf0a 100644
--- a/include/caffe/layers/yolov3_layer.hpp
+++ b/include/caffe/layers/yolov3_layer.hpp
@@ -27,17 +27,17 @@ public:
 
   virtual inline const char* type() const { return "Yolov3"; }
   
-  class PredictionResult {
-  public:
-    Dtype x;
-    Dtype y;
-    Dtype w;
-    Dtype h;
-    Dtype objScore;
-    Dtype classScore;
-    Dtype confidence;
-    int classType;
-  };
+  //class PredictionResult {
+  //public:
+  //  Dtype x;
+  //  Dtype y;
+  //  Dtype w;
+  //  Dtype h;
+  //  Dtype objScore;
+  //  Dtype classScore;
+  //  Dtype confidence;
+  //  int classType;
+  //};
 
   protected:
   virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom,
diff --git a/include/caffe/net.hpp b/include/caffe/net.hpp
index fd5b00c..3ac57a1 100644
--- a/include/caffe/net.hpp
+++ b/include/caffe/net.hpp
@@ -195,8 +195,8 @@ class Net {
     return param_display_names_;
   }
   /// @brief Input and output blob numbers
-  inline int num_inputs() const { return net_input_blobs_.size(); }
-  inline int num_outputs() const { return net_output_blobs_.size(); }
+  inline int num_inputs() const { return (int)net_input_blobs_.size(); }
+  inline int num_outputs() const { return (int)net_output_blobs_.size(); }
   inline const vector<Blob<Dtype>*>& input_blobs() const {
     return net_input_blobs_;
   }
diff --git a/include/caffe/util/io.hpp b/include/caffe/util/io.hpp
index 263976e..af1adee 100644
--- a/include/caffe/util/io.hpp
+++ b/include/caffe/util/io.hpp
@@ -47,7 +47,7 @@ inline void MakeTempFilename(string* temp_filename) {
     temp_files_subpath = path_string;
   }
   *temp_filename =
-    (temp_files_subpath/caffe::format_int(next_temp_file++, 9)).string();
+    (temp_files_subpath/caffe::format_int((int)next_temp_file++, 9)).string();
 }
 
 inline void GetTempDirname(string* temp_dirname) {
@@ -79,7 +79,7 @@ inline void GetTempFilename(string* temp_filename) {
     temp_files_subpath = path_string;
   }
   *temp_filename =
-    (temp_files_subpath/caffe::format_int(next_temp_file++, 9)).string();
+    (temp_files_subpath/caffe::format_int((int)next_temp_file++, 9)).string();
 }
 
 bool ReadProtoFromTextFile(const char* filename, Message* proto);
diff --git a/include/caffe/util/math_functions.hpp b/include/caffe/util/math_functions.hpp
index 64cd2f4..594f96a 100644
--- a/include/caffe/util/math_functions.hpp
+++ b/include/caffe/util/math_functions.hpp
@@ -12,9 +12,9 @@
 
 namespace caffe {
 
-static inline float logistic_activate(float x) { return 1. / (1. + exp(-x)); }
-static inline float logistic_gradient(float x) { return (1 - x)*x; }
-static inline float hard_sigmoid(float x) { return  std::min(1., std::max(0., x * 0.2 + 0.5)); }
+static inline float logistic_activate(float x) { return (float)(1. / (1. + exp(-x))); }
+static inline float logistic_gradient(float x) { return (float)((1 - x)*x); }
+static inline float hard_sigmoid(float x) { return  (float)std::min(1., std::max(0., x * 0.2 + 0.5)); }
 
 template <typename Dtype>
 void caffe_cpu_logistic_activate(Dtype *x, const int n);
diff --git a/python/caffe/_caffe.cpp b/python/caffe/_caffe.cpp
index 82bf21e..bd12976 100644
--- a/python/caffe/_caffe.cpp
+++ b/python/caffe/_caffe.cpp
@@ -59,9 +59,9 @@ void InitLogLevel(int level) {
   FLAGS_minloglevel = level;
   InitLog();
 }
-void InitLogLevelPipe(int level, bool stderr) {
+void InitLogLevelPipe(int level, bool std_err) {
   FLAGS_minloglevel = level;
-  FLAGS_logtostderr = stderr;
+  FLAGS_logtostderr = std_err;
   InitLog();
 }
 void Log(const string& s) {
diff --git a/python/requirements.txt b/python/requirements.txt
index e7d89e6..f4b29b8 100644
--- a/python/requirements.txt
+++ b/python/requirements.txt
@@ -5,7 +5,6 @@ scikit-image>=0.9.3
 matplotlib>=1.3.1
 ipython>=3.0.0
 h5py>=2.2.0
-leveldb>=0.191
 networkx>=1.8.1
 nose>=1.3.0
 pandas>=0.12.0
diff --git a/src/caffe/CMakeLists.txt b/src/caffe/CMakeLists.txt
index 4a80556..7d4f659 100644
--- a/src/caffe/CMakeLists.txt
+++ b/src/caffe/CMakeLists.txt
@@ -8,6 +8,11 @@ caffe_default_properties(caffeproto)
 target_link_libraries(caffeproto PUBLIC ${PROTOBUF_LIBRARIES})
 target_include_directories(caffeproto PUBLIC ${PROTOBUF_INCLUDE_DIR})
 
+#if(MSVC)
+#    message(STATUS "add_compile_options(caffeproto /wd4267.")
+#	add_compile_options(caffeproto /wd4267)
+#endif()
+
 list(INSERT Caffe_LINKER_LIBS 0 PUBLIC caffeproto) # note, crucial to prepend!
 
 # --[ Caffe library
@@ -15,8 +20,41 @@ list(INSERT Caffe_LINKER_LIBS 0 PUBLIC caffeproto) # note, crucial to prepend!
 # creates 'test_srcs', 'srcs', 'test_cuda', 'cuda' lists
 caffe_pickup_caffe_sources(${PROJECT_SOURCE_DIR})
 
+# add this option here since CUDA will not honor
+# target_compile_definitions
+if(MSVC AND NOT BUILD_SHARED_LIBS)
+  set(_caffe_static_compile_def -DCAFFE_BUILDING_STATIC_LIB)
+endif()
+
 if(HAVE_CUDA)
+  # collect any compile definitions from imported targets. This important so that
+  # preprocessor macros such as GLOG_NO_ABBREVIATED_SEVERITIES are defined.
+  # this is required since CUDA macros do not honor the INTERFACE_COMPILE_DEFINITIONS
+  unset(__cuda_options)
+  foreach(__lib ${Caffe_LINKER_LIBS})
+    if(TARGET ${__lib})
+      get_target_property(__interface_compile_definitions ${__lib} INTERFACE_COMPILE_DEFINITIONS)
+      if(__interface_compile_definitions)
+        foreach(__def ${__interface_compile_definitions})
+          # espace any parentheses because they are failing the build
+          # see cmake issue https://cmake.org/Bug/view.php?id=16065
+          string(REPLACE "(" "\\\(" __def_escaped ${__def})
+          string(REPLACE ")" "\\\)" __def_escaped ${__def_escaped})
+          # add the required -D flag
+          list(APPEND __cuda_options "-D${__def_escaped}")
+        endforeach()
+      endif()
+    endif()
+  endforeach()
+  list(APPEND __cuda_options ${_caffe_static_compile_def})
+  # add the required definitions
+  add_definitions(${__cuda_options})
+  # it seems that using the OPTIONS argument like:
+  # caffe_cuda_compile(cuda_objs ${cuda} OPTIONS ${__cuda_options})
+  # does not work. Use add/remove_definitions instead.
   caffe_cuda_compile(cuda_objs ${cuda})
+  # remove them
+  remove_definitions(${__cuda_options})
   list(APPEND srcs ${cuda_objs} ${cuda})
 endif()
 
@@ -29,12 +67,41 @@ target_include_directories(caffe ${Caffe_INCLUDE_DIRS}
                                  $<INSTALL_INTERFACE:include>)
 target_compile_definitions(caffe ${Caffe_DEFINITIONS})
 if(Caffe_COMPILE_OPTIONS)
-  target_compile_options(caffe ${Caffe_COMPILE_OPTIONS})
+  target_compile_options(caffe ${Caffe_COMPILE_OPTIONS} /MP
+  /wd4819 /wd4706 /wd4702 /wd4505 /wd4793 /wd4267 /wd4244 /wd5999 /wd4661)
 endif()
 set_target_properties(caffe PROPERTIES
     VERSION   ${CAFFE_TARGET_VERSION}
     SOVERSION ${CAFFE_TARGET_SOVERSION}
     )
+if(MSVC AND BUILD_SHARED_LIBS)
+  # CMake 3.4 introduced a WINDOWS_EXPORT_ALL_SYMBOLS target property that makes it possible to
+  # build shared libraries without using the usual declspec() decoration.
+  # See: https://blog.kitware.com/create-dlls-on-windows-without-declspec-using-new-cmake-export-all-feature/
+  # and https://cmake.org/cmake/help/v3.5/prop_tgt/WINDOWS_EXPORT_ALL_SYMBOLS.html
+  # for details.
+  set_target_properties(caffe PROPERTIES WINDOWS_EXPORT_ALL_SYMBOLS TRUE)
+elseif(MSVC AND NOT BUILD_SHARED_LIBS)
+  # add a custom build command that generates a list of symbols
+  # to force linking. This is required because MSVC as nothing
+  # the whole-archive option
+  windows_create_link_header(caffe ${caffe_symbols_hdr})
+  get_filename_component(_name ${caffe_symbols_hdr} NAME)
+  set(CAFFE_INCLUDE_SYMBOLS "#include \"caffe/${_name}\"")
+  # definition needed to include CMake generated files
+  target_compile_definitions(caffe PRIVATE ${_caffe_static_compile_def}
+                                   PUBLIC -DCMAKE_WINDOWS_BUILD)
+endif()
+if(MSVC)
+  # Disable Boost autolinking for consuming projects
+  target_compile_definitions(caffe PUBLIC -DBOOST_ALL_NO_LIB)
+  #add_compile_options(/dw4819 /dw4793 /dw4244 /dw4267)
+endif()
+if(MSVC AND USE_NCCL)
+  add_dependencies(caffe nccl)
+endif()
+
+configure_file(${caffe_export_hdr_in} ${caffe_export_hdr})
 
 # ---[ Tests
  add_subdirectory(test)
@@ -43,6 +110,9 @@ set_target_properties(caffe PROPERTIES
 install(DIRECTORY ${Caffe_INCLUDE_DIR}/caffe DESTINATION ${CMAKE_INSTALL_INCLUDEDIR})
 install(FILES ${proto_hdrs} DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/caffe/proto)
 install(TARGETS caffe caffeproto EXPORT CaffeTargets DESTINATION ${CMAKE_INSTALL_LIBDIR})
+if(MSVC AND NOT BUILD_SHARED_LIBS)
+  install(FILES ${caffe_export_hdr} ${caffe_symbols_hdr} DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/caffe)
+endif()
 
 file(WRITE ${PROJECT_BINARY_DIR}/__init__.py)
 list(APPEND proto_python ${PROJECT_BINARY_DIR}/__init__.py)
diff --git a/src/caffe/Pruner.cpp b/src/caffe/Pruner.cpp
index 524c5ea..8626c27 100644
--- a/src/caffe/Pruner.cpp
+++ b/src/caffe/Pruner.cpp
@@ -3,7 +3,16 @@
 #include <cmath>
 #include <fstream>
 #include "caffe/util/math_functions.hpp"
+#ifdef _WIN32
+#include <chrono>
+#include <thread>
+void inline sleep(int second) {
+	std::chrono::milliseconds dura(second);
+	std::this_thread::sleep_for(dura);
+}
+#else
 #include <unistd.h>
+#endif
 #include <stdio.h>
 #include <stdlib.h>
 using namespace caffe;
@@ -610,7 +619,7 @@ int  Pruner::writePrototxt(const string prototxt1, const string prototxt2){
 			}
 		}
 		else{
-      if(str.find("group:") != -1 and overwrite)
+      if(str.find("group:") != -1 && overwrite)
       {
         std::string previous = str.substr(0, str.find(":")+2);
         std::string group_num = str.substr(str.find(":")+2);
diff --git a/src/caffe/common.cpp b/src/caffe/common.cpp
index 4f6f9bc..59874a5 100644
--- a/src/caffe/common.cpp
+++ b/src/caffe/common.cpp
@@ -1,3 +1,8 @@
+#if defined(_MSC_VER)
+#include <process.h>
+#define getpid() _getpid()
+#endif
+
 #include <boost/thread.hpp>
 #include <glog/logging.h>
 #include <cmath>
@@ -46,7 +51,10 @@ void GlobalInit(int* pargc, char*** pargv) {
   // Google logging.
   ::google::InitGoogleLogging(*(pargv)[0]);
   // Provide a backtrace on segfault.
+  // Windows port of glogs doesn't have this function built
+#if !defined(_MSC_VER)
   ::google::InstallFailureSignalHandler();
+#endif
 }
 
 #ifdef CPU_ONLY  // CPU-only Caffe.
diff --git a/src/caffe/data_transformer.cpp b/src/caffe/data_transformer.cpp
index d20e02d..4e8b0fc 100644
--- a/src/caffe/data_transformer.cpp
+++ b/src/caffe/data_transformer.cpp
@@ -575,8 +575,19 @@ void DataTransformer<Dtype>::DistortImage(const Datum& datum,
     } else {
       cv_img = DecodeDatumToCVMatNative(datum);
     }
-    // Distort the image.
-    cv::Mat distort_img = ApplyDistort(cv_img, param_.distort_param());
+	cv::Mat distort_img;
+	if (cv_img.channels() == 1) {
+		DLOG(ERROR) << "img.channels() == 1";
+		cv::cvtColor(cv_img, cv_img, cv::COLOR_GRAY2BGR);
+		// Distort the image.
+		distort_img = ApplyDistort(cv_img, param_.distort_param());
+//		cv::cvtColor(distort_img, distort_img, cv::COLOR_BGRA2GRAY);
+		distort_datum->set_channels(3);
+	} else {
+		// Distort the image.
+		distort_img = ApplyDistort(cv_img, param_.distort_param());
+	}
+
     // Save the image into datum.
     EncodeCVMatToDatum(distort_img, "jpg", distort_datum);
     distort_datum->set_label(datum.label());
diff --git a/src/caffe/layer_factory.cpp b/src/caffe/layer_factory.cpp
index 9f9026b..8805704 100644
--- a/src/caffe/layer_factory.cpp
+++ b/src/caffe/layer_factory.cpp
@@ -7,8 +7,10 @@
 
 #include "caffe/layer.hpp"
 #include "caffe/layer_factory.hpp"
+#include "caffe/layers/input_layer.hpp"
 #include "caffe/layers/conv_layer.hpp"
 #include "caffe/layers/deconv_layer.hpp"
+#include "caffe/layers/depthwise_conv_layer.hpp"
 #include "caffe/layers/lrn_layer.hpp"
 #include "caffe/layers/pooling_layer.hpp"
 #include "caffe/layers/relu_layer.hpp"
@@ -16,6 +18,26 @@
 #include "caffe/layers/softmax_layer.hpp"
 #include "caffe/layers/tanh_layer.hpp"
 #include "caffe/proto/caffe.pb.h"
+#include "caffe/layers/yolov3_layer.hpp"
+#include "caffe/layers/yolov3_detection_output_layer.hpp"
+#include "caffe/layers/yolo_detection_output_layer.hpp"
+#include "caffe/layers/detection_output_layer.hpp"
+#include "caffe/layers/annotated_data_layer.hpp"
+#include "caffe/layers/detection_evaluate_layer.hpp"
+#include "caffe/layers/inner_product_layer.hpp"
+
+#include "caffe/layers/batch_norm_layer.hpp"
+#include "caffe/layers/scale_layer.hpp"
+#include "caffe/layers/bias_layer.hpp"
+#include "caffe/layers/permute_layer.hpp"
+#include "caffe/layers/flatten_layer.hpp"
+#include "caffe/layers/prior_box_layer.hpp"
+#include "caffe/layers/concat_layer.hpp"
+#include "caffe/layers/multibox_loss_layer.hpp"
+#include "caffe/layers/smooth_L1_loss_layer.hpp"
+#include "caffe/layers/sigmoid_cross_entropy_loss_layer.hpp"
+#include "caffe/layers/reshape_layer.hpp"
+#include "caffe/layers/relu6_layer.hpp"
 
 #ifdef USE_CUDNN
 #include "caffe/layers/cudnn_conv_layer.hpp"
@@ -33,8 +55,29 @@
 #include "caffe/layers/python_layer.hpp"
 #endif
 
+
+#include "caffe/solver.hpp"
+#include "caffe/solver_factory.hpp"
+#include "caffe/sgd_solvers.hpp"
+
+
 namespace caffe {
 
+template <typename Dtype>
+typename LayerRegistry<Dtype>::CreatorRegistry&
+LayerRegistry<Dtype>::Registry() {
+	static CreatorRegistry* g_registry_ = new CreatorRegistry();
+	return *g_registry_;
+}
+
+// Get input layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetInputLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new InputLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Input, GetInputLayer);
+
+
 // Get convolution layer according to engine.
 template <typename Dtype>
 shared_ptr<Layer<Dtype> > GetConvolutionLayer(
@@ -72,15 +115,15 @@ shared_ptr<Layer<Dtype> > GetConvolutionLayer(
     throw;  // Avoids missing return warning
   }
 }
-
 REGISTER_LAYER_CREATOR(Convolution, GetConvolutionLayer);
 
+
 // Get deconvolution layer according to engine.
 template <typename Dtype>
 shared_ptr<Layer<Dtype> > GetDeconvolutionLayer(const LayerParameter& param) {
   ConvolutionParameter conv_param = param.convolution_param();
   ConvolutionParameter_Engine engine = conv_param.engine();
-#ifdef USE_CUDNN
+#ifdef notUSE_CUDNN
   bool use_dilation = false;
   for (int i = 0; i < conv_param.dilation_size(); ++i) {
     if (conv_param.dilation(i) > 1) {
@@ -90,7 +133,7 @@ shared_ptr<Layer<Dtype> > GetDeconvolutionLayer(const LayerParameter& param) {
 #endif
   if (engine == ConvolutionParameter_Engine_DEFAULT) {
     engine = ConvolutionParameter_Engine_CAFFE;
-#ifdef USE_CUDNN
+#ifdef notUSE_CUDNN
     if (!use_dilation) {
       engine = ConvolutionParameter_Engine_CUDNN;
     }
@@ -98,7 +141,7 @@ shared_ptr<Layer<Dtype> > GetDeconvolutionLayer(const LayerParameter& param) {
   }
   if (engine == ConvolutionParameter_Engine_CAFFE) {
     return shared_ptr<Layer<Dtype> >(new DeconvolutionLayer<Dtype>(param));
-#ifdef USE_CUDNN
+#ifdef notUSE_CUDNN
   } else if (engine == ConvolutionParameter_Engine_CUDNN) {
     if (use_dilation) {
       LOG(FATAL) << "CuDNN doesn't support the dilated deconvolution at Layer "
@@ -111,9 +154,47 @@ shared_ptr<Layer<Dtype> > GetDeconvolutionLayer(const LayerParameter& param) {
     throw;  // Avoids missing return warning
   }
 }
-
 REGISTER_LAYER_CREATOR(Deconvolution, GetDeconvolutionLayer);
 
+// Get convolution layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetDepthwiseConvolutionLayer(
+	const LayerParameter& param) {
+	ConvolutionParameter conv_param = param.convolution_param();
+	ConvolutionParameter_Engine engine = conv_param.engine();
+#ifdef USE_CUDNN
+	bool use_dilation = false;
+	for (int i = 0; i < conv_param.dilation_size(); ++i) {
+		if (conv_param.dilation(i) > 1) {
+			use_dilation = true;
+		}
+	}
+#endif
+	if (engine == ConvolutionParameter_Engine_DEFAULT) {
+		engine = ConvolutionParameter_Engine_CAFFE;
+#ifdef USE_CUDNN
+		if (!use_dilation) {
+			engine = ConvolutionParameter_Engine_CUDNN;
+		}
+#endif
+	}
+	if (engine == ConvolutionParameter_Engine_CAFFE) {
+		return shared_ptr<Layer<Dtype> >(new ConvolutionLayer<Dtype>(param));
+#ifdef USE_CUDNN
+	} else if (engine == ConvolutionParameter_Engine_CUDNN) {
+		if (use_dilation) {
+			LOG(FATAL) << "CuDNN doesn't support the dilated DepthwiseConvolution at Layer "
+				<< param.name();
+		}
+		return shared_ptr<Layer<Dtype> >(new CuDNNConvolutionLayer<Dtype>(param));
+#endif
+	} else {
+		LOG(FATAL) << "Layer " << param.name() << " has unknown engine.";
+		throw;  // Avoids missing return warning
+	}
+}
+REGISTER_LAYER_CLASS(DepthwiseConvolution);
+
 // Get pooling layer according to engine.
 template <typename Dtype>
 shared_ptr<Layer<Dtype> > GetPoolingLayer(const LayerParameter& param) {
@@ -286,6 +367,157 @@ shared_ptr<Layer<Dtype> > GetTanHLayer(const LayerParameter& param) {
 
 REGISTER_LAYER_CREATOR(TanH, GetTanHLayer);
 
+// Get tanh layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetBatchNormLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new BatchNormLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(BatchNorm, GetBatchNormLayer);
+
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetScaleLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new ScaleLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Scale, GetScaleLayer);
+
+// Bias
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetBiasLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new BiasLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Bias, GetBiasLayer);
+
+
+// Get Yolov3 layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetYolov3Layer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new Yolov3Layer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Yolov3, GetYolov3Layer);
+
+// Get Yolov3DetectionOutput layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetYolov3DetectionOutputLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new Yolov3DetectionOutputLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Yolov3DetectionOutput, GetYolov3DetectionOutputLayer);
+
+// Get YoloDetectionOutputLayer layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetYoloDetectionOutputLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new YoloDetectionOutputLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(YoloDetectionOutputLayer, GetYoloDetectionOutputLayer);
+
+// Get DetectionOutput layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetDetectionOutputLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new DetectionOutputLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(DetectionOutput, GetDetectionOutputLayer);
+
+// Get AnnotatedData layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetAnnotatedDataLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new AnnotatedDataLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(AnnotatedData, GetAnnotatedDataLayer);
+
+// Get DetectionEvaluate  layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetDetectionEvaluateLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new DetectionEvaluateLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(DetectionEvaluate, GetDetectionEvaluateLayer);
+
+// Permute layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetPermuteLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new PermuteLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Permute, GetPermuteLayer);
+
+// Flatten layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetFlattenLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new FlattenLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Flatten, GetFlattenLayer);
+
+// Flatten layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetPriorBoxLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new PriorBoxLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(PriorBox, GetPriorBoxLayer);
+
+// Flatten layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetConcatLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new ConcatLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Concat, GetConcatLayer);
+
+// MultiBoxLoss layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetMultiBoxLossLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new MultiBoxLossLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(MultiBoxLoss, GetMultiBoxLossLayer);
+
+// MultiBoxLoss layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetSmoothL1LossLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new SmoothL1LossLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(SmoothL1Loss, GetSmoothL1LossLayer);
+
+// SigmoidCrossEntropyLoss layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetSigmoidCrossEntropyLossLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new SigmoidCrossEntropyLossLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(SigmoidCrossEntropyLoss, GetSigmoidCrossEntropyLossLayer);
+
+// Reshape layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetReshapeLayer(const LayerParameter& param) {
+	return shared_ptr<Layer<Dtype> >(new ReshapeLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Reshape, GetReshapeLayer);
+
+// Power layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetPowerLayer(const LayerParameter& param) {
+    return shared_ptr<Layer<Dtype> >(new PowerLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(Power, GetPowerLayer);
+
+// ReLU6 layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetReLU6Layer(const LayerParameter& param) {
+    return shared_ptr<Layer<Dtype> >(new ReLU6Layer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(ReLU6, GetReLU6Layer);
+
+// InnerProduct layer according to engine.
+template <typename Dtype>
+shared_ptr<Layer<Dtype> > GetInnerProductLayer(const LayerParameter& param) {
+    return shared_ptr<Layer<Dtype> >(new InnerProductLayer<Dtype>(param));
+}
+REGISTER_LAYER_CREATOR(InnerProduct, GetInnerProductLayer);
+
+//  ========================================================================================================
+
+
+// Get DetectionOutput Solver according to engine.
+template <typename Dtype>
+Solver<Dtype> * GetRMSPropSolver(const SolverParameter& param) {
+	return new RMSPropSolver<Dtype>(param);
+}
+REGISTER_SOLVER_CREATOR(RMSProp, GetRMSPropSolver);
+
+
 #ifdef WITH_PYTHON_LAYER
 template <typename Dtype>
 shared_ptr<Layer<Dtype> > GetPythonLayer(const LayerParameter& param) {
diff --git a/src/caffe/layers/annotated_data_layer.cpp b/src/caffe/layers/annotated_data_layer.cpp
index 1caf5ed..b9b8235 100644
--- a/src/caffe/layers/annotated_data_layer.cpp
+++ b/src/caffe/layers/annotated_data_layer.cpp
@@ -1,4 +1,4 @@
-#ifdef USE_OPENCV
+#ifdef USE_OPENCV
 #include <opencv2/core/core.hpp>
 #include <opencv2/highgui/highgui.hpp>
 #include <opencv2/imgproc/imgproc.hpp>
@@ -238,6 +238,11 @@ void AnnotatedDataLayer<Dtype>::load_batch(Batch<Dtype>* batch) {
   }
   vector<int> top_shape =
 	  this->data_transformer_->InferBlobShape(anno_datum.datum(), policy_num_);
+
+  if (top_shape[1] == 1) {
+	  top_shape[1] = 3;
+	  // 3
+  }
   // Reshape batch according to the batch_size.
   this->transformed_data_.Reshape(top_shape);
   top_shape[0] = batch_size;
@@ -314,6 +319,8 @@ void AnnotatedDataLayer<Dtype>::load_batch(Batch<Dtype>* batch) {
 
     vector<int> shape =
         this->data_transformer_->InferBlobShape(sampled_datum->datum(), policy_num_);
+	//shape[1] = 3; // 3
+	//this->transformed_data_.Reshape(shape);
 
 	//LOG(INFO) << shape[2] << "," << shape[3];
     if (transform_param.resize_param_size()) {
@@ -619,6 +626,6 @@ void AnnotatedDataLayer<Dtype>::load_batch(Batch<Dtype>* batch) {
 }
 
 INSTANTIATE_CLASS(AnnotatedDataLayer);
-REGISTER_LAYER_CLASS(AnnotatedData);
+// REGISTER_LAYER_CLASS(AnnotatedData);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/batch_norm_layer.cpp b/src/caffe/layers/batch_norm_layer.cpp
index c6a1d5b..1160370 100644
--- a/src/caffe/layers/batch_norm_layer.cpp
+++ b/src/caffe/layers/batch_norm_layer.cpp
@@ -247,5 +247,5 @@ STUB_GPU(BatchNormLayer);
 #endif
 
 INSTANTIATE_CLASS(BatchNormLayer);
-REGISTER_LAYER_CLASS(BatchNorm);
+//REGISTER_LAYER_CLASS(BatchNorm);
 }  // namespace caffe
diff --git a/src/caffe/layers/bias_layer.cpp b/src/caffe/layers/bias_layer.cpp
index 4726a72..0d66397 100644
--- a/src/caffe/layers/bias_layer.cpp
+++ b/src/caffe/layers/bias_layer.cpp
@@ -116,6 +116,6 @@ STUB_GPU(BiasLayer);
 #endif
 
 INSTANTIATE_CLASS(BiasLayer);
-REGISTER_LAYER_CLASS(Bias);
+// REGISTER_LAYER_CLASS(Bias);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/bnll_layer.cu b/src/caffe/layers/bnll_layer.cu
index 8df8ef0..cd0c1ca 100644
--- a/src/caffe/layers/bnll_layer.cu
+++ b/src/caffe/layers/bnll_layer.cu
@@ -5,7 +5,7 @@
 
 namespace caffe {
 
-const float kBNLL_THRESHOLD = 50.;
+#define kBNLL_THRESHOLD  (50)
 
 template <typename Dtype>
 __global__ void BNLLForward(const int n, const Dtype* in, Dtype* out) {
diff --git a/src/caffe/layers/concat_layer.cpp b/src/caffe/layers/concat_layer.cpp
index 580bd47..0a89ac1 100644
--- a/src/caffe/layers/concat_layer.cpp
+++ b/src/caffe/layers/concat_layer.cpp
@@ -99,6 +99,6 @@ STUB_GPU(ConcatLayer);
 #endif
 
 INSTANTIATE_CLASS(ConcatLayer);
-REGISTER_LAYER_CLASS(Concat);
+// REGISTER_LAYER_CLASS(Concat);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/cudnn_deconv_layer.cpp b/src/caffe/layers/cudnn_deconv_layer.cpp
index 260da5c..b8cfcbf 100644
--- a/src/caffe/layers/cudnn_deconv_layer.cpp
+++ b/src/caffe/layers/cudnn_deconv_layer.cpp
@@ -4,6 +4,10 @@
 
 #include "caffe/layers/cudnn_deconv_layer.hpp"
 
+#if CUDNN_MAJOR >= 7
+#define USE_CUDNN_GROUP_INTERNAL
+#endif
+
 namespace caffe {
 
 // Set to three for the benefit of the backward pass, which
@@ -19,8 +23,13 @@ void CuDNNDeconvolutionLayer<Dtype>::LayerSetUp(
     const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
   DeconvolutionLayer<Dtype>::LayerSetUp(bottom, top);
   // Initialize CUDA streams and cuDNN.
-  stream_         = new cudaStream_t[this->group_ * CUDNN_STREAMS_PER_GROUP];
-  handle_         = new cudnnHandle_t[this->group_ * CUDNN_STREAMS_PER_GROUP];
+#ifdef USE_CUDNN_GROUP_INTERNAL
+  stream_ = new cudaStream_t[CUDNN_STREAMS_PER_GROUP];
+  handle_ = new cudnnHandle_t[CUDNN_STREAMS_PER_GROUP];
+#else
+  stream_ = new cudaStream_t[this->group_ * CUDNN_STREAMS_PER_GROUP];
+  handle_ = new cudnnHandle_t[this->group_ * CUDNN_STREAMS_PER_GROUP];
+#endif
 
   // Initialize algorithm arrays
   fwd_algo_       = new cudnnConvolutionFwdAlgo_t[bottom.size()];
@@ -35,7 +44,11 @@ void CuDNNDeconvolutionLayer<Dtype>::LayerSetUp(
   // workspace data
   workspaceSizeInBytes = 0;
   workspaceData = NULL;
-  workspace = new void*[this->group_ * CUDNN_STREAMS_PER_GROUP];
+#ifdef USE_CUDNN_GROUP_INTERNAL
+  workspace = new void* [CUDNN_STREAMS_PER_GROUP];
+#else
+  workspace = new void* [this->group_ * CUDNN_STREAMS_PER_GROUP];
+#endif
 
   for (size_t i = 0; i < bottom.size(); ++i) {
     // initialize all to default algorithms
@@ -48,25 +61,42 @@ void CuDNNDeconvolutionLayer<Dtype>::LayerSetUp(
     workspace_bwd_filter_sizes_[i] = 0;
   }
 
+#ifdef USE_CUDNN_GROUP_INTERNAL
+  for (int g = 0; g < CUDNN_STREAMS_PER_GROUP; g++) {
+      CUDA_CHECK(cudaStreamCreate(&stream_[g]));
+      CUDNN_CHECK(cudnnCreate(&handle_[g]));
+      CUDNN_CHECK(cudnnSetStream(handle_[g], stream_[g]));
+      workspace[g] = NULL;
+  }
+#else
   for (int g = 0; g < this->group_ * CUDNN_STREAMS_PER_GROUP; g++) {
-    CUDA_CHECK(cudaStreamCreate(&stream_[g]));
-    CUDNN_CHECK(cudnnCreate(&handle_[g]));
-    CUDNN_CHECK(cudnnSetStream(handle_[g], stream_[g]));
-    workspace[g] = NULL;
+      CUDA_CHECK(cudaStreamCreate(&stream_[g]));
+      CUDNN_CHECK(cudnnCreate(&handle_[g]));
+      CUDNN_CHECK(cudnnSetStream(handle_[g], stream_[g]));
+      workspace[g] = NULL;
   }
+#endif
 
   // Set the indexing parameters.
-  bias_offset_ = (this->num_output_ / this->group_);
+  //bias_offset_ = (this->num_output_ / this->group_);
 
   // Create filter descriptor.
   const int* kernel_shape_data = this->kernel_shape_.cpu_data();
   const int kernel_h = kernel_shape_data[0];
   const int kernel_w = kernel_shape_data[1];
+#ifdef USE_CUDNN_GROUP_INTERNAL
+  cudnn::createFilterDesc<Dtype>(&filter_desc_,
+      this->channels_ / this->group_,
+      this->num_output_ ,
+      kernel_h,
+      kernel_w);
+#else
   cudnn::createFilterDesc<Dtype>(&filter_desc_,
                                  this->channels_ / this->group_,
                                  this->num_output_ / this->group_,
                                  kernel_h,
                                  kernel_w);
+#endif
 
   // Create tensor descriptor(s) for data and corresponding convolution(s).
   for (int i = 0; i < bottom.size(); i++) {
@@ -78,6 +108,9 @@ void CuDNNDeconvolutionLayer<Dtype>::LayerSetUp(
     top_descs_.push_back(top_desc);
     cudnnConvolutionDescriptor_t conv_desc;
     cudnn::createConvolutionDesc<Dtype>(&conv_desc);
+#ifdef USE_CUDNN_GROUP_INTERNAL
+    cudnnSetConvolutionGroupCount(conv_desc, this->group_);
+#endif
     conv_descs_.push_back(conv_desc);
   }
 
@@ -97,8 +130,8 @@ void CuDNNDeconvolutionLayer<Dtype>::Reshape(
       << "CuDNNDeconvolutionLayer input must have 2 spatial axes "
       << "(e.g., height and width). "
       << "Use 'engine: CAFFE' for general ND convolution.";
-  bottom_offset_ = this->bottom_dim_ / this->group_;
-  top_offset_ = this->top_dim_ / this->group_;
+  //bottom_offset_ = this->bottom_dim_ / this->group_;
+  //top_offset_ = this->top_dim_ / this->group_;
   const int height = bottom[0]->shape(this->channel_axis_ + 1);
   const int width = bottom[0]->shape(this->channel_axis_ + 2);
   const int height_out = top[0]->shape(this->channel_axis_ + 1);
@@ -115,31 +148,35 @@ void CuDNNDeconvolutionLayer<Dtype>::Reshape(
   size_t workspace_limit_bytes = 8*1024*1024;
 
   for (int i = 0; i < bottom.size(); i++) {
-    cudnn::setTensor4dDesc<Dtype>(&bottom_descs_[i],
-                                  this->num_,
-                                  this->channels_ / this->group_,
-                                  height,
-                                  width,
-                                  this->channels_ * height * width,
-                                  height * width,
-                                  width,
-                                  1);
-    cudnn::setTensor4dDesc<Dtype>(&top_descs_[i],
-                                  this->num_,
-                                  this->num_output_ / this->group_,
-                                  height_out,
-                                  width_out,
-                                  this->num_output_ * height_out * width_out,
-                                  height_out * width_out,
-                                  width_out,
-                                  1);
-    cudnn::setConvolutionDesc<Dtype>(&conv_descs_[i],
-                                     top_descs_[i],
-                                     filter_desc_,
-                                     pad_h,
-                                     pad_w,
-                                     stride_h,
-                                     stride_w);
+#ifdef USE_CUDNN_GROUP_INTERNAL
+      cudnn::setTensor4dDesc<Dtype>(&bottom_descs_[i],
+          this->num_,
+          this->channels_, height, width,
+          this->channels_ * height * width,
+          height * width, width, 1);
+      cudnn::setTensor4dDesc<Dtype>(&top_descs_[i],
+          this->num_,
+          this->num_output_, height_out, width_out,
+          this->num_output_ * height_out * width_out,
+          height_out * width_out, width_out, 1);
+      cudnn::setConvolutionDesc<Dtype>(&conv_descs_[i], top_descs_[i],
+          filter_desc_, pad_h, pad_w,
+          stride_h, stride_w);
+#else
+      cudnn::setTensor4dDesc<Dtype>(&bottom_descs_[i],
+          this->num_,
+          this->channels_ / this->group_, height, width,
+          this->channels_ * height * width,
+          height * width, width, 1);
+      cudnn::setTensor4dDesc<Dtype>(&top_descs_[i],
+          this->num_,
+          this->num_output_ / this->group_, height_out, width_out,
+          this->num_output_ * height_out * width_out,
+          height_out * width_out, width_out, 1);
+      cudnn::setConvolutionDesc<Dtype>(&conv_descs_[i], top_descs_[i],
+          filter_desc_, pad_h, pad_w,
+          stride_h, stride_w);
+#endif
 
     // choose forward and backward algorithms + workspace(s)
     CUDNN_CHECK(cudnnGetConvolutionForwardAlgorithm(
@@ -244,8 +281,12 @@ void CuDNNDeconvolutionLayer<Dtype>::Reshape(
                              total_workspace_bwd_data);
   max_workspace = std::max(max_workspace, total_workspace_bwd_filter);
   // ensure all groups have enough workspace
+#ifdef USE_CUDNN_GROUP_INTERNAL
+  size_t total_max_workspace = max_workspace * CUDNN_STREAMS_PER_GROUP;
+#else
   size_t total_max_workspace = max_workspace *
-                               (this->group_ * CUDNN_STREAMS_PER_GROUP);
+      (this->group_ * CUDNN_STREAMS_PER_GROUP);
+#endif
 
   // this is the total amount of storage needed over all groups + streams
   if (total_max_workspace > workspaceSizeInBytes) {
@@ -268,24 +309,42 @@ void CuDNNDeconvolutionLayer<Dtype>::Reshape(
       }
 
       // NULL out all workspace pointers
+#ifdef USE_CUDNN_GROUP_INTERNAL
+      for (int g = 0; g < CUDNN_STREAMS_PER_GROUP; g++) {
+          workspace[g] = NULL;
+      }
+#else
       for (int g = 0; g < (this->group_ * CUDNN_STREAMS_PER_GROUP); g++) {
-        workspace[g] = NULL;
+          workspace[g] = NULL;
       }
+#endif
+
       // NULL out underlying data
       workspaceData = NULL;
       workspaceSizeInBytes = 0;
     }
 
     // if we succeed in the allocation, set pointer aliases for workspaces
+#ifdef USE_CUDNN_GROUP_INTERNAL
+    for (int g = 0; g < CUDNN_STREAMS_PER_GROUP; g++) {
+        workspace[g] = reinterpret_cast<char*>(workspaceData) + g * max_workspace;
+    }
+#else
     for (int g = 0; g < (this->group_ * CUDNN_STREAMS_PER_GROUP); g++) {
-      workspace[g] = reinterpret_cast<char *>(workspaceData) + g*max_workspace;
+        workspace[g] = reinterpret_cast<char*>(workspaceData) + g * max_workspace;
     }
+#endif
   }
 
   // Tensor descriptor for bias.
   if (this->bias_term_) {
-    cudnn::setTensor4dDesc<Dtype>(
-        &bias_desc_, 1, this->num_output_ / this->group_, 1, 1);
+#ifdef USE_CUDNN_GROUP_INTERNAL
+      cudnn::setTensor4dDesc<Dtype>(&bias_desc_,
+          1, this->num_output_, 1, 1);
+#else
+      cudnn::setTensor4dDesc<Dtype>(&bias_desc_,
+          1, this->num_output_ / this->group_, 1, 1);
+#endif
   }
 }
 
@@ -304,13 +363,19 @@ CuDNNDeconvolutionLayer<Dtype>::~CuDNNDeconvolutionLayer() {
   }
   cudnnDestroyFilterDescriptor(filter_desc_);
 
+#ifdef USE_CUDNN_GROUP_INTERNAL
+  for (int g = 0; g < CUDNN_STREAMS_PER_GROUP; g++) {
+      cudaStreamDestroy(stream_[g]);
+      cudnnDestroy(handle_[g]);
+  }
+#else
   for (int g = 0; g < this->group_ * CUDNN_STREAMS_PER_GROUP; g++) {
-    cudaStreamDestroy(stream_[g]);
-    cudnnDestroy(handle_[g]);
+      cudaStreamDestroy(stream_[g]);
+      cudnnDestroy(handle_[g]);
   }
+#endif
 
   cudaFree(workspaceData);
-  delete [] workspace;
   delete [] stream_;
   delete [] handle_;
   delete [] fwd_algo_;
diff --git a/src/caffe/layers/depthwise_conv_layer.cpp b/src/caffe/layers/depthwise_conv_layer.cpp
index 1fec5be..55b4e62 100644
--- a/src/caffe/layers/depthwise_conv_layer.cpp
+++ b/src/caffe/layers/depthwise_conv_layer.cpp
@@ -76,5 +76,4 @@ STUB_GPU(DepthwiseConvolutionLayer);
 #endif
 
 INSTANTIATE_CLASS(DepthwiseConvolutionLayer);
-REGISTER_LAYER_CLASS(DepthwiseConvolution);
 }  // namespace caffe
diff --git a/src/caffe/layers/depthwise_conv_layer.cu b/src/caffe/layers/depthwise_conv_layer.cu
index edd1343..c2c5da0 100644
--- a/src/caffe/layers/depthwise_conv_layer.cu
+++ b/src/caffe/layers/depthwise_conv_layer.cu
@@ -272,7 +272,7 @@ const vector<Blob<Dtype>*>& bottom) {
 
 	const bool bias_term_ = this->bias_term_;
 	Dtype* bias_diff = bias_term_ ? this->blobs_[1]->mutable_gpu_diff() : 0;
-	const bool bias_propagate_down_ = this->param_propagate_down_[1];
+	const bool bias_propagate_down_ = (this->param_propagate_down_.size() >1)? this->param_propagate_down_[1] : false;
 	const bool weight_propagate_down_ = this->param_propagate_down_[0];
 
 
diff --git a/src/caffe/layers/detection_evaluate_layer.cpp b/src/caffe/layers/detection_evaluate_layer.cpp
index c15af14..048ab59 100644
--- a/src/caffe/layers/detection_evaluate_layer.cpp
+++ b/src/caffe/layers/detection_evaluate_layer.cpp
@@ -245,6 +245,6 @@ void DetectionEvaluateLayer<Dtype>::Forward_cpu(
 }
 
 INSTANTIATE_CLASS(DetectionEvaluateLayer);
-REGISTER_LAYER_CLASS(DetectionEvaluate);
+//REGISTER_LAYER_CLASS(DetectionEvaluate);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/detection_output_layer.cpp b/src/caffe/layers/detection_output_layer.cpp
index ff64255..81229ba 100644
--- a/src/caffe/layers/detection_output_layer.cpp
+++ b/src/caffe/layers/detection_output_layer.cpp
@@ -326,6 +326,6 @@ STUB_GPU_FORWARD(DetectionOutputLayer, Forward);
 #endif
 
 INSTANTIATE_CLASS(DetectionOutputLayer);
-REGISTER_LAYER_CLASS(DetectionOutput);
+//REGISTER_LAYER_CLASS(DetectionOutput);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/flatten_layer.cpp b/src/caffe/layers/flatten_layer.cpp
index d4ab393..1584793 100644
--- a/src/caffe/layers/flatten_layer.cpp
+++ b/src/caffe/layers/flatten_layer.cpp
@@ -39,6 +39,6 @@ void FlattenLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
 }
 
 INSTANTIATE_CLASS(FlattenLayer);
-REGISTER_LAYER_CLASS(Flatten);
+//REGISTER_LAYER_CLASS(Flatten);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/gaussian_yolov3_layer.cpp b/src/caffe/layers/gaussian_yolov3_layer.cpp
index 435d8c6..abb83ce 100644
--- a/src/caffe/layers/gaussian_yolov3_layer.cpp
+++ b/src/caffe/layers/gaussian_yolov3_layer.cpp
@@ -93,19 +93,19 @@ namespace caffe {
 
     float in_exp_x = (tx - x[index + 0*stride])/x[index+1*stride];
     float in_exp_x_2 = pow(in_exp_x, 2);
-    float normal_dist_x = exp(in_exp_x_2*(-1./2.))/(sqrt(M_PI * 2.0)*(x[index+1*stride]+sigma_const));
+    float normal_dist_x = exp(in_exp_x_2*(-1./2.))/(sqrt(CV_PI * 2.0)*(x[index+1*stride]+sigma_const));
 
     float in_exp_y = (ty - x[index + 2*stride])/x[index+3*stride];
     float in_exp_y_2 = pow(in_exp_y, 2);
-    float normal_dist_y = exp(in_exp_y_2*(-1./2.))/(sqrt(M_PI * 2.0)*(x[index+3*stride]+sigma_const));
+    float normal_dist_y = exp(in_exp_y_2*(-1./2.))/(sqrt(CV_PI * 2.0)*(x[index+3*stride]+sigma_const));
 
     float in_exp_w = (tw - x[index + 4*stride])/x[index+5*stride];
     float in_exp_w_2 = pow(in_exp_w, 2);
-    float normal_dist_w = exp(in_exp_w_2*(-1./2.))/(sqrt(M_PI * 2.0)*(x[index+5*stride]+sigma_const));
+    float normal_dist_w = exp(in_exp_w_2*(-1./2.))/(sqrt(CV_PI * 2.0)*(x[index+5*stride]+sigma_const));
 
     float in_exp_h = (th - x[index + 6*stride])/x[index+7*stride];
     float in_exp_h_2 = pow(in_exp_h, 2);
-    float normal_dist_h = exp(in_exp_h_2*(-1./2.))/(sqrt(M_PI * 2.0)*(x[index+7*stride]+sigma_const));
+    float normal_dist_h = exp(in_exp_h_2*(-1./2.))/(sqrt(CV_PI * 2.0)*(x[index+7*stride]+sigma_const));
 
     float temp_x = (1./2.) * 1./(normal_dist_x+epsi) * normal_dist_x * scale;
     float temp_y = (1./2.) * 1./(normal_dist_y+epsi) * normal_dist_y * scale;
diff --git a/src/caffe/layers/inner_product_layer.cpp b/src/caffe/layers/inner_product_layer.cpp
index 57fdbe1..f458ba1 100644
--- a/src/caffe/layers/inner_product_layer.cpp
+++ b/src/caffe/layers/inner_product_layer.cpp
@@ -145,6 +145,6 @@ STUB_GPU(InnerProductLayer);
 #endif
 
 INSTANTIATE_CLASS(InnerProductLayer);
-REGISTER_LAYER_CLASS(InnerProduct);
+//REGISTER_LAYER_CLASS(InnerProduct);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/input_layer.cpp b/src/caffe/layers/input_layer.cpp
index 667d8ad..a5e2823 100644
--- a/src/caffe/layers/input_layer.cpp
+++ b/src/caffe/layers/input_layer.cpp
@@ -22,6 +22,5 @@ void InputLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom,
 }
 
 INSTANTIATE_CLASS(InputLayer);
-REGISTER_LAYER_CLASS(Input);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/multibox_loss_layer.cpp b/src/caffe/layers/multibox_loss_layer.cpp
index 1dad83f..b725dc2 100644
--- a/src/caffe/layers/multibox_loss_layer.cpp
+++ b/src/caffe/layers/multibox_loss_layer.cpp
@@ -370,6 +370,6 @@ void MultiBoxLossLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
 }
 
 INSTANTIATE_CLASS(MultiBoxLossLayer);
-REGISTER_LAYER_CLASS(MultiBoxLoss);
+//REGISTER_LAYER_CLASS(MultiBoxLoss);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/permute_layer.cpp b/src/caffe/layers/permute_layer.cpp
index 0f7f6d6..5d0df45 100644
--- a/src/caffe/layers/permute_layer.cpp
+++ b/src/caffe/layers/permute_layer.cpp
@@ -137,6 +137,6 @@ STUB_GPU(PermuteLayer);
 #endif
 
 INSTANTIATE_CLASS(PermuteLayer);
-REGISTER_LAYER_CLASS(Permute);
+//REGISTER_LAYER_CLASS(Permute);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/power_layer.cpp b/src/caffe/layers/power_layer.cpp
index d99b77c..db41fc3 100644
--- a/src/caffe/layers/power_layer.cpp
+++ b/src/caffe/layers/power_layer.cpp
@@ -97,6 +97,6 @@ STUB_GPU(PowerLayer);
 #endif
 
 INSTANTIATE_CLASS(PowerLayer);
-REGISTER_LAYER_CLASS(Power);
+//REGISTER_LAYER_CLASS(Power);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/prior_box_layer.cpp b/src/caffe/layers/prior_box_layer.cpp
index d860bb3..c243ae5 100644
--- a/src/caffe/layers/prior_box_layer.cpp
+++ b/src/caffe/layers/prior_box_layer.cpp
@@ -219,6 +219,6 @@ void PriorBoxLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
 }
 
 INSTANTIATE_CLASS(PriorBoxLayer);
-REGISTER_LAYER_CLASS(PriorBox);
+//REGISTER_LAYER_CLASS(PriorBox);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/relu6_layer.cpp b/src/caffe/layers/relu6_layer.cpp
index 6911eac..44f1760 100644
--- a/src/caffe/layers/relu6_layer.cpp
+++ b/src/caffe/layers/relu6_layer.cpp
@@ -41,6 +41,6 @@ STUB_GPU(ReLU6Layer);
 #endif
 
 INSTANTIATE_CLASS(ReLU6Layer);
-REGISTER_LAYER_CLASS(ReLU6);
+//REGISTER_LAYER_CLASS(ReLU6);
 }  // namespace caffe
 
diff --git a/src/caffe/layers/reshape_layer.cpp b/src/caffe/layers/reshape_layer.cpp
index 45dd090..593942b 100644
--- a/src/caffe/layers/reshape_layer.cpp
+++ b/src/caffe/layers/reshape_layer.cpp
@@ -91,6 +91,6 @@ void ReshapeLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
 }
 
 INSTANTIATE_CLASS(ReshapeLayer);
-REGISTER_LAYER_CLASS(Reshape);
+//REGISTER_LAYER_CLASS(Reshape);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/scale_layer.cpp b/src/caffe/layers/scale_layer.cpp
index e652dad..c0e1101 100644
--- a/src/caffe/layers/scale_layer.cpp
+++ b/src/caffe/layers/scale_layer.cpp
@@ -222,6 +222,6 @@ STUB_GPU(ScaleLayer);
 #endif
 
 INSTANTIATE_CLASS(ScaleLayer);
-REGISTER_LAYER_CLASS(Scale);
+// REGISTER_LAYER_CLASS(Scale);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp b/src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
index 99fa3eb..ecf7bf0 100644
--- a/src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
+++ b/src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
@@ -135,6 +135,6 @@ STUB_GPU(SigmoidCrossEntropyLossLayer);
 #endif
 
 INSTANTIATE_CLASS(SigmoidCrossEntropyLossLayer);
-REGISTER_LAYER_CLASS(SigmoidCrossEntropyLoss);
+// REGISTER_LAYER_CLASS(SigmoidCrossEntropyLoss);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/smooth_L1_loss_layer.cpp b/src/caffe/layers/smooth_L1_loss_layer.cpp
index e0c7c63..78a4f3a 100644
--- a/src/caffe/layers/smooth_L1_loss_layer.cpp
+++ b/src/caffe/layers/smooth_L1_loss_layer.cpp
@@ -103,6 +103,6 @@ STUB_GPU(SmoothL1LossLayer);
 #endif
 
 INSTANTIATE_CLASS(SmoothL1LossLayer);
-REGISTER_LAYER_CLASS(SmoothL1Loss);
+//REGISTER_LAYER_CLASS(SmoothL1Loss);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/yolo_detection_output_layer.cpp b/src/caffe/layers/yolo_detection_output_layer.cpp
index 3a4eb43..5147468 100644
--- a/src/caffe/layers/yolo_detection_output_layer.cpp
+++ b/src/caffe/layers/yolo_detection_output_layer.cpp
@@ -351,6 +351,6 @@ void YoloDetectionOutputLayer<Dtype>::Forward_cpu(
 #endif
 
 INSTANTIATE_CLASS(YoloDetectionOutputLayer);
-REGISTER_LAYER_CLASS(YoloDetectionOutput);
+//REGISTER_LAYER_CLASS(YoloDetectionOutput);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/yolov3_detection_output_layer.cpp b/src/caffe/layers/yolov3_detection_output_layer.cpp
index 1a08c7a..e5a5400 100644
--- a/src/caffe/layers/yolov3_detection_output_layer.cpp
+++ b/src/caffe/layers/yolov3_detection_output_layer.cpp
@@ -13,17 +13,17 @@
 #include <vector>
 
 
-//#include "caffe/layers/region_loss_layer.hpp"
+#include "caffe/layers/region_loss_layer.hpp"
 #include "caffe/layers/yolov3_detection_output_layer.hpp"
 #include "caffe/util/io.hpp"
 
 
 namespace caffe {
-template <typename Dtype>
-inline Dtype sigmoid(Dtype x)
-{
-  return 1. / (1. + exp(-x));
-}
+//template <typename Dtype>
+//inline Dtype sigmoid(Dtype x)
+//{
+//  return 1. / (1. + exp(-x));
+//}
 
 template <typename Dtype>
 Dtype overlap(Dtype x1, Dtype w1, Dtype x2, Dtype w2)
@@ -405,6 +405,6 @@ void Yolov3DetectionOutputLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>&
 #endif
 
 INSTANTIATE_CLASS(Yolov3DetectionOutputLayer);
-REGISTER_LAYER_CLASS(Yolov3DetectionOutput);
+//REGISTER_LAYER_CLASS(Yolov3DetectionOutput);
 
 }  // namespace caffe
diff --git a/src/caffe/layers/yolov3_detection_output_layer.cu b/src/caffe/layers/yolov3_detection_output_layer.cu
index 7085ce9..abcd242 100644
--- a/src/caffe/layers/yolov3_detection_output_layer.cu
+++ b/src/caffe/layers/yolov3_detection_output_layer.cu
@@ -1,6 +1,7 @@
 #include <cmath>
 #include <vector>
 
+#include "caffe/layers/region_loss_layer.hpp"
 #include "caffe/layers/yolov3_detection_output_layer.hpp"
 #include "caffe/util/math_functions.hpp"
 #include "caffe/util/bbox_util.hpp"
diff --git a/src/caffe/layers/yolov3_layer.cpp b/src/caffe/layers/yolov3_layer.cpp
index 6359119..edf6588 100644
--- a/src/caffe/layers/yolov3_layer.cpp
+++ b/src/caffe/layers/yolov3_layer.cpp
@@ -529,7 +529,7 @@ namespace caffe {
     if (!(iter_ % 16))
     {
       if(time_count_>0 ) {
-        LOG(INFO) << "noobj: " << score_.avg_anyobj / 10. << " obj: " << score_.avg_obj / time_count_ <<
+        DLOG(INFO) << "noobj: " << score_.avg_anyobj / 10. << " obj: " << score_.avg_obj / time_count_ <<
           " iou: " << score_.avg_iou / time_count_ << " cat: " << score_.avg_cat / time_count_ << " recall: " << score_.recall / time_count_ << " recall75: " << score_.recall75 / time_count_<< " count: " << class_count_/time_count_;
         //LOG(INFO) << "avg_noobj: "<< avg_anyobj/(side_*side_*num_*bottom[0]->num()) << " avg_obj: " << avg_obj/count <<" avg_iou: " << avg_iou/count << " avg_cat: " << avg_cat/class_count << " recall: " << recall/count << " recall75: " << recall75 / count;
         score_.avg_anyobj = score_.avg_obj = score_.avg_iou = score_.avg_cat = score_.recall = score_.recall75 = 0;
@@ -616,6 +616,6 @@ namespace caffe {
 #endif
 
   INSTANTIATE_CLASS(Yolov3Layer);
-  REGISTER_LAYER_CLASS(Yolov3);
+//  REGISTER_LAYER_CLASS(Yolov3);
 
 }  // namespace caffe
diff --git a/src/caffe/solvers/rmsprop_solver.cpp b/src/caffe/solvers/rmsprop_solver.cpp
index 3251ee4..3a1ddad 100644
--- a/src/caffe/solvers/rmsprop_solver.cpp
+++ b/src/caffe/solvers/rmsprop_solver.cpp
@@ -65,6 +65,6 @@ void RMSPropSolver<Dtype>::ComputeUpdateValue(int param_id, Dtype rate) {
 }
 
 INSTANTIATE_CLASS(RMSPropSolver);
-REGISTER_SOLVER_CLASS(RMSProp);
+//REGISTER_SOLVER_CLASS(RMSProp);
 
 }  // namespace caffe
diff --git a/src/caffe/util/db_lmdb.cpp b/src/caffe/util/db_lmdb.cpp
index 491a9bd..d006fca 100644
--- a/src/caffe/util/db_lmdb.cpp
+++ b/src/caffe/util/db_lmdb.cpp
@@ -1,6 +1,11 @@
 #ifdef USE_LMDB
 #include "caffe/util/db_lmdb.hpp"
 
+#if defined(_MSC_VER)
+#include <direct.h>
+#define mkdir(X, Y) _mkdir(X)
+#endif
+//#define ALLOW_LMDB_NOLOCK
 #include <sys/stat.h>
 
 #include <string>
diff --git a/src/caffe/util/io.cpp b/src/caffe/util/io.cpp
index bc6f36a..0ed8cba 100644
--- a/src/caffe/util/io.cpp
+++ b/src/caffe/util/io.cpp
@@ -1,3 +1,8 @@
+#include <fcntl.h>
+
+#if defined(_MSC_VER)
+#include <io.h>
+#endif
 #include <boost/algorithm/string/classification.hpp>
 #include <boost/algorithm/string/split.hpp>
 #include <boost/filesystem.hpp>
@@ -41,173 +46,177 @@ using google::protobuf::io::CodedOutputStream;
 using google::protobuf::Message;
 
 bool ReadProtoFromTextFile(const char* filename, Message* proto) {
-  int fd = open(filename, O_RDONLY);
-  CHECK_NE(fd, -1) << "File not found: " << filename;
-  FileInputStream* input = new FileInputStream(fd);
-  bool success = google::protobuf::TextFormat::Parse(input, proto);
-  delete input;
-  close(fd);
-  return success;
+	int fd = open(filename, O_RDONLY);
+	CHECK_NE(fd, -1) << "File not found: " << filename;
+	FileInputStream* input = new FileInputStream(fd);
+	bool success = google::protobuf::TextFormat::Parse(input, proto);
+	delete input;
+	close(fd);
+	return success;
 }
 
 void WriteProtoToTextFile(const Message& proto, const char* filename) {
-  int fd = open(filename, O_WRONLY | O_CREAT | O_TRUNC, 0644);
-  FileOutputStream* output = new FileOutputStream(fd);
-  CHECK(google::protobuf::TextFormat::Print(proto, output));
-  delete output;
-  close(fd);
+	int fd = open(filename, O_WRONLY | O_CREAT | O_TRUNC, 0644);
+	FileOutputStream* output = new FileOutputStream(fd);
+	CHECK(google::protobuf::TextFormat::Print(proto, output));
+	delete output;
+	close(fd);
 }
 
 bool ReadProtoFromBinaryFile(const char* filename, Message* proto) {
-  int fd = open(filename, O_RDONLY);
-  CHECK_NE(fd, -1) << "File not found: " << filename;
-  ZeroCopyInputStream* raw_input = new FileInputStream(fd);
-  CodedInputStream* coded_input = new CodedInputStream(raw_input);
-  coded_input->SetTotalBytesLimit(kProtoReadBytesLimit, 536870912);
+#if defined (_MSC_VER)  // for MSC compiler binary flag needs to be specified
+	int fd = open(filename, O_RDONLY | O_BINARY);
+#else
+	int fd = open(filename, O_RDONLY);
+#endif
+	CHECK_NE(fd, -1) << "File not found: " << filename;
+	ZeroCopyInputStream* raw_input = new FileInputStream(fd);
+	CodedInputStream* coded_input = new CodedInputStream(raw_input);
+	coded_input->SetTotalBytesLimit(kProtoReadBytesLimit, 536870912);
 
-  bool success = proto->ParseFromCodedStream(coded_input);
+	bool success = proto->ParseFromCodedStream(coded_input);
 
-  delete coded_input;
-  delete raw_input;
-  close(fd);
-  return success;
+	delete coded_input;
+	delete raw_input;
+	close(fd);
+	return success;
 }
 
 void WriteProtoToBinaryFile(const Message& proto, const char* filename) {
-  fstream output(filename, ios::out | ios::trunc | ios::binary);
-  CHECK(proto.SerializeToOstream(&output));
+	fstream output(filename, ios::out | ios::trunc | ios::binary);
+	CHECK(proto.SerializeToOstream(&output));
 }
 
 #ifdef USE_OPENCV
 cv::Mat ReadImageToCVMat(const string& filename,
-    const int height, const int width, const bool is_color,
-    const bool nearest_neighbour_interp) {
-  cv::Mat cv_img;
-  int cv_read_flag = (is_color ? CV_LOAD_IMAGE_COLOR :
-    CV_LOAD_IMAGE_GRAYSCALE);
-  cv::Mat cv_img_origin = cv::imread(filename, cv_read_flag);
-  if (!cv_img_origin.data) {
-    LOG(ERROR) << "Could not open or find file " << filename;
-    return cv_img_origin;
-  }
-  if (height > 0 && width > 0) {
-    int cv_interp_flag = nearest_neighbour_interp ? CV_INTER_NN :
-                                                    CV_INTER_LINEAR;
-    cv::resize(cv_img_origin, cv_img, cv::Size(width, height), 0, 0,
-        cv_interp_flag);
-  } else {
-    cv_img = cv_img_origin;
-  }
-  return cv_img;
+	const int height, const int width, const bool is_color,
+	const bool nearest_neighbour_interp) {
+	cv::Mat cv_img;
+	int cv_read_flag = (is_color ? CV_LOAD_IMAGE_COLOR :
+		CV_LOAD_IMAGE_GRAYSCALE);
+	cv::Mat cv_img_origin = cv::imread(filename, cv_read_flag);
+	if (!cv_img_origin.data) {
+		LOG(ERROR) << "Could not open or find file " << filename;
+		return cv_img_origin;
+	}
+	if (height > 0 && width > 0) {
+		int cv_interp_flag = nearest_neighbour_interp ? CV_INTER_NN :
+			CV_INTER_LINEAR;
+		cv::resize(cv_img_origin, cv_img, cv::Size(width, height), 0, 0,
+			cv_interp_flag);
+	} else {
+		cv_img = cv_img_origin;
+	}
+	return cv_img;
 }
 cv::Mat ReadImageToCVMat(const string& filename, const int height,
-    const int width, const int min_dim, const int max_dim,
-    const bool is_color) {
-  cv::Mat cv_img;
-  int cv_read_flag = (is_color ? CV_LOAD_IMAGE_COLOR :
-    CV_LOAD_IMAGE_GRAYSCALE);
-  cv::Mat cv_img_origin = cv::imread(filename, cv_read_flag);
-  //LOG(INFO) << filename;
-  if (!cv_img_origin.data) {
-    LOG(ERROR) << "Could not open or find file " << filename;
-    return cv_img_origin;
-  }
-  if (min_dim > 0 || max_dim > 0) {
-    int num_rows = cv_img_origin.rows;
-    int num_cols = cv_img_origin.cols;
-    int min_num = std::min(num_rows, num_cols);
-    int max_num = std::max(num_rows, num_cols);
-    float scale_factor = 1;
-    if (min_dim > 0 && min_num < min_dim) {
-      scale_factor = static_cast<float>(min_dim) / min_num;
-    }
-    if (max_dim > 0 && static_cast<int>(scale_factor * max_num) > max_dim) {
-      // Make sure the maximum dimension is less than max_dim.
-      scale_factor = static_cast<float>(max_dim) / max_num;
-    }
-    if (scale_factor == 1) {
-      cv_img = cv_img_origin;
-    } else {
-      cv::resize(cv_img_origin, cv_img, cv::Size(0, 0),
-                 scale_factor, scale_factor);
-    }
-  } else if (height > 0 && width > 0) {
-    cv::resize(cv_img_origin, cv_img, cv::Size(width, height));
-  } else {
-    cv_img = cv_img_origin;
-  }
-  return cv_img;
+	const int width, const int min_dim, const int max_dim,
+	const bool is_color) {
+	cv::Mat cv_img;
+	int cv_read_flag = (is_color ? CV_LOAD_IMAGE_COLOR :
+		CV_LOAD_IMAGE_GRAYSCALE);
+	cv::Mat cv_img_origin = cv::imread(filename, cv_read_flag);
+	if (!cv_img_origin.data) {
+		LOG(ERROR) << "Could not open or find file " << filename;
+		return cv_img_origin;
+	}
+	if (min_dim > 0 || max_dim > 0) {
+		int num_rows = cv_img_origin.rows;
+		int num_cols = cv_img_origin.cols;
+		int min_num = std::min(num_rows, num_cols);
+		int max_num = std::max(num_rows, num_cols);
+		float scale_factor = 1;
+		if (min_dim > 0 && min_num < min_dim) {
+			scale_factor = static_cast<float>(min_dim) / min_num;
+		}
+		if (max_dim > 0 && static_cast<int>(scale_factor * max_num) > max_dim) {
+			// Make sure the maximum dimension is less than max_dim.
+			scale_factor = static_cast<float>(max_dim) / max_num;
+		}
+		if (scale_factor == 1) {
+			cv_img = cv_img_origin;
+		} else {
+			cv::resize(cv_img_origin, cv_img, cv::Size(0, 0),
+				scale_factor, scale_factor);
+		}
+	} else if (height > 0 && width > 0) {
+		cv::resize(cv_img_origin, cv_img, cv::Size(width, height));
+	} else {
+		cv_img = cv_img_origin;
+	}
+	return cv_img;
 }
 
 cv::Mat ReadImageToCVMat(const string& filename, const int height,
-    const int width, const int min_dim, const int max_dim) {
-  return ReadImageToCVMat(filename, height, width, min_dim, max_dim, true);
+	const int width, const int min_dim, const int max_dim) {
+	return ReadImageToCVMat(filename, height, width, min_dim, max_dim, true);
 }
 
 cv::Mat ReadImageToCVMat(const string& filename,
-    const int height, const int width, const bool is_color) {
-  return ReadImageToCVMat(filename, height, width, 0, 0, is_color);
+	const int height, const int width, const bool is_color) {
+	return ReadImageToCVMat(filename, height, width, 0, 0, is_color);
 }
 
 cv::Mat ReadImageToCVMat(const string& filename,
-    const int height, const int width) {
-  return ReadImageToCVMat(filename, height, width, true);
+	const int height, const int width) {
+	return ReadImageToCVMat(filename, height, width, true);
 }
 
 cv::Mat ReadImageToCVMat(const string& filename,
-    const bool is_color) {
-  return ReadImageToCVMat(filename, 0, 0, is_color);
+	const bool is_color) {
+	return ReadImageToCVMat(filename, 0, 0, is_color);
 }
 
 cv::Mat ReadImageToCVMat(const string& filename) {
-  return ReadImageToCVMat(filename, 0, 0, true);
+	return ReadImageToCVMat(filename, 0, 0, true);
 }
 
 // Do the file extension and encoding match?
-static bool matchExt(const std::string & fn,
-                     std::string en) {
-  size_t p = fn.rfind('.') + 1;
-  std::string ext = p != fn.npos ? fn.substr(p) : fn;
-  std::transform(ext.begin(), ext.end(), ext.begin(), ::tolower);
-  std::transform(en.begin(), en.end(), en.begin(), ::tolower);
-  if ( ext == en )
-    return true;
-  if ( en == "jpg" && ext == "jpeg" )
-    return true;
-  return false;
+static bool matchExt(const std::string& fn,
+	std::string en) {
+	size_t p = fn.rfind('.') + 1;
+	std::string ext = p != fn.npos ? fn.substr(p) : fn;
+	std::transform(ext.begin(), ext.end(), ext.begin(), ::tolower);
+	std::transform(en.begin(), en.end(), en.begin(), ::tolower);
+	if (ext == en)
+		return true;
+	if (en == "jpg" && ext == "jpeg")
+		return true;
+	return false;
 }
 
 bool ReadImageToDatum(const string& filename, const int label,
-    const int height, const int width, const int min_dim, const int max_dim,
-    const bool is_color, const std::string & encoding, Datum* datum) {
-  cv::Mat cv_img = ReadImageToCVMat(filename, height, width, min_dim, max_dim,
-                                    is_color);
-  if (cv_img.data) {
-    if (encoding.size()) {
-      if ( (cv_img.channels() == 3) == is_color && !height && !width &&
-          !min_dim && !max_dim && matchExt(filename, encoding) ) {
-        datum->set_channels(cv_img.channels());
-        datum->set_height(cv_img.rows);
-        datum->set_width(cv_img.cols);
-        return ReadFileToDatum(filename, label, datum);
-      }
-      EncodeCVMatToDatum(cv_img, encoding, datum);
-      datum->set_label(label);
-      return true;
-    }
-    CVMatToDatum(cv_img, datum);
-    datum->set_label(label);
-    return true;
-  } else {
-    return false;
-  }
+	const int height, const int width, const int min_dim, const int max_dim,
+	const bool is_color, const std::string& encoding, Datum* datum) {
+	cv::Mat cv_img = ReadImageToCVMat(filename, height, width, min_dim, max_dim,
+		is_color);
+	if (cv_img.data) {
+		if (encoding.size()) {
+			if ((cv_img.channels() == 3) == is_color && !height && !width &&
+				!min_dim && !max_dim && matchExt(filename, encoding)) {
+				datum->set_channels(cv_img.channels());
+				datum->set_height(cv_img.rows);
+				datum->set_width(cv_img.cols);
+				return ReadFileToDatum(filename, label, datum);
+				EncodeCVMatToDatum(cv_img, encoding, datum);
+				datum->set_label(label);
+				return true;
+			}
+			CVMatToDatum(cv_img, datum);
+			datum->set_label(label);
+			return true;
+		} else {
+			return false;
+		}
+	}
 }
+
 bool ReadImageToDatumSeg(const string& filename, const int label,
 	const int height, const int width, const int min_dim, const int max_dim,
 	const bool is_color, const std::string & encoding, Datum* datum) {
 	cv::Mat cv_img = ReadImageToCVMat(filename, height, width, min_dim, max_dim,
 		is_color);
-	if (cv_img.data) {		
+	if (cv_img.data) {
 		if (encoding.size()) {
 
 			if ((cv_img.channels() == 3) == is_color && !height && !width &&
@@ -218,161 +227,160 @@ bool ReadImageToDatumSeg(const string& filename, const int label,
 				return ReadFileToDatumSeg(filename, label, datum);
 			}
 			EncodeCVMatToDatumSeg(cv_img, encoding, datum);
-			
+
 			//datum->set_label(label);
 			return true;
 		}
 		CVMatToDatumSeg(cv_img, datum);
 		//datum->set_label(label);
 		return true;
-	}
-	else {
+	} else {
 		return false;
 	}
 }
 void GetImageSize(const string& filename, int* height, int* width) {
-  cv::Mat cv_img = cv::imread(filename);
-  if (!cv_img.data) {
-    LOG(ERROR) << "Could not open or find file " << filename;
-    return;
-  }
-  *height = cv_img.rows;
-  *width = cv_img.cols;
+	cv::Mat cv_img = cv::imread(filename);
+	if (!cv_img.data) {
+		LOG(ERROR) << "Could not open or find file " << filename;
+		return;
+	}
+	*height = cv_img.rows;
+	*width = cv_img.cols;
 }
 bool ReadRichImageToAnnotatedDatumWithSeg(const string& filename,
-    const string& labelfile,const string& seg_filename, const int height, const int width,
-    const int min_dim, const int max_dim, const bool is_color,
-    const std::string& encoding, const AnnotatedDatum_AnnotationType type,
-    const string& labeltype, const std::map<string, int>& name_to_label,
-    AnnotatedDatum* anno_datum) {
-  // Read image to datum.
-  bool status = ReadImageToDatum(filename, -1, height, width,
-                                 min_dim, max_dim, is_color, encoding,
-                                 anno_datum->mutable_datum());    
-  if (status == false) {
-    return status;
-  }
-  anno_datum->clear_annotation_group();
-  if (!boost::filesystem::exists(labelfile)) {
-    return true;
-  }
-  cv::Mat cv_img;
-  switch (type) {
-    case AnnotatedDatum_AnnotationType_BBOX:
-      int ori_height, ori_width;
-      GetImageSize(filename, &ori_height, &ori_width);
-      if (labeltype == "xml") {
-        return ReadXMLToAnnotatedDatum(labelfile, ori_height, ori_width,
-                                       name_to_label, anno_datum);
-      } else if (labeltype == "json") {
-        return ReadJSONToAnnotatedDatum(labelfile, ori_height, ori_width,
-                                        name_to_label, anno_datum);
-      } else if (labeltype == "txt") {
-        return ReadTxtToAnnotatedDatum(labelfile, ori_height, ori_width,
-                                       anno_datum);
-      } else {
-        LOG(FATAL) << "Unknown label file type.";
-        return false;
-      }
-      break;
-    case AnnotatedDatum_AnnotationType_BBOXandSeg:
-      GetImageSize(filename, &ori_height, &ori_width);
-      if (labeltype == "xml") {
-        ReadXMLToAnnotatedDatum(labelfile, ori_height, ori_width,
-                                       name_to_label, anno_datum);
-      } else if (labeltype == "json") {
-        ReadJSONToAnnotatedDatum(labelfile, ori_height, ori_width,
-                                        name_to_label, anno_datum);
-      } else if (labeltype == "txt") {
-        ReadTxtToAnnotatedDatum(labelfile, ori_height, ori_width,
-                                       anno_datum);
-      } else {
-        LOG(FATAL) << "Unknown label file type.";
-        return false;
-      }
-      //LOG(INFO) << "ttttttttttttttttttttttttttttttttttttttttttttttttttttttt";
-      status = ReadImageToDatumSeg(seg_filename, -1, height, width,
-		  min_dim, max_dim, is_color, "png",
-		  anno_datum->mutable_datum());
-      //cv_img = ReadImageToCVMat(seg_filename, height, width, min_dim, max_dim,is_color);
-      //CVMatToDatumSeg(cv_img,anno_datum->mutable_datum()); 
-      return status;                           
-      break;
-
-    default:
-      LOG(FATAL) << "Unknown annotation type.";
-      return false;
-  }
+	const string& labelfile, const string& seg_filename, const int height, const int width,
+	const int min_dim, const int max_dim, const bool is_color,
+	const std::string& encoding, const AnnotatedDatum_AnnotationType type,
+	const string& labeltype, const std::map<string, int>& name_to_label,
+	AnnotatedDatum* anno_datum) {
+	// Read image to datum.
+	bool status = ReadImageToDatum(filename, -1, height, width,
+		min_dim, max_dim, is_color, encoding,
+		anno_datum->mutable_datum());
+	if (status == false) {
+		return status;
+	}
+	anno_datum->clear_annotation_group();
+	if (!boost::filesystem::exists(labelfile)) {
+		return true;
+	}
+	cv::Mat cv_img;
+	switch (type) {
+	case AnnotatedDatum_AnnotationType_BBOX:
+		int ori_height, ori_width;
+		GetImageSize(filename, &ori_height, &ori_width);
+		if (labeltype == "xml") {
+			return ReadXMLToAnnotatedDatum(labelfile, ori_height, ori_width,
+				name_to_label, anno_datum);
+		} else if (labeltype == "json") {
+			return ReadJSONToAnnotatedDatum(labelfile, ori_height, ori_width,
+				name_to_label, anno_datum);
+		} else if (labeltype == "txt") {
+			return ReadTxtToAnnotatedDatum(labelfile, ori_height, ori_width,
+				anno_datum);
+		} else {
+			LOG(FATAL) << "Unknown label file type.";
+			return false;
+		}
+		break;
+	case AnnotatedDatum_AnnotationType_BBOXandSeg:
+		GetImageSize(filename, &ori_height, &ori_width);
+		if (labeltype == "xml") {
+			ReadXMLToAnnotatedDatum(labelfile, ori_height, ori_width,
+				name_to_label, anno_datum);
+		} else if (labeltype == "json") {
+			ReadJSONToAnnotatedDatum(labelfile, ori_height, ori_width,
+				name_to_label, anno_datum);
+		} else if (labeltype == "txt") {
+			ReadTxtToAnnotatedDatum(labelfile, ori_height, ori_width,
+				anno_datum);
+		} else {
+			LOG(FATAL) << "Unknown label file type.";
+			return false;
+		}
+		//LOG(INFO) << "ttttttttttttttttttttttttttttttttttttttttttttttttttttttt";
+		status = ReadImageToDatumSeg(seg_filename, -1, height, width,
+			min_dim, max_dim, is_color, "png",
+			anno_datum->mutable_datum());
+		//cv_img = ReadImageToCVMat(seg_filename, height, width, min_dim, max_dim,is_color);
+		//CVMatToDatumSeg(cv_img,anno_datum->mutable_datum()); 
+		return status;
+		break;
+
+	default:
+		LOG(FATAL) << "Unknown annotation type.";
+		return false;
+	}
 
 }
 bool ReadRichImageToAnnotatedDatum(const string& filename,
-    const string& labelfile, const int height, const int width,
-    const int min_dim, const int max_dim, const bool is_color,
-    const string& encoding, const AnnotatedDatum_AnnotationType type,
-    const string& labeltype, const std::map<string, int>& name_to_label,
-    AnnotatedDatum* anno_datum) {
-  // Read image to datum.
-  bool status = ReadImageToDatum(filename, -1, height, width,
-                                 min_dim, max_dim, is_color, encoding,
-                                 anno_datum->mutable_datum());
-  if (status == false) {
-    return status;
-  }
-  anno_datum->clear_annotation_group();
-  if (!boost::filesystem::exists(labelfile)) {
-    return true;
-  }
-  switch (type) {
-    case AnnotatedDatum_AnnotationType_BBOX:
-      int ori_height, ori_width;
-      GetImageSize(filename, &ori_height, &ori_width);
-      if (labeltype == "xml") {
-        return ReadXMLToAnnotatedDatum(labelfile, ori_height, ori_width,
-                                       name_to_label, anno_datum);
-      } else if (labeltype == "json") {
-        return ReadJSONToAnnotatedDatum(labelfile, ori_height, ori_width,
-                                        name_to_label, anno_datum);
-      } else if (labeltype == "txt") {
-        return ReadTxtToAnnotatedDatum(labelfile, ori_height, ori_width,
-                                       anno_datum);
-      } else {
-        LOG(FATAL) << "Unknown label file type.";
-        return false;
-      }
-      break;
-    case AnnotatedDatum_AnnotationType_LANE:
-      //LOG(INFO)<<labeltype;
-      GetImageSize(filename, &ori_height, &ori_width);
-      if (labeltype == "json") {
-        return ReadJSONToAnnotatedDatum(labelfile, ori_height, ori_width, anno_datum);
-      }
-      return false;
-    default:
-      LOG(FATAL) << "Unknown annotation type.";
-      return false;
-  }
+	const string& labelfile, const int height, const int width,
+	const int min_dim, const int max_dim, const bool is_color,
+	const string& encoding, const AnnotatedDatum_AnnotationType type,
+	const string& labeltype, const std::map<string, int>& name_to_label,
+	AnnotatedDatum* anno_datum) {
+	// Read image to datum.
+	bool status = ReadImageToDatum(filename, -1, height, width,
+		min_dim, max_dim, is_color, encoding,
+		anno_datum->mutable_datum());
+	if (status == false) {
+		return status;
+	}
+	anno_datum->clear_annotation_group();
+	if (!boost::filesystem::exists(labelfile)) {
+		return true;
+	}
+	switch (type) {
+	case AnnotatedDatum_AnnotationType_BBOX:
+		int ori_height, ori_width;
+		GetImageSize(filename, &ori_height, &ori_width);
+		if (labeltype == "xml") {
+			return ReadXMLToAnnotatedDatum(labelfile, ori_height, ori_width,
+				name_to_label, anno_datum);
+		} else if (labeltype == "json") {
+			return ReadJSONToAnnotatedDatum(labelfile, ori_height, ori_width,
+				name_to_label, anno_datum);
+		} else if (labeltype == "txt") {
+			return ReadTxtToAnnotatedDatum(labelfile, ori_height, ori_width,
+				anno_datum);
+		} else {
+			LOG(FATAL) << "Unknown label file type.";
+			return false;
+		}
+		break;
+	case AnnotatedDatum_AnnotationType_LANE:
+		//LOG(INFO)<<labeltype;
+		GetImageSize(filename, &ori_height, &ori_width);
+		if (labeltype == "json") {
+			return ReadJSONToAnnotatedDatum(labelfile, ori_height, ori_width, anno_datum);
+		}
+		return false;
+	default:
+		LOG(FATAL) << "Unknown annotation type.";
+		return false;
+	}
 }
 
 #endif  // USE_OPENCV
 
 bool ReadFileToDatum(const string& filename, const int label,
-    Datum* datum) {
-  std::streampos size;
-
-  fstream file(filename.c_str(), ios::in|ios::binary|ios::ate);
-  if (file.is_open()) {
-    size = file.tellg();
-    std::string buffer(size, ' ');
-    file.seekg(0, ios::beg);
-    file.read(&buffer[0], size);
-    file.close();
-    datum->set_data(buffer);
-    datum->set_label(label);
-    datum->set_encoded(true);
-    return true;
-  } else {
-    return false;
-  }
+	Datum* datum) {
+	std::streampos size;
+
+	fstream file(filename.c_str(), ios::in | ios::binary | ios::ate);
+	if (file.is_open()) {
+		size = file.tellg();
+		std::string buffer(size, ' ');
+		file.seekg(0, ios::beg);
+		file.read(&buffer[0], size);
+		file.close();
+		datum->set_data(buffer);
+		datum->set_label(label);
+		datum->set_encoded(true);
+		return true;
+	} else {
+		return false;
+	}
 }
 bool ReadFileToDatumSeg(const string& filename, const int label,
 	Datum* datum) {
@@ -389,538 +397,539 @@ bool ReadFileToDatumSeg(const string& filename, const int label,
 		//datum->set_label(label);
 		//datum->set_encoded(true);
 		return true;
-	}
-	else {
+	} else {
 		return false;
 	}
 }
-// Parse VOC/ILSVRC detection annotation.
+	// Parse VOC/ILSVRC detection annotation.
 bool ReadXMLToAnnotatedDatum(const string& labelfile, const int img_height,
-    const int img_width, const std::map<string, int>& name_to_label,
-    AnnotatedDatum* anno_datum) {
-  ptree pt;
-  read_xml(labelfile, pt);
-
-  // Parse annotation.
-  int width = 0, height = 0;
-  try {
-    height = pt.get<int>("annotation.size.height");
-    width = pt.get<int>("annotation.size.width");
-  } catch (const ptree_error &e) {
-    LOG(WARNING) << "When parsing " << labelfile << ": " << e.what();
-    height = img_height;
-    width = img_width;
-  }
-  LOG_IF(WARNING, height != img_height) << labelfile <<
-      " inconsistent image height.";
-  LOG_IF(WARNING, width != img_width) << labelfile <<
-      " inconsistent image width.";
-  CHECK(width != 0 && height != 0) << labelfile <<
-      " no valid image width/height.";
-  int instance_id = 0;
-  BOOST_FOREACH(ptree::value_type &v1, pt.get_child("annotation")) {
-    ptree pt1 = v1.second;
-    if (v1.first == "object") {
-      Annotation* anno = NULL;
-      bool difficult = false;
-      ptree object = v1.second;
-      BOOST_FOREACH(ptree::value_type &v2, object.get_child("")) {
-        ptree pt2 = v2.second;
-        if (v2.first == "name") {
-          string name = pt2.data();
-          if (name_to_label.find(name) == name_to_label.end()) {
-            LOG(FATAL) << "Unknown name: " << name;
-          }
-          int label = name_to_label.find(name)->second;
-          bool found_group = false;
-          for (int g = 0; g < anno_datum->annotation_group_size(); ++g) {
-            AnnotationGroup* anno_group =
-                anno_datum->mutable_annotation_group(g);
-            if (label == anno_group->group_label()) {
-              if (anno_group->annotation_size() == 0) {
-                instance_id = 0;
-              } else {
-                instance_id = anno_group->annotation(
-                    anno_group->annotation_size() - 1).instance_id() + 1;
-              }
-              anno = anno_group->add_annotation();
-              found_group = true;
-            }
-          }
-          if (!found_group) {
-            // If there is no such annotation_group, create a new one.
-            AnnotationGroup* anno_group = anno_datum->add_annotation_group();
-            anno_group->set_group_label(label);
-            anno = anno_group->add_annotation();
-            instance_id = 0;
-          }
-          anno->set_instance_id(instance_id++);
-        } else if (v2.first == "difficult") {
-          difficult = pt2.data() == "1";
-        } else if (v2.first == "bndbox") {
-          int xmin = pt2.get("xmin", 0);
-          int ymin = pt2.get("ymin", 0);
-          int xmax = pt2.get("xmax", 0);
-          int ymax = pt2.get("ymax", 0);
-          CHECK_NOTNULL(anno);
-          LOG_IF(WARNING, xmin > width) << labelfile <<
-              " bounding box exceeds image boundary.";
-          LOG_IF(WARNING, ymin > height) << labelfile <<
-              " bounding box exceeds image boundary.";
-          LOG_IF(WARNING, xmax > width) << labelfile <<
-              " bounding box exceeds image boundary.";
-          LOG_IF(WARNING, ymax > height) << labelfile <<
-              " bounding box exceeds image boundary.";
-          LOG_IF(WARNING, xmin < 0) << labelfile <<
-              " bounding box exceeds image boundary.";
-          LOG_IF(WARNING, ymin < 0) << labelfile <<
-              " bounding box exceeds image boundary.";
-          LOG_IF(WARNING, xmax < 0) << labelfile <<
-              " bounding box exceeds image boundary.";
-          LOG_IF(WARNING, ymax < 0) << labelfile <<
-              " bounding box exceeds image boundary.";
-          LOG_IF(WARNING, xmin > xmax) << labelfile <<
-              " bounding box irregular.";
-          LOG_IF(WARNING, ymin > ymax) << labelfile <<
-              " bounding box irregular.";
-          // Store the normalized bounding box.
-          NormalizedBBox* bbox = anno->mutable_bbox();
-          bbox->set_xmin(static_cast<float>(xmin) / width);
-          bbox->set_ymin(static_cast<float>(ymin) / height);
-          bbox->set_xmax(static_cast<float>(xmax) / width);
-          bbox->set_ymax(static_cast<float>(ymax) / height);
-          bbox->set_difficult(difficult);
-        }
-      }
-    }
-  }
-  return true;
+	const int img_width, const std::map<string, int>& name_to_label,
+	AnnotatedDatum* anno_datum) {
+	ptree pt;
+	read_xml(labelfile, pt);
+
+	// Parse annotation.
+	int width = 0, height = 0;
+	try {
+		height = pt.get<int>("annotation.size.height");
+		width = pt.get<int>("annotation.size.width");
+	} catch (const ptree_error& e) {
+		LOG(WARNING) << "When parsing " << labelfile << ": " << e.what();
+		height = img_height;
+		width = img_width;
+	}
+	LOG_IF(WARNING, height != img_height) << labelfile <<
+		" inconsistent image height.";
+	LOG_IF(WARNING, width != img_width) << labelfile <<
+		" inconsistent image width.";
+	CHECK(width != 0 && height != 0) << labelfile <<
+		" no valid image width/height.";
+	int instance_id = 0;
+	BOOST_FOREACH(ptree::value_type & v1, pt.get_child("annotation")) {
+		ptree pt1 = v1.second;
+		if (v1.first == "object") {
+			Annotation* anno = NULL;
+			bool difficult = false;
+			ptree object = v1.second;
+			BOOST_FOREACH(ptree::value_type & v2, object.get_child("")) {
+				ptree pt2 = v2.second;
+				if (v2.first == "name") {
+					string name = pt2.data();
+					if (name_to_label.find(name) == name_to_label.end()) {
+						LOG(FATAL) << "Unknown name: " << name;
+					}
+					int label = name_to_label.find(name)->second;
+					bool found_group = false;
+					for (int g = 0; g < anno_datum->annotation_group_size(); ++g) {
+						AnnotationGroup* anno_group =
+							anno_datum->mutable_annotation_group(g);
+						if (label == anno_group->group_label()) {
+							if (anno_group->annotation_size() == 0) {
+								instance_id = 0;
+							} else {
+								instance_id = anno_group->annotation(
+									anno_group->annotation_size() - 1).instance_id() + 1;
+							}
+							anno = anno_group->add_annotation();
+							found_group = true;
+						}
+					}
+					if (!found_group) {
+						// If there is no such annotation_group, create a new one.
+						AnnotationGroup* anno_group = anno_datum->add_annotation_group();
+						anno_group->set_group_label(label);
+						anno = anno_group->add_annotation();
+						instance_id = 0;
+					}
+					anno->set_instance_id(instance_id++);
+				} else if (v2.first == "difficult") {
+					difficult = pt2.data() == "1";
+				} else if (v2.first == "bndbox") {
+					int xmin = pt2.get("xmin", 0);
+					int ymin = pt2.get("ymin", 0);
+					int xmax = pt2.get("xmax", 0);
+					int ymax = pt2.get("ymax", 0);
+					CHECK_NOTNULL(anno);
+					LOG_IF(WARNING, xmin > width) << labelfile <<
+						" bounding box exceeds image boundary.";
+					LOG_IF(WARNING, ymin > height) << labelfile <<
+						" bounding box exceeds image boundary.";
+					LOG_IF(WARNING, xmax > width) << labelfile <<
+						" bounding box exceeds image boundary.";
+					LOG_IF(WARNING, ymax > height) << labelfile <<
+						" bounding box exceeds image boundary.";
+					LOG_IF(WARNING, xmin < 0) << labelfile <<
+						" bounding box exceeds image boundary.";
+					LOG_IF(WARNING, ymin < 0) << labelfile <<
+						" bounding box exceeds image boundary.";
+					LOG_IF(WARNING, xmax < 0) << labelfile <<
+						" bounding box exceeds image boundary.";
+					LOG_IF(WARNING, ymax < 0) << labelfile <<
+						" bounding box exceeds image boundary.";
+					LOG_IF(WARNING, xmin > xmax) << labelfile <<
+						" bounding box irregular.";
+					LOG_IF(WARNING, ymin > ymax) << labelfile <<
+						" bounding box irregular.";
+					// Store the normalized bounding box.
+					NormalizedBBox* bbox = anno->mutable_bbox();
+					bbox->set_xmin(static_cast<float>(xmin) / width);
+					bbox->set_ymin(static_cast<float>(ymin) / height);
+					bbox->set_xmax(static_cast<float>(xmax) / width);
+					bbox->set_ymax(static_cast<float>(ymax) / height);
+					bbox->set_difficult(difficult);
+				}
+			}
+		}
+	}
+	return true;
 }
 // Parse tusimple lane detection annotation.
 bool ReadJSONToAnnotatedDatum(const string& labelfile, const int img_height,
-    const int img_width,AnnotatedDatum* anno_datum) {
-  ptree pt;
-  read_json(labelfile, pt);
-  
-  // Get image info.
-  int width = 0, height = 0;
-  try {
-    height = pt.get<int>("image.height");
-    width = pt.get<int>("image.width");
-  } catch (const ptree_error &e) {
-    //LOG(WARNING) << "When parsing " << labelfile << ": " << e.what();
-    height = img_height;
-    width = img_width;
-  }
-
-  LOG_IF(WARNING, height != img_height) << labelfile <<
-      " inconsistent image height.";
-  LOG_IF(WARNING, width != img_width) << labelfile <<
-      " inconsistent image width.";
-  CHECK(width != 0 && height != 0) << labelfile <<
-      " no valid image width/height.";
-  int instance_id = 0;
-  std::vector<int> h_samples;
-  BOOST_FOREACH(ptree::value_type& v1, pt.get_child("h_samples")) { //h_samples
-    Annotation* anno = NULL;
-    ptree object = v1.second;
-    h_samples.push_back(object.get_value<int>());
-  } 
-  //int count = 0;
-  BOOST_FOREACH(ptree::value_type& v1, pt.get_child("lanes")) { 
-    Annotation* anno = NULL;
-    ptree object = v1.second;
-    //LOG(INFO) << object.get_value<int>();
-    //object.get_value<std::vector>;
-    std::vector<int> lane;
-    lane.clear();
-    BOOST_FOREACH(ptree::value_type& v2, object.get_child("")) {
-      ptree object2 = v2.second;
-      //LOG(INFO) << object2.get_value<int>();
-      lane.push_back(object2.get_value<int>());
-    }
-    CHECK_EQ(h_samples.size(), lane.size());
-    //anno = anno_group->add_annotation();
-    //LOG(INFO)<<h_samples.size();
-    AnnotationGroup* anno_group = anno_datum->add_annotation_group();
-    for(int i=0;i<h_samples.size();i++) {
-      
-      anno = anno_group->add_annotation();
-      instance_id = i;
-      
-      LaneInfo* LaneXY = anno->mutable_lanes();
-      
-      LaneXY->set_x(lane[i] / (float)width);
-      LaneXY->set_y(h_samples[i] / (float)height);
-      
-      anno->set_instance_id(instance_id);
-      
-    }  
-    //count++;
-  }    
-  //LOG(INFO)<<count;
-  //LOG(INFO)<<lanes.size();
- 
-  return true;
+	const int img_width, AnnotatedDatum* anno_datum) {
+	ptree pt;
+	read_json(labelfile, pt);
+
+	// Get image info.
+	int width = 0, height = 0;
+	try {
+		height = pt.get<int>("image.height");
+		width = pt.get<int>("image.width");
+	} catch (const ptree_error& e) {
+		//LOG(WARNING) << "When parsing " << labelfile << ": " << e.what();
+		height = img_height;
+		width = img_width;
+	}
+
+	LOG_IF(WARNING, height != img_height) << labelfile <<
+		" inconsistent image height.";
+	LOG_IF(WARNING, width != img_width) << labelfile <<
+		" inconsistent image width.";
+	CHECK(width != 0 && height != 0) << labelfile <<
+		" no valid image width/height.";
+	int instance_id = 0;
+	std::vector<int> h_samples;
+	BOOST_FOREACH(ptree::value_type & v1, pt.get_child("h_samples")) { //h_samples
+		Annotation* anno = NULL;
+		ptree object = v1.second;
+		h_samples.push_back(object.get_value<int>());
+	}
+	//int count = 0;
+	BOOST_FOREACH(ptree::value_type & v1, pt.get_child("lanes")) {
+		Annotation* anno = NULL;
+		ptree object = v1.second;
+		//LOG(INFO) << object.get_value<int>();
+		//object.get_value<std::vector>;
+		std::vector<int> lane;
+		lane.clear();
+		BOOST_FOREACH(ptree::value_type & v2, object.get_child("")) {
+			ptree object2 = v2.second;
+			//LOG(INFO) << object2.get_value<int>();
+			lane.push_back(object2.get_value<int>());
+		}
+		CHECK_EQ(h_samples.size(), lane.size());
+		//anno = anno_group->add_annotation();
+		//LOG(INFO)<<h_samples.size();
+		AnnotationGroup* anno_group = anno_datum->add_annotation_group();
+		for (int i = 0; i < h_samples.size(); i++) {
+
+			anno = anno_group->add_annotation();
+			instance_id = i;
+
+			LaneInfo* LaneXY = anno->mutable_lanes();
+
+			LaneXY->set_x(lane[i] / (float)width);
+			LaneXY->set_y(h_samples[i] / (float)height);
+
+			anno->set_instance_id(instance_id);
+
+		}
+		//count++;
+	}
+	//LOG(INFO)<<count;
+	//LOG(INFO)<<lanes.size();
+
+	return true;
 }
+
 // Parse MSCOCO detection annotation.
 bool ReadJSONToAnnotatedDatum(const string& labelfile, const int img_height,
-    const int img_width, const std::map<string, int>& name_to_label,
-    AnnotatedDatum* anno_datum) {
-  ptree pt;
-  read_json(labelfile, pt);
-
-  // Get image info.
-  int width = 0, height = 0;
-  try {
-    height = pt.get<int>("image.height");
-    width = pt.get<int>("image.width");
-  } catch (const ptree_error &e) {
-    LOG(WARNING) << "When parsing " << labelfile << ": " << e.what();
-    height = img_height;
-    width = img_width;
-  }
-  LOG_IF(WARNING, height != img_height) << labelfile <<
-      " inconsistent image height.";
-  LOG_IF(WARNING, width != img_width) << labelfile <<
-      " inconsistent image width.";
-  CHECK(width != 0 && height != 0) << labelfile <<
-      " no valid image width/height.";
-
-  // Get annotation info.
-  int instance_id = 0;
-  BOOST_FOREACH(ptree::value_type& v1, pt.get_child("annotation")) {
-    Annotation* anno = NULL;
-    bool iscrowd = false;
-    ptree object = v1.second;
-    // Get category_id.
-    string name = object.get<string>("category_id");
-    if (name_to_label.find(name) == name_to_label.end()) {
-      LOG(FATAL) << "Unknown name: " << name;
-    }
-    int label = name_to_label.find(name)->second;
-    bool found_group = false;
-    for (int g = 0; g < anno_datum->annotation_group_size(); ++g) {
-      AnnotationGroup* anno_group =
-          anno_datum->mutable_annotation_group(g);
-      if (label == anno_group->group_label()) {
-        if (anno_group->annotation_size() == 0) {
-          instance_id = 0;
-        } else {
-          instance_id = anno_group->annotation(
-              anno_group->annotation_size() - 1).instance_id() + 1;
-        }
-        anno = anno_group->add_annotation();
-        found_group = true;
-      }
-    }
-    if (!found_group) {
-      // If there is no such annotation_group, create a new one.
-      AnnotationGroup* anno_group = anno_datum->add_annotation_group();
-      anno_group->set_group_label(label);
-      anno = anno_group->add_annotation();
-      instance_id = 0;
-    }
-    anno->set_instance_id(instance_id++);
-
-    // Get iscrowd.
-    iscrowd = object.get<int>("iscrowd", 0);
-
-    // Get bbox.
-    vector<float> bbox_items;
-    BOOST_FOREACH(ptree::value_type& v2, object.get_child("bbox")) {
-      bbox_items.push_back(v2.second.get_value<float>());
-    }
-    CHECK_EQ(bbox_items.size(), 4);
-    float xmin = bbox_items[0];
-    float ymin = bbox_items[1];
-    float xmax = bbox_items[0] + bbox_items[2];
-    float ymax = bbox_items[1] + bbox_items[3];
-    CHECK_NOTNULL(anno);
-    LOG_IF(WARNING, xmin > width) << labelfile <<
-        " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, ymin > height) << labelfile <<
-        " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, xmax > width) << labelfile <<
-        " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, ymax > height) << labelfile <<
-        " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, xmin < 0) << labelfile <<
-        " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, ymin < 0) << labelfile <<
-        " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, xmax < 0) << labelfile <<
-        " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, ymax < 0) << labelfile <<
-        " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, xmin > xmax) << labelfile <<
-        " bounding box irregular.";
-    LOG_IF(WARNING, ymin > ymax) << labelfile <<
-        " bounding box irregular.";
-    // Store the normalized bounding box.
-    NormalizedBBox* bbox = anno->mutable_bbox();
-    bbox->set_xmin(xmin / width);
-    bbox->set_ymin(ymin / height);
-    bbox->set_xmax(xmax / width);
-    bbox->set_ymax(ymax / height);
-    bbox->set_difficult(iscrowd);
-  }
-  return true;
+	const int img_width, const std::map<string, int>& name_to_label,
+	AnnotatedDatum* anno_datum) {
+	ptree pt;
+	read_json(labelfile, pt);
+
+	// Get image info.
+	int width = 0, height = 0;
+	try {
+		height = pt.get<int>("image.height");
+		width = pt.get<int>("image.width");
+	} catch (const ptree_error& e) {
+		LOG(WARNING) << "When parsing " << labelfile << ": " << e.what();
+		height = img_height;
+		width = img_width;
+	}
+	LOG_IF(WARNING, height != img_height) << labelfile <<
+		" inconsistent image height.";
+	LOG_IF(WARNING, width != img_width) << labelfile <<
+		" inconsistent image width.";
+	CHECK(width != 0 && height != 0) << labelfile <<
+		" no valid image width/height.";
+
+	// Get annotation info.
+	int instance_id = 0;
+	BOOST_FOREACH(ptree::value_type & v1, pt.get_child("annotation")) {
+		Annotation* anno = NULL;
+		bool iscrowd = false;
+		ptree object = v1.second;
+		// Get category_id.
+		string name = object.get<string>("category_id");
+		if (name_to_label.find(name) == name_to_label.end()) {
+			LOG(FATAL) << "Unknown name: " << name;
+		}
+		int label = name_to_label.find(name)->second;
+		bool found_group = false;
+		for (int g = 0; g < anno_datum->annotation_group_size(); ++g) {
+			AnnotationGroup* anno_group =
+				anno_datum->mutable_annotation_group(g);
+			if (label == anno_group->group_label()) {
+				if (anno_group->annotation_size() == 0) {
+					instance_id = 0;
+				} else {
+					instance_id = anno_group->annotation(
+						anno_group->annotation_size() - 1).instance_id() + 1;
+				}
+				anno = anno_group->add_annotation();
+				found_group = true;
+			}
+		}
+		if (!found_group) {
+			// If there is no such annotation_group, create a new one.
+			AnnotationGroup* anno_group = anno_datum->add_annotation_group();
+			anno_group->set_group_label(label);
+			anno = anno_group->add_annotation();
+			instance_id = 0;
+		}
+		anno->set_instance_id(instance_id++);
+
+		// Get iscrowd.
+		iscrowd = object.get<int>("iscrowd", 0);
+
+		// Get bbox.
+		vector<float> bbox_items;
+		BOOST_FOREACH(ptree::value_type & v2, object.get_child("bbox")) {
+			bbox_items.push_back(v2.second.get_value<float>());
+		}
+		CHECK_EQ(bbox_items.size(), 4);
+		float xmin = bbox_items[0];
+		float ymin = bbox_items[1];
+		float xmax = bbox_items[0] + bbox_items[2];
+		float ymax = bbox_items[1] + bbox_items[3];
+		CHECK_NOTNULL(anno);
+		LOG_IF(WARNING, xmin > width) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, ymin > height) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, xmax > width) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, ymax > height) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, xmin < 0) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, ymin < 0) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, xmax < 0) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, ymax < 0) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, xmin > xmax) << labelfile <<
+			" bounding box irregular.";
+		LOG_IF(WARNING, ymin > ymax) << labelfile <<
+			" bounding box irregular.";
+		// Store the normalized bounding box.
+		NormalizedBBox* bbox = anno->mutable_bbox();
+		bbox->set_xmin(xmin / width);
+		bbox->set_ymin(ymin / height);
+		bbox->set_xmax(xmax / width);
+		bbox->set_ymax(ymax / height);
+		bbox->set_difficult(iscrowd);
+	}
+	return true;
 }
 
 // Parse plain txt detection annotation: label_id, xmin, ymin, xmax, ymax.
 bool ReadTxtToAnnotatedDatum(const string& labelfile, const int height,
-    const int width, AnnotatedDatum* anno_datum) {
-  std::ifstream infile(labelfile.c_str());
-  if (!infile.good()) {
-    LOG(INFO) << "Cannot open " << labelfile;
-    return false;
-  }
-  int label;
-  float xmin, ymin, xmax, ymax;
-  while (infile >> label >> xmin >> ymin >> xmax >> ymax) {
-    Annotation* anno = NULL;
-    int instance_id = 0;
-    bool found_group = false;
-    for (int g = 0; g < anno_datum->annotation_group_size(); ++g) {
-      AnnotationGroup* anno_group = anno_datum->mutable_annotation_group(g);
-      if (label == anno_group->group_label()) {
-        if (anno_group->annotation_size() == 0) {
-          instance_id = 0;
-        } else {
-          instance_id = anno_group->annotation(
-              anno_group->annotation_size() - 1).instance_id() + 1;
-        }
-        anno = anno_group->add_annotation();
-        found_group = true;
-      }
-    }
-    if (!found_group) {
-      // If there is no such annotation_group, create a new one.
-      AnnotationGroup* anno_group = anno_datum->add_annotation_group();
-      anno_group->set_group_label(label);
-      anno = anno_group->add_annotation();
-      instance_id = 0;
-    }
-    anno->set_instance_id(instance_id++);
-    LOG_IF(WARNING, xmin > width) << labelfile <<
-      " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, ymin > height) << labelfile <<
-      " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, xmax > width) << labelfile <<
-      " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, ymax > height) << labelfile <<
-      " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, xmin < 0) << labelfile <<
-      " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, ymin < 0) << labelfile <<
-      " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, xmax < 0) << labelfile <<
-      " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, ymax < 0) << labelfile <<
-      " bounding box exceeds image boundary.";
-    LOG_IF(WARNING, xmin > xmax) << labelfile <<
-      " bounding box irregular.";
-    LOG_IF(WARNING, ymin > ymax) << labelfile <<
-      " bounding box irregular.";
-    // Store the normalized bounding box.
-    NormalizedBBox* bbox = anno->mutable_bbox();
-    bbox->set_xmin(xmin / width);
-    bbox->set_ymin(ymin / height);
-    bbox->set_xmax(xmax / width);
-    bbox->set_ymax(ymax / height);
-    bbox->set_difficult(false);
-  }
-  return true;
+	const int width, AnnotatedDatum* anno_datum) {
+	std::ifstream infile(labelfile.c_str());
+	if (!infile.good()) {
+		LOG(INFO) << "Cannot open " << labelfile;
+		return false;
+	}
+	int label;
+	float xmin, ymin, xmax, ymax;
+	while (infile >> label >> xmin >> ymin >> xmax >> ymax) {
+		Annotation* anno = NULL;
+		int instance_id = 0;
+		bool found_group = false;
+		for (int g = 0; g < anno_datum->annotation_group_size(); ++g) {
+			AnnotationGroup* anno_group = anno_datum->mutable_annotation_group(g);
+			if (label == anno_group->group_label()) {
+				if (anno_group->annotation_size() == 0) {
+					instance_id = 0;
+				} else {
+					instance_id = anno_group->annotation(
+						anno_group->annotation_size() - 1).instance_id() + 1;
+				}
+				anno = anno_group->add_annotation();
+				found_group = true;
+			}
+		}
+		if (!found_group) {
+			// If there is no such annotation_group, create a new one.
+			AnnotationGroup* anno_group = anno_datum->add_annotation_group();
+			anno_group->set_group_label(label);
+			anno = anno_group->add_annotation();
+			instance_id = 0;
+		}
+		anno->set_instance_id(instance_id++);
+		LOG_IF(WARNING, xmin > width) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, ymin > height) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, xmax > width) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, ymax > height) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, xmin < 0) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, ymin < 0) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, xmax < 0) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, ymax < 0) << labelfile <<
+			" bounding box exceeds image boundary.";
+		LOG_IF(WARNING, xmin > xmax) << labelfile <<
+			" bounding box irregular.";
+		LOG_IF(WARNING, ymin > ymax) << labelfile <<
+			" bounding box irregular.";
+		// Store the normalized bounding box.
+		NormalizedBBox* bbox = anno->mutable_bbox();
+		bbox->set_xmin(xmin / width);
+		bbox->set_ymin(ymin / height);
+		bbox->set_xmax(xmax / width);
+		bbox->set_ymax(ymax / height);
+		bbox->set_difficult(false);
+	}
+	return true;
 }
 
 bool ReadLabelFileToLabelMap(const string& filename, bool include_background,
-    const string& delimiter, LabelMap* map) {
-  // cleanup
-  map->Clear();
-
-  std::ifstream file(filename.c_str());
-  string line;
-  // Every line can have [1, 3] number of fields.
-  // The delimiter between fields can be one of " :;".
-  // The order of the fields are:
-  //  name [label] [display_name]
-  //  ...
-  int field_size = -1;
-  int label = 0;
-  LabelMapItem* map_item;
-  // Add background (none_of_the_above) class.
-  if (include_background) {
-    map_item = map->add_item();
-    map_item->set_name("none_of_the_above");
-    map_item->set_label(label++);
-    map_item->set_display_name("background");
-  }
-  while (std::getline(file, line)) {
-    vector<string> fields;
-    fields.clear();
-    boost::split(fields, line, boost::is_any_of(delimiter));
-    if (field_size == -1) {
-      field_size = fields.size();
-    } else {
-      CHECK_EQ(field_size, fields.size())
-          << "Inconsistent number of fields per line.";
-    }
-    map_item = map->add_item();
-    map_item->set_name(fields[0]);
-    switch (field_size) {
-      case 1:
-        map_item->set_label(label++);
-        map_item->set_display_name(fields[0]);
-        break;
-      case 2:
-        label = std::atoi(fields[1].c_str());
-        map_item->set_label(label);
-        map_item->set_display_name(fields[0]);
-        break;
-      case 3:
-        label = std::atoi(fields[1].c_str());
-        map_item->set_label(label);
-        map_item->set_display_name(fields[2]);
-        break;
-      default:
-        LOG(FATAL) << "The number of fields should be [1, 3].";
-        break;
-    }
-  }
-  return true;
+	const string& delimiter, LabelMap* map) {
+	// cleanup
+	map->Clear();
+
+	std::ifstream file(filename.c_str());
+	string line;
+	// Every line can have [1, 3] number of fields.
+	// The delimiter between fields can be one of " :;".
+	// The order of the fields are:
+	//  name [label] [display_name]
+	//  ...
+	int field_size = -1;
+	int label = 0;
+	LabelMapItem* map_item;
+	// Add background (none_of_the_above) class.
+	if (include_background) {
+		map_item = map->add_item();
+		map_item->set_name("none_of_the_above");
+		map_item->set_label(label++);
+		map_item->set_display_name("background");
+	}
+	while (std::getline(file, line)) {
+		vector<string> fields;
+		fields.clear();
+		boost::split(fields, line, boost::is_any_of(delimiter));
+		if (field_size == -1) {
+			field_size = fields.size();
+		} else {
+			CHECK_EQ(field_size, fields.size())
+				<< "Inconsistent number of fields per line.";
+		}
+		map_item = map->add_item();
+		map_item->set_name(fields[0]);
+		switch (field_size) {
+		case 1:
+			map_item->set_label(label++);
+			map_item->set_display_name(fields[0]);
+			break;
+		case 2:
+			label = std::atoi(fields[1].c_str());
+			map_item->set_label(label);
+			map_item->set_display_name(fields[0]);
+			break;
+		case 3:
+			label = std::atoi(fields[1].c_str());
+			map_item->set_label(label);
+			map_item->set_display_name(fields[2]);
+			break;
+		default:
+			LOG(FATAL) << "The number of fields should be [1, 3].";
+			break;
+		}
+	}
+	return true;
 }
 
 bool MapNameToLabel(const LabelMap& map, const bool strict_check,
-    std::map<string, int>* name_to_label) {
-  // cleanup
-  name_to_label->clear();
-
-  for (int i = 0; i < map.item_size(); ++i) {
-    const string& name = map.item(i).name();
-    const int label = map.item(i).label();
-    if (strict_check) {
-      if (!name_to_label->insert(std::make_pair(name, label)).second) {
-        LOG(FATAL) << "There are many duplicates of name: " << name;
-        return false;
-      }
-    } else {
-      (*name_to_label)[name] = label;
-    }
-  }
-  return true;
+	std::map<string, int>* name_to_label) {
+	// cleanup
+	name_to_label->clear();
+
+	for (int i = 0; i < map.item_size(); ++i) {
+		const string& name = map.item(i).name();
+		const int label = map.item(i).label();
+		if (strict_check) {
+			if (!name_to_label->insert(std::make_pair(name, label)).second) {
+				LOG(FATAL) << "There are many duplicates of name: " << name;
+				return false;
+			}
+		} else {
+			(*name_to_label)[name] = label;
+		}
+	}
+	return true;
 }
 
 bool MapLabelToName(const LabelMap& map, const bool strict_check,
-    std::map<int, string>* label_to_name) {
-  // cleanup
-  label_to_name->clear();
-
-  for (int i = 0; i < map.item_size(); ++i) {
-    const string& name = map.item(i).name();
-    const int label = map.item(i).label();
-    if (strict_check) {
-      if (!label_to_name->insert(std::make_pair(label, name)).second) {
-        LOG(FATAL) << "There are many duplicates of label: " << label;
-        return false;
-      }
-    } else {
-      (*label_to_name)[label] = name;
-    }
-  }
-  return true;
+	std::map<int, string>* label_to_name) {
+	// cleanup
+	label_to_name->clear();
+
+	for (int i = 0; i < map.item_size(); ++i) {
+		const string& name = map.item(i).name();
+		const int label = map.item(i).label();
+		if (strict_check) {
+			if (!label_to_name->insert(std::make_pair(label, name)).second) {
+				LOG(FATAL) << "There are many duplicates of label: " << label;
+				return false;
+			}
+		} else {
+			(*label_to_name)[label] = name;
+		}
+	}
+	return true;
 }
 
 bool MapLabelToDisplayName(const LabelMap& map, const bool strict_check,
-    std::map<int, string>* label_to_display_name) {
-  // cleanup
-  label_to_display_name->clear();
-
-  for (int i = 0; i < map.item_size(); ++i) {
-    const string& display_name = map.item(i).display_name();
-    const int label = map.item(i).label();
-    if (strict_check) {
-      if (!label_to_display_name->insert(
-              std::make_pair(label, display_name)).second) {
-        LOG(FATAL) << "There are many duplicates of label: " << label;
-        return false;
-      }
-    } else {
-      (*label_to_display_name)[label] = display_name;
-    }
-  }
-  return true;
+	std::map<int, string>* label_to_display_name) {
+	// cleanup
+	label_to_display_name->clear();
+
+	for (int i = 0; i < map.item_size(); ++i) {
+		const string& display_name = map.item(i).display_name();
+		const int label = map.item(i).label();
+		if (strict_check) {
+			if (!label_to_display_name->insert(
+				std::make_pair(label, display_name)).second) {
+				LOG(FATAL) << "There are many duplicates of label: " << label;
+				return false;
+			}
+		} else {
+			(*label_to_display_name)[label] = display_name;
+		}
+	}
+	return true;
 }
 
 #ifdef USE_OPENCV
 cv::Mat DecodeDatumToCVMatNative(const Datum& datum) {
-  cv::Mat cv_img;
-  CHECK(datum.encoded()) << "Datum not encoded";
-  const string& data = datum.data();
-  std::vector<char> vec_data(data.c_str(), data.c_str() + data.size());
-  cv_img = cv::imdecode(vec_data, -1);
-  if (!cv_img.data) {
-    LOG(ERROR) << "Could not decode datum ";
-  }
-  return cv_img;
+	cv::Mat cv_img;
+	CHECK(datum.encoded()) << "Datum not encoded";
+	const string& data = datum.data();
+	std::vector<char> vec_data(data.c_str(), data.c_str() + data.size());
+	cv_img = cv::imdecode(vec_data, -1);
+	if (!cv_img.data) {
+		LOG(ERROR) << "Could not decode datum ";
+	}
+	return cv_img;
 }
 cv::Mat DecodeDatumToCVMat(const Datum& datum, bool is_color) {
-  cv::Mat cv_img;
-  CHECK(datum.encoded()) << "Datum not encoded";
-  const string& data = datum.data();
-  std::vector<char> vec_data(data.c_str(), data.c_str() + data.size());
-  int cv_read_flag = (is_color ? CV_LOAD_IMAGE_COLOR :
-    CV_LOAD_IMAGE_GRAYSCALE);
-  cv_img = cv::imdecode(vec_data, cv_read_flag);
-  if (!cv_img.data) {
-    LOG(ERROR) << "Could not decode datum ";
-  }
-  return cv_img;
+	cv::Mat cv_img;
+	CHECK(datum.encoded()) << "Datum not encoded";
+	const string& data = datum.data();
+	std::vector<char> vec_data(data.c_str(), data.c_str() + data.size());
+	int cv_read_flag = (is_color ? CV_LOAD_IMAGE_COLOR :
+		CV_LOAD_IMAGE_GRAYSCALE);
+	cv_img = cv::imdecode(vec_data, cv_read_flag);
+	if (!cv_img.data) {
+		LOG(ERROR) << "Could not decode datum ";
+	}
+	return cv_img;
 }
 cv::Mat DecodeDatumToCVMatSeg(const Datum& datum, bool is_color) {
-  cv::Mat cv_img;
-  CHECK(datum.encoded()) << "Datum not encoded";
-  const string& data = datum.seg_label();
-  std::vector<char> vec_data(data.c_str(), data.c_str() + data.size());
-  int cv_read_flag = (is_color ? CV_LOAD_IMAGE_COLOR :
-	  CV_LOAD_IMAGE_GRAYSCALE);
-  cv_img = cv::imdecode(vec_data, cv_read_flag);
-  if (!cv_img.data) {
-	  LOG(ERROR) << "Could not decode datum ";
-  }
-  return cv_img;
+	cv::Mat cv_img;
+	CHECK(datum.encoded()) << "Datum not encoded";
+	const string& data = datum.seg_label();
+	std::vector<char> vec_data(data.c_str(), data.c_str() + data.size());
+	int cv_read_flag = (is_color ? CV_LOAD_IMAGE_COLOR :
+		CV_LOAD_IMAGE_GRAYSCALE);
+	cv_img = cv::imdecode(vec_data, cv_read_flag);
+	if (!cv_img.data) {
+		LOG(ERROR) << "Could not decode datum ";
+	}
+	return cv_img;
 }
+
 // If Datum is encoded will decoded using DecodeDatumToCVMat and CVMatToDatum
 // If Datum is not encoded will do nothing
 bool DecodeDatumNative(Datum* datum) {
-  if (datum->encoded()) {
-    cv::Mat cv_img = DecodeDatumToCVMatNative((*datum));
-    CVMatToDatum(cv_img, datum);
-    return true;
-  } else {
-    return false;
-  }
+	if (datum->encoded()) {
+		cv::Mat cv_img = DecodeDatumToCVMatNative((*datum));
+		CVMatToDatum(cv_img, datum);
+		return true;
+	} else {
+		return false;
+	}
 }
 bool DecodeDatum(Datum* datum, bool is_color) {
-  if (datum->encoded()) {
-    cv::Mat cv_img = DecodeDatumToCVMat((*datum), is_color);
-    CVMatToDatum(cv_img, datum);
-    return true;
-  } else {
-    return false;
-  }
+	if (datum->encoded()) {
+		cv::Mat cv_img = DecodeDatumToCVMat((*datum), is_color);
+		CVMatToDatum(cv_img, datum);
+		return true;
+	} else {
+		return false;
+	}
 }
 
 void EncodeCVMatToDatum(const cv::Mat& cv_img, const string& encoding,
-                        Datum* datum) {
-  std::vector<uchar> buf;
-  cv::imencode("."+encoding, cv_img, buf);
-  datum->set_data(std::string(reinterpret_cast<char*>(&buf[0]),
-                              buf.size()));
-  datum->set_channels(cv_img.channels());
-  datum->set_height(cv_img.rows);
-  datum->set_width(cv_img.cols);
-  datum->set_encoded(true);
+	Datum* datum) {
+	std::vector<uchar> buf;
+	cv::imencode("." + encoding, cv_img, buf);
+	datum->set_data(std::string(reinterpret_cast<char*>(&buf[0]),
+		buf.size()));
+	datum->set_channels(cv_img.channels());
+	datum->set_height(cv_img.rows);
+	datum->set_width(cv_img.cols);
+	datum->set_encoded(true);
 }
 void EncodeCVMatToDatumSeg(const cv::Mat& cv_img, const string& encoding,
 	Datum* datum) {
@@ -935,47 +944,48 @@ void EncodeCVMatToDatumSeg(const cv::Mat& cv_img, const string& encoding,
 }
 void CVMatToDatumSeg(const cv::Mat& cv_img, Datum* datum) {
 
-  int datum_channels = datum->channels();
-  int datum_height = datum->height();
-  int datum_width = datum->width();
-  int datum_size = datum_channels * datum_height * datum_width;
-  std::string buffer(datum_size, ' ');
-  for (int h = 0; h < datum_height; ++h) {
-    const uchar* ptr = cv_img.ptr<uchar>(h);
-    int img_index = 0;
-    for (int w = 0; w < datum_width; ++w) {
-      for (int c = 0; c < datum_channels; ++c) {
-        int datum_index = (c * datum_height + h) * datum_width + w;
-        buffer[datum_index] = static_cast<char>(ptr[img_index++]);
-      }
-    }
-  }
-  datum->set_seg_label(buffer);
+	int datum_channels = datum->channels();
+	int datum_height = datum->height();
+	int datum_width = datum->width();
+	int datum_size = datum_channels * datum_height * datum_width;
+	std::string buffer(datum_size, ' ');
+	for (int h = 0; h < datum_height; ++h) {
+		const uchar* ptr = cv_img.ptr<uchar>(h);
+		int img_index = 0;
+		for (int w = 0; w < datum_width; ++w) {
+			for (int c = 0; c < datum_channels; ++c) {
+				int datum_index = (c * datum_height + h) * datum_width + w;
+				buffer[datum_index] = static_cast<char>(ptr[img_index++]);
+			}
+		}
+	}
+	datum->set_seg_label(buffer);
 }
+
 void CVMatToDatum(const cv::Mat& cv_img, Datum* datum) {
-  CHECK(cv_img.depth() == CV_8U) << "Image data type must be unsigned byte";
-  datum->set_channels(cv_img.channels());
-  datum->set_height(cv_img.rows);
-  datum->set_width(cv_img.cols);
-  datum->clear_data();
-  datum->clear_float_data();
-  datum->set_encoded(false);
-  int datum_channels = datum->channels();
-  int datum_height = datum->height();
-  int datum_width = datum->width();
-  int datum_size = datum_channels * datum_height * datum_width;
-  std::string buffer(datum_size, ' ');
-  for (int h = 0; h < datum_height; ++h) {
-    const uchar* ptr = cv_img.ptr<uchar>(h);
-    int img_index = 0;
-    for (int w = 0; w < datum_width; ++w) {
-      for (int c = 0; c < datum_channels; ++c) {
-        int datum_index = (c * datum_height + h) * datum_width + w;
-        buffer[datum_index] = static_cast<char>(ptr[img_index++]);
-      }
-    }
-  }
-  datum->set_data(buffer);
+	CHECK(cv_img.depth() == CV_8U) << "Image data type must be unsigned byte";
+	datum->set_channels(cv_img.channels());
+	datum->set_height(cv_img.rows);
+	datum->set_width(cv_img.cols);
+	datum->clear_data();
+	datum->clear_float_data();
+	datum->set_encoded(false);
+	int datum_channels = datum->channels();
+	int datum_height = datum->height();
+	int datum_width = datum->width();
+	int datum_size = datum_channels * datum_height * datum_width;
+	std::string buffer(datum_size, ' ');
+	for (int h = 0; h < datum_height; ++h) {
+		const uchar* ptr = cv_img.ptr<uchar>(h);
+		int img_index = 0;
+		for (int w = 0; w < datum_width; ++w) {
+			for (int c = 0; c < datum_channels; ++c) {
+				int datum_index = (c * datum_height + h) * datum_width + w;
+				buffer[datum_index] = static_cast<char>(ptr[img_index++]);
+			}
+		}
+	}
+	datum->set_data(buffer);
 }
 #endif  // USE_OPENCV
 }  // namespace caffe
diff --git a/src/caffe/util/signal_handler.cpp b/src/caffe/util/signal_handler.cpp
index 5d764ec..6599db4 100644
--- a/src/caffe/util/signal_handler.cpp
+++ b/src/caffe/util/signal_handler.cpp
@@ -13,9 +13,15 @@ namespace {
 
   void handle_signal(int signal) {
     switch (signal) {
+#ifdef _MSC_VER
+    case SIGBREAK:  // there is no SIGHUP in windows, take SIGBREAK instead.
+      got_sighup = true;
+      break;
+#else
     case SIGHUP:
       got_sighup = true;
       break;
+#endif
     case SIGINT:
       got_sigint = true;
       break;
@@ -27,7 +33,14 @@ namespace {
       LOG(FATAL) << "Tried to hookup signal handlers more than once.";
     }
     already_hooked_up = true;
-
+#ifdef _MSC_VER
+    if (signal(SIGBREAK, handle_signal) == SIG_ERR) {
+      LOG(FATAL) << "Cannot install SIGBREAK handler.";
+    }
+    if (signal(SIGINT, handle_signal) == SIG_ERR) {
+      LOG(FATAL) << "Cannot install SIGINT handler.";
+    }
+#else
     struct sigaction sa;
     // Setup the handler
     sa.sa_handler = &handle_signal;
@@ -42,11 +55,20 @@ namespace {
     if (sigaction(SIGINT, &sa, NULL) == -1) {
       LOG(FATAL) << "Cannot install SIGINT handler.";
     }
+#endif
   }
 
   // Set the signal handlers to the default.
   void UnhookHandler() {
     if (already_hooked_up) {
+#ifdef _MSC_VER
+      if (signal(SIGBREAK, SIG_DFL) == SIG_ERR) {
+        LOG(FATAL) << "Cannot uninstall SIGBREAK handler.";
+      }
+      if (signal(SIGINT, SIG_DFL) == SIG_ERR) {
+        LOG(FATAL) << "Cannot uninstall SIGINT handler.";
+      }
+#else
       struct sigaction sa;
       // Setup the sighub handler
       sa.sa_handler = SIG_DFL;
@@ -61,7 +83,7 @@ namespace {
       if (sigaction(SIGINT, &sa, NULL) == -1) {
         LOG(FATAL) << "Cannot uninstall SIGINT handler.";
       }
-
+#endif
       already_hooked_up = false;
     }
   }
diff --git a/tools/CMakeLists.txt b/tools/CMakeLists.txt
index 3789450..ec162ba 100644
--- a/tools/CMakeLists.txt
+++ b/tools/CMakeLists.txt
@@ -15,6 +15,14 @@ foreach(source ${srcs})
   target_link_libraries(${name} ${Caffe_LINK})
   caffe_default_properties(${name})
 
+  if(MSVC AND COPY_PREREQUISITES)
+    caffe_copy_prerequisites(${name} USE_HARD_LINKS)
+  endif()
+  if(MSVC)
+    target_compile_options(${name} ${Caffe_COMPILE_OPTIONS} /MP
+	/wd4819 /wd4706 /wd4702 /wd4505 /wd4793 /wd4267 /wd4244 /wd5999 /wd4661)
+  endif()
+
   # set back RUNTIME_OUTPUT_DIRECTORY
   caffe_set_runtime_directory(${name} "${PROJECT_BINARY_DIR}/tools")
   caffe_set_solution_folder(${name} tools)
@@ -22,9 +30,17 @@ foreach(source ${srcs})
   # restore output name without suffix
   if(name MATCHES "caffe.bin")
     set_target_properties(${name} PROPERTIES OUTPUT_NAME caffe)
+      if(MSVC)
+    	# the exectuable will have an import library with the same name as the caffe lib
+	    # so change the import library to avoid name clashes
+      set_target_properties(${name} PROPERTIES IMPORT_SUFFIX ".bin.lib")
+    endif()
   endif()
 
   # Install
   install(TARGETS ${name} DESTINATION ${CMAKE_INSTALL_BINDIR})
 
+  if(MSVC AND INSTALL_PREREQUISITES)
+    caffe_install_prerequisites(${name} DESTINATION ${CMAKE_INSTALL_BINDIR})
+  endif()
 endforeach(source)
diff --git a/tools/caffe.cpp b/tools/caffe.cpp
index 1b876df..bc9d738 100644
--- a/tools/caffe.cpp
+++ b/tools/caffe.cpp
@@ -206,7 +206,9 @@ int train() {
   }
 
   vector<int> gpus;
+#ifndef CPU_ONLY
   get_gpus(&gpus);
+#endif
   if (gpus.size() == 0) {
     LOG(INFO) << "Use CPU.";
     Caffe::set_mode(Caffe::CPU);
@@ -277,7 +279,9 @@ int test() {
 
   // Set device id and mode
   vector<int> gpus;
+#ifndef CPU_ONLY
   get_gpus(&gpus);
+#endif
   if (gpus.size() != 0) {
     LOG(INFO) << "Use GPU with device ID " << gpus[0];
 #ifndef CPU_ONLY
@@ -350,7 +354,9 @@ int time() {
 
   // Set device id and mode
   vector<int> gpus;
+#ifndef CPU_ONLY
   get_gpus(&gpus);
+#endif
   if (gpus.size() != 0) {
     LOG(INFO) << "Use GPU with device ID " << gpus[0];
     Caffe::SetDevice(gpus[0]);
diff --git a/tools/convert_annoset.cpp b/tools/convert_annoset.cpp
index 014ca37..1c32007 100644
--- a/tools/convert_annoset.cpp
+++ b/tools/convert_annoset.cpp
@@ -1,3 +1,8 @@
+/*	--anno_type=detection --label_type=txt --label_map_file=data/VOC0712/labelmap_voc.prototxt --check_label=True --min_dim=0 --max_dim=0 --resize_height=0 --resize_width=0 --backend=lmdb --shuffle=False --check_size=False --encode_type=jpg --encoded=True --gray=False data/carwindow/ data/carwindow/train.txt examples/VOC0712/VOC0712_trainval_lmdb
+
+--anno_type=detection --label_type=txt --label_map_file=data/VOC0712/labelmap_voc.prototxt --check_label=True --min_dim=0 --max_dim=0 --resize_height=0 --resize_width=0 --backend=lmdb --shuffle=False --check_size=False --encode_type=jpg --encoded=True --gray=False data/carwindow/ data/carwindow/test.txt examples/VOC0712/VOC0712_test_lmdb
+
+*/
 // This program converts a set of images and annotations to a lmdb/leveldb by
 // storing them as AnnotatedDatum proto buffers.
 // Usage:
